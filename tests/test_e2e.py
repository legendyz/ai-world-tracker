"""
ç«¯åˆ°ç«¯æµ‹è¯?

æµ‹è¯•å®Œæ•´çš„AI World Trackerå·¥ä½œæµç¨‹
"""

import sys
import os
import pytest
import asyncio
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
import json
import tempfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from TheWorldOfAI import AIWorldTracker
from data_collector import AIDataCollector
from content_classifier import ContentClassifier
from importance_evaluator import ImportanceEvaluator
from ai_analyzer import AIAnalyzer
from visualizer import DataVisualizer
from web_publisher import WebPublisher


@pytest.fixture
def mock_data():
    """æä¾›å®Œæ•´çš„mockæ•°æ®"""
    return [
        {
            'title': 'Breakthrough in Generative AI',
            'summary': 'New model achieves state-of-the-art results in text generation',
            'link': 'https://arxiv.org/abs/2025.12345',
            'source': 'arXiv',
            'published': datetime.now().isoformat()
        },
        {
            'title': 'OpenAI Releases GPT-5',
            'summary': 'Major update brings improved reasoning capabilities',
            'link': 'https://openai.com/blog/gpt5',
            'source': 'TechCrunch',
            'published': datetime.now().isoformat()
        },
        {
            'title': 'New Open Source AI Framework',
            'summary': 'Simplifies building and deploying AI models',
            'link': 'https://github.com/company/ai-framework',
            'source': 'GitHub',
            'published': datetime.now().isoformat()
        },
        {
            'title': 'AI Market Analysis Q4 2025',
            'summary': 'Industry insights and growth projections',
            'link': 'https://market.com/report',
            'source': 'MarketWatch',
            'published': datetime.now().isoformat()
        },
        {
            'title': 'Computer Vision Advances in Healthcare',
            'summary': 'AI-powered diagnostics show promising results',
            'link': 'https://nature.com/article',
            'source': 'Nature',
            'published': datetime.now().isoformat()
        }
    ]


@pytest.mark.asyncio
class TestEndToEndPipeline:
    """ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯?""
    
    async def test_full_pipeline_with_mock_data(self, mock_data):
        """æµ‹è¯•å®Œæ•´æµç¨‹ï¼ˆä½¿ç”¨mockæ•°æ®ï¼?""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯æµ‹è¯• - å®Œæ•´æµç¨‹")
        print("="*60)
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # ============ æ­¥éª¤1: æ•°æ®æ”¶é›† ============
        print("\n\U0001f4e1 æ­¥éª¤1: æ•°æ®æ”¶é›†")
        with patch.object(tracker.collector, 'collect_all',
                         new=AsyncMock(return_value={'research': mock_data})):
            async with tracker.collector:
                collected_data_dict = await tracker.collector.collect_all()
                collected_data = []
                for items in collected_data_dict.values():
                    collected_data.extend(items)
        
        assert len(collected_data) == len(mock_data)
        print(f"  âœ?æ”¶é›†äº?{len(collected_data)} æ¡æ•°æ?)
        for item in collected_data[:3]:
            print(f"    - {item['title'][:50]}")
        
        # ============ æ­¥éª¤2: å†…å®¹åˆ†ç±» ============
        print("\nğŸ·ï¸? æ­¥éª¤2: å†…å®¹åˆ†ç±»")
        classified_data = []
        for item in collected_data:
            classified = tracker.classifier.classify_item(item)
            classified_data.append(classified)
        
        assert len(classified_data) == len(collected_data)
        content_types = [d.get('content_type', 'unknown') for d in classified_data]
        print(f"  âœ?åˆ†ç±»å®Œæˆ: {len(classified_data)} æ?)
        print(f"    ç±»å‹åˆ†å¸ƒ: {set(content_types)}")
        
        # ============ æ­¥éª¤3: é‡è¦æ€§è¯„ä¼?============
        print("\nâ­?æ­¥éª¤3: é‡è¦æ€§è¯„ä¼?)
        evaluated_data = []
        for item in classified_data:
            importance = tracker.analyzer.importance_evaluator.calculate_importance(item)
            item['importance'] = importance
            evaluated_data.append(item)
        
        importances = [d['importance'] for d in evaluated_data]
        avg_importance = sum(importances) / len(importances) if importances else 0
        print(f"  âœ?è¯„ä¼°å®Œæˆ: {len(evaluated_data)} æ?)
        print(f"    å¹³å‡é‡è¦æ€? {avg_importance:.2f}")
        print(f"    åˆ†æ•°èŒƒå›´: {min(importances):.2f} - {max(importances):.2f}")
        
        # ============ æ­¥éª¤4: è¶‹åŠ¿åˆ†æ ============
        print("\nğŸ“Š æ­¥éª¤4: è¶‹åŠ¿åˆ†æ")
        analyzer = AIAnalyzer()
        trends = analyzer.analyze_trends(evaluated_data)
        
        assert 'tech_categories' in trends
        assert 'content_distribution' in trends
        print(f"  âœ?åˆ†æå®Œæˆ")
        print(f"    æŠ€æœ¯çƒ­ç‚¹æ•°: {len(trends.get('tech_categories', {}))}")
        print(f"    å†…å®¹åˆ†å¸ƒ: {len(trends.get('content_distribution', {}))}")
        
        # ============ æ­¥éª¤5: æ•°æ®å¯è§†åŒ?============
        print("\nğŸ¨ æ­¥éª¤5: æ•°æ®å¯è§†åŒ?)
        with tempfile.TemporaryDirectory() as tmp_dir:
            visualizer = DataVisualizer()
            visualizer.output_dir = tmp_dir
            
            chart_files = visualizer.visualize_all(trends)
            
            assert isinstance(chart_files, dict)
            print(f"  âœ?ç”Ÿæˆå›¾è¡¨: {len(chart_files)} ä¸?)
            for name in list(chart_files.keys())[:3]:
                print(f"    - {name}")
        
        # ============ æ­¥éª¤6: Webå‘å¸ƒ ============
        print("\nğŸŒ æ­¥éª¤6: Webå‘å¸ƒ")
        with tempfile.TemporaryDirectory() as tmp_dir:
            publisher = WebPublisher()
            publisher.output_dir = tmp_dir
            
            html_file = publisher.generate_html_page(evaluated_data, trends, chart_files)
            
            assert html_file is not None
            assert os.path.exists(html_file)
            print(f"  âœ?ç”ŸæˆHTML: {os.path.basename(html_file)}")
            print(f"    æ–‡ä»¶å¤§å°: {os.path.getsize(html_file)} å­—èŠ‚")
        
        print("\n" + "="*60)
        print("âœ?ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ?- æ‰€æœ‰æ­¥éª¤æˆåŠ?)
        print("="*60)
    
    async def test_pipeline_data_flow(self, mock_data):
        """æµ‹è¯•æ•°æ®åœ¨æµç¨‹ä¸­çš„æµåŠ?""
        print("\nğŸ”„ æµ‹è¯•æ•°æ®æµè½¬")
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # æ”¶é›†
        with patch.object(tracker.collector, 'collect_all',
                         new=AsyncMock(return_value={'research': mock_data})):
            async with tracker.collector:
                data_dict = await tracker.collector.collect_all()
                data = []
                for items in data_dict.values():
                    data.extend(items)
        
        original_count = len(data)
        
        # åˆ†ç±»
        data = tracker.classifier.classify_batch(data)
        
        # è¯„ä¼°
        from importance_evaluator import ImportanceEvaluator
        evaluator = ImportanceEvaluator()
        for item in data:
            item['importance'] = evaluator.calculate_importance(item)
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€?
        assert len(data) == original_count
        assert all('content_type' in item for item in data)
        assert all('importance' in item for item in data)
        
        print(f"  âœ?æ•°æ®å®Œæ•´æ€§éªŒè¯? {original_count} æ¡æ•°æ®ä¿æŒå®Œæ•?)
        print("âœ?æ•°æ®æµè½¬æµ‹è¯•é€šè¿‡")


@pytest.mark.asyncio
class TestErrorRecovery:
    """æµ‹è¯•é”™è¯¯æ¢å¤èƒ½åŠ›"""
    
    async def test_partial_collection_failure(self):
        """æµ‹è¯•éƒ¨åˆ†æ”¶é›†å¤±è´¥çš„æƒ…å†?""
        print("\nâš ï¸  æµ‹è¯•éƒ¨åˆ†æ”¶é›†å¤±è´¥æ¢å¤")
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # Mockéƒ¨åˆ†æˆåŠŸçš„æ•°æ®æ”¶é›?
        partial_data = [
            {'title': 'Success 1', 'summary': 'OK', 'link': 'https://test.com/1', 'source': 'test'},
            {'title': 'Success 2', 'summary': 'OK', 'link': 'https://test.com/2', 'source': 'test'}
        ]
        
        with patch.object(tracker.collector, 'collect_all',
                         new=AsyncMock(return_value={'research': partial_data})):
            async with tracker.collector:
                data_dict = await tracker.collector.collect_all()
                data = []
                for items in data_dict.values():
                    data.extend(items)
        
        # å³ä½¿éƒ¨åˆ†å¤±è´¥ï¼Œä¹Ÿåº”è¯¥èƒ½å¤„ç†æˆåŠŸçš„æ•°æ®
        assert len(data) > 0
        print(f"  âœ?æˆåŠŸå¤„ç†äº?{len(data)} æ¡æ•°æ?)
        print("âœ?éƒ¨åˆ†å¤±è´¥æ¢å¤æµ‹è¯•é€šè¿‡")
    
    def test_classification_with_incomplete_data(self):
        """æµ‹è¯•ä¸å®Œæ•´æ•°æ®çš„åˆ†ç±»"""
        print("\nâš ï¸  æµ‹è¯•ä¸å®Œæ•´æ•°æ®åˆ†ç±?)
        
        tracker = AIWorldTracker(auto_mode=True)
        
        incomplete_items = [
            {'title': 'Only title'},
            {'summary': 'Only summary'},
            {'title': 'Title', 'summary': 'Summary'}
        ]
        
        results = []
        for item in incomplete_items:
            try:
                result = tracker.classifier.classify_item(item)  # ä½¿ç”¨classify_itemå¤„ç†å•æ¡
                results.append(result)
            except Exception as e:
                print(f"  ! åˆ†ç±»å¤±è´¥: {e}")
        
        print(f"  âœ?æˆåŠŸåˆ†ç±»: {len(results)}/{len(incomplete_items)}")
        print("âœ?ä¸å®Œæ•´æ•°æ®å¤„ç†æµ‹è¯•é€šè¿‡")


class TestDataQuality:
    """æµ‹è¯•æ•°æ®è´¨é‡"""
    
    def test_classification_accuracy(self, mock_data):
        """æµ‹è¯•åˆ†ç±»å‡†ç¡®æ€?""
        print("\nğŸ¯ æµ‹è¯•åˆ†ç±»å‡†ç¡®æ€?)
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # å·²çŸ¥ç±»å‹çš„æµ‹è¯•æ•°æ?
        known_types = {
            'Breakthrough in Generative AI': 'research',
            'OpenAI Releases GPT-5': 'product',
            'New Open Source AI Framework': 'developer',
            'AI Market Analysis Q4 2025': 'market'
        }
        
        correct = 0
        total = 0
        
        for item in mock_data:
            if item['title'] in known_types:
                result = tracker.classifier.classify_item(item)  # ä½¿ç”¨classify_item
                expected = known_types[item['title']]
                actual = result.get('content_type')
                
                if actual == expected:
                    correct += 1
                total += 1
                
                print(f"  {'âœ? if actual == expected else 'âœ?} {item['title'][:40]}: "
                      f"é¢„æœŸ={expected}, å®é™…={actual}")
        
        accuracy = correct / total if total > 0 else 0
        print(f"\n  å‡†ç¡®ç? {accuracy:.1%} ({correct}/{total})")
        print("âœ?åˆ†ç±»å‡†ç¡®æ€§æµ‹è¯•å®Œæˆ?)
    
    def test_importance_consistency(self, mock_data):
        """æµ‹è¯•é‡è¦æ€§è¯„åˆ†ä¸€è‡´æ€?""
        print("\nğŸ“ˆ æµ‹è¯•é‡è¦æ€§è¯„åˆ†ä¸€è‡´æ€?)
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # å¯¹åŒä¸€æ•°æ®å¤šæ¬¡è¯„åˆ†
        from importance_evaluator import ImportanceEvaluator
        evaluator = ImportanceEvaluator()
        test_item = mock_data[0].copy()
        test_item['content_type'] = 'research'
        test_item['tech_categories'] = ['Generative AI']
        
        scores = []
        for _ in range(3):
            score = evaluator.calculate_importance(test_item)
            scores.append(score)
        
        # è¯„åˆ†åº”è¯¥ä¸€è‡?
        assert len(set(scores)) <= 2  # å…è®¸å¾®å°å·®å¼‚
        avg_score = sum(scores) / len(scores)
        print(f"  è¯„åˆ†: {scores}")
        print(f"  å¹³å‡: {avg_score:.3f}")
        print("âœ?é‡è¦æ€§è¯„åˆ†ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")


@pytest.mark.asyncio
class TestPerformance:
    """æµ‹è¯•æ€§èƒ½"""
    
    async def test_large_dataset_handling(self):
        """æµ‹è¯•å¤§æ•°æ®é›†å¤„ç†"""
        print("\nâš?æµ‹è¯•å¤§æ•°æ®é›†å¤„ç†")
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # ç”Ÿæˆ100æ¡mockæ•°æ®
        large_dataset = []
        for i in range(100):
            large_dataset.append({
                'title': f'AI Article {i}',
                'summary': f'This is a test article about AI technology number {i}',
                'link': f'https://test.com/{i}',
                'source': 'test',
                'published': datetime.now().isoformat()
            })
        
        import time
        start_time = time.time()
        
        # åˆ†ç±»å’Œè¯„ä¼?
        from importance_evaluator import ImportanceEvaluator
        evaluator = ImportanceEvaluator()
        classified = tracker.classifier.classify_batch(large_dataset)
        for item in classified:
            item['importance'] = evaluator.calculate_importance(item)
        
        duration = time.time() - start_time
        avg_time = duration / len(large_dataset) * 1000  # æ¯«ç§’
        
        print(f"  å¤„ç† {len(large_dataset)} æ¡æ•°æ?)
        print(f"  æ€»è€—æ—¶: {duration:.2f}ç§?)
        print(f"  å¹³å‡: {avg_time:.1f}ms/æ?)
        
        assert len(classified) == len(large_dataset)
        print("âœ?å¤§æ•°æ®é›†å¤„ç†æµ‹è¯•é€šè¿‡")


class TestIntegrationPoints:
    """æµ‹è¯•é›†æˆç‚?""
    
    def test_analyzer_visualizer_integration(self, mock_data):
        """æµ‹è¯•åˆ†æå™¨å’Œå¯è§†åŒ–å™¨é›†æˆ"""
        print("\nğŸ”— æµ‹è¯•åˆ†æå™?å¯è§†åŒ–å™¨é›†æˆ")
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # å‡†å¤‡æ•°æ®
        from importance_evaluator import ImportanceEvaluator
        evaluator = ImportanceEvaluator()
        classified = tracker.classifier.classify_batch(mock_data)
        for item in classified:
            item['importance'] = evaluator.calculate_importance(item)
        
        # åˆ†æ
        analyzer = AIAnalyzer()
        trends = analyzer.analyze_trends(classified)
        
        # å¯è§†åŒ?
        with tempfile.TemporaryDirectory() as tmp_dir:
            visualizer = DataVisualizer()
            visualizer.output_dir = tmp_dir
            charts = visualizer.visualize_all(trends)
        
        assert len(charts) > 0
        print(f"  âœ?ç”Ÿæˆäº?{len(charts)} ä¸ªå›¾è¡?)
        print("âœ?é›†æˆæµ‹è¯•é€šè¿‡")
    
    def test_visualizer_publisher_integration(self, mock_data):
        """æµ‹è¯•å¯è§†åŒ–å™¨å’Œå‘å¸ƒå™¨é›†æˆ"""
        print("\nğŸ”— æµ‹è¯•å¯è§†åŒ–å™¨-å‘å¸ƒå™¨é›†æˆ?)
        
        tracker = AIWorldTracker(auto_mode=True)
        
        # å‡†å¤‡å®Œæ•´æ•°æ®
        from importance_evaluator import ImportanceEvaluator
        evaluator = ImportanceEvaluator()
        classified = tracker.classifier.classify_batch(mock_data)
        for item in classified:
            item['importance'] = evaluator.calculate_importance(item)
        
        analyzer = AIAnalyzer()
        trends = analyzer.analyze_trends(classified)
        
        with tempfile.TemporaryDirectory() as tmp_dir:
            # å¯è§†åŒ?
            visualizer = DataVisualizer()
            visualizer.output_dir = tmp_dir
            charts = visualizer.visualize_all(trends)
            
            # å‘å¸ƒ
            publisher = WebPublisher()
            publisher.output_dir = tmp_dir
            html = publisher.generate_html_page(classified, trends, charts)
        
        assert os.path.exists(html)
        print(f"  âœ?ç”ŸæˆHTML: {os.path.basename(html)}")
        print("âœ?é›†æˆæµ‹è¯•é€šè¿‡")


if __name__ == '__main__':
    print("\n" + "ğŸŒŸ" * 30)
    print("   AI World Tracker ç«¯åˆ°ç«¯æµ‹è¯?)
    print("ğŸŒŸ" * 30)
    
    # è¿è¡Œæµ‹è¯•
    pytest.main([__file__, '-v', '-s'])

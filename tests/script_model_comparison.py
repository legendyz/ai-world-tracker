"""æµ‹è¯• Qwen3:8b vs DeepSeek R1:8b åˆ†ç±»æ•ˆæœå¯¹æ¯”"""
from llm_classifier import LLMClassifier
import time

# æµ‹è¯•ç”¨ä¾‹
test_cases = [
    {
        'title': 'OpenAI officially launches GPT-4o with new features',
        'summary': 'OpenAI announces the general availability of GPT-4o model with multimodal capabilities',
        'source': 'TechCrunch',
        'expected': 'llm/product'
    },
    {
        'title': 'New research paper: Attention is All You Need 2.0',
        'summary': 'Researchers publish groundbreaking paper on transformer architecture improvements',
        'source': 'arXiv',
        'expected': 'research'
    },
    {
        'title': 'Tesla unveils humanoid robot Optimus Gen 3',
        'summary': 'Tesla announces next generation of its humanoid robot with improved capabilities',
        'source': 'The Verge',
        'expected': 'robotics'
    },
    {
        'title': 'EU passes comprehensive AI regulation act',
        'summary': 'European Union finalizes AI Act with strict requirements for high-risk AI systems',
        'source': 'Reuters',
        'expected': 'ethics'
    }
]

def run_model_test(model_name: str):
    """è¿è¡Œå•ä¸ªæ¨¡å‹æµ‹è¯•ï¼ˆé pytest æµ‹è¯•å‡½æ•°ï¼‰"""
    print(f"\n{'=' * 60}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"{'=' * 60}")
    
    classifier = LLMClassifier(
        provider='ollama',
        model=model_name,
        enable_cache=False  # ç¦ç”¨ç¼“å­˜ä»¥ä¾¿å…¬å¹³æµ‹è¯•
    )
    
    total_time = 0
    results = []
    
    for i, test in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {test['title'][:40]}...")
        
        start = time.time()
        result = classifier.classify_item(test)
        elapsed = time.time() - start
        total_time += elapsed
        
        print(f"   é¢„æœŸ: {test['expected']}")
        print(f"   å®é™…: {result.get('content_type')}")
        print(f"   ç½®ä¿¡åº¦: {result.get('confidence', 0):.0%}")
        print(f"   è€—æ—¶: {elapsed:.1f}s")
        print(f"   ç†ç”±: {result.get('llm_reasoning', 'N/A')[:50]}")
        
        results.append({
            'expected': test['expected'],
            'actual': result.get('content_type'),
            'time': elapsed
        })
    
    print(f"\nğŸ“Š {model_name} æ€»ç»“:")
    print(f"   å¹³å‡è€—æ—¶: {total_time/len(test_cases):.1f}s")
    print(f"   æ€»è€—æ—¶: {total_time:.1f}s")
    
    return results

# æµ‹è¯• Qwen3:8b
print("\n" + "ğŸš€" * 30)
print("   Qwen3:8b vs DeepSeek R1:8b å¯¹æ¯”æµ‹è¯•")
print("ğŸš€" * 30)

qwen_results = run_model_test('qwen3:8b')

print("\n" + "-" * 60)

r1_results = run_model_test('deepseek-r1:8b')

# å¯¹æ¯”æ€»ç»“
print("\n" + "=" * 60)
print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”")
print("=" * 60)
print(f"{'æµ‹è¯•é¡¹':<30} {'Qwen3:8b':<15} {'DeepSeek R1:8b':<15}")
print("-" * 60)
for i, (q, r) in enumerate(zip(qwen_results, r1_results), 1):
    print(f"æµ‹è¯• {i} ç»“æœ:                  {q['actual']:<15} {r['actual']:<15}")
print("-" * 60)
qwen_avg = sum(r['time'] for r in qwen_results) / len(qwen_results)
r1_avg = sum(r['time'] for r in r1_results) / len(r1_results)
print(f"{'å¹³å‡å“åº”æ—¶é—´:':<30} {qwen_avg:.1f}s{'':<10} {r1_avg:.1f}s")
print(f"{'é€Ÿåº¦æå‡:':<30} {r1_avg/qwen_avg:.1f}x æ›´å¿«" if qwen_avg < r1_avg else f"{'é€Ÿåº¦å¯¹æ¯”:':<30} ç›¸è¿‘")

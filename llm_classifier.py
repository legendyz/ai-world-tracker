"""
LLMå¢å¼ºåˆ†ç±»å™¨ - LLM Classifier
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†ç±»

æ”¯æŒçš„æä¾›å•†:
- Ollama (æœ¬åœ°): Qwen3:8b, Llama3.2:3b, Mistral:7b
- Azure OpenAI: GPT-4o-mini, GPT-4o

åŠŸèƒ½ç‰¹æ€§:
- å¤šæä¾›å•†æ”¯æŒï¼Œçµæ´»åˆ‡æ¢
- MD5å†…å®¹ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
- è‡ªåŠ¨é™çº§åˆ°è§„åˆ™åˆ†ç±»
- å¹¶å‘å¤„ç†åŠ é€Ÿ
- è¯¦ç»†çš„åˆ†ç±»æ¨ç†
- GPUè‡ªåŠ¨æ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
"""

import os
import json
import hashlib
import time
import subprocess
import platform
import threading
import re
import requests
import yaml
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum

# å¯¼å…¥è§„åˆ™åˆ†ç±»å™¨ä½œä¸ºå¤‡ä»½
from content_classifier import ContentClassifier
from importance_evaluator import ImportanceEvaluator
from logger import get_log_helper

# å¯¼å…¥å›½é™…åŒ–æ¨¡å—
try:
    from i18n import t, get_language
except ImportError:
    def t(key, **kwargs): return key
    def get_language(): return 'zh'

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('llm_classifier')

# ============== é™çº§ç­–ç•¥é…ç½® ==============

class FallbackReason(Enum):
    """é™çº§åŸå› æšä¸¾"""
    TIMEOUT = "timeout"
    CONNECTION_ERROR = "connection_error"
    PARSE_ERROR = "parse_error"
    INVALID_RESPONSE = "invalid_response"
    API_ERROR = "api_error"
    RATE_LIMIT = "rate_limit"
    MODEL_ERROR = "model_error"


class FallbackStrategy:
    """æ™ºèƒ½é™çº§ç­–ç•¥ç®¡ç†å™¨"""
    
    def __init__(self):
        self.error_counts = {}  # é”™è¯¯è®¡æ•°
        self.last_error_time = {}  # æœ€åé”™è¯¯æ—¶é—´
        self.circuit_breaker_open = False  # æ–­è·¯å™¨çŠ¶æ€
        self.circuit_breaker_open_time = None
        self.circuit_breaker_threshold = 5  # è¿ç»­å¤±è´¥é˜ˆå€¼
        self.circuit_breaker_timeout = 60  # æ–­è·¯å™¨æ‰“å¼€æ—¶é—´ï¼ˆç§’ï¼‰
    
    def should_use_llm(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ LLMï¼ˆæ–­è·¯å™¨æ£€æŸ¥ï¼‰"""
        if not self.circuit_breaker_open:
            return True
        
        # æ£€æŸ¥æ–­è·¯å™¨æ˜¯å¦åº”è¯¥å…³é—­
        if self.circuit_breaker_open_time:
            elapsed = time.time() - self.circuit_breaker_open_time
            if elapsed > self.circuit_breaker_timeout:
                log.dual_info("ğŸ”„ Circuit breaker closed, retrying LLM")
                self.circuit_breaker_open = False
                self.circuit_breaker_open_time = None
                self.error_counts.clear()
                return True
        
        return False
    
    def record_error(self, reason: FallbackReason):
        """è®°å½•é”™è¯¯å¹¶æ›´æ–°æ–­è·¯å™¨çŠ¶æ€"""
        reason_key = reason.value
        self.error_counts[reason_key] = self.error_counts.get(reason_key, 0) + 1
        self.last_error_time[reason_key] = time.time()
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰“å¼€æ–­è·¯å™¨
        total_errors = sum(self.error_counts.values())
        if total_errors >= self.circuit_breaker_threshold and not self.circuit_breaker_open:
            self.circuit_breaker_open = True
            self.circuit_breaker_open_time = time.time()
            log.dual_warning(f"âš ï¸ Circuit breaker opened after {total_errors} errors")
    
    def record_success(self):
        """è®°å½•æˆåŠŸï¼Œé‡ç½®é”™è¯¯è®¡æ•°"""
        if self.error_counts:
            self.error_counts.clear()
            self.last_error_time.clear()
    
    def get_fallback_action(self, reason: FallbackReason, item: Dict) -> str:
        """æ ¹æ®é”™è¯¯ç±»å‹å†³å®šé™çº§ç­–ç•¥
        
        Returns:
            'retry': é‡è¯• LLM
            'quick': å¿«é€Ÿé™çº§ï¼ˆç®€åŒ–è§„åˆ™ï¼‰
            'full_rule': å®Œæ•´è§„åˆ™åˆ†ç±»
        """
        # è¶…æ—¶é”™è¯¯ï¼šä½¿ç”¨å¿«é€Ÿé™çº§
        if reason == FallbackReason.TIMEOUT:
            return 'quick'
        
        # è¿æ¥é”™è¯¯ï¼šæ–­è·¯å™¨æ‰“å¼€ï¼Œä½¿ç”¨å®Œæ•´è§„åˆ™
        if reason in [FallbackReason.CONNECTION_ERROR, FallbackReason.API_ERROR]:
            self.record_error(reason)
            return 'full_rule' if self.circuit_breaker_open else 'retry'
        
        # è§£æé”™è¯¯ï¼šé‡è¯•ä¸€æ¬¡ï¼Œå¤±è´¥åˆ™é™çº§
        if reason in [FallbackReason.PARSE_ERROR, FallbackReason.INVALID_RESPONSE]:
            error_count = self.error_counts.get(reason.value, 0)
            if error_count < 2:
                return 'retry'
            return 'full_rule'
        
        # é€Ÿç‡é™åˆ¶ï¼šç­‰å¾…åé‡è¯•
        if reason == FallbackReason.RATE_LIMIT:
            time.sleep(2)
            return 'retry'
        
        # é»˜è®¤ï¼šå®Œæ•´è§„åˆ™åˆ†ç±»
        return 'full_rule'


# åŠ è½½ç¼“å­˜ç›®å½•é…ç½®
def _get_cache_dir():
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    cache_dir = 'data/cache'
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                cache_dir = cfg.get('data', {}).get('cache_dir', cache_dir)
    except (OSError, yaml.YAMLError, KeyError) as e:
        # é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        pass
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir

DATA_CACHE_DIR = _get_cache_dir()

# æ¨¡å‹ä¿æ´»æ—¶é—´ï¼ˆç§’ï¼‰
MODEL_KEEP_ALIVE_SECONDS = 5 * 60  # 5åˆ†é’Ÿ

# Ollama è¶…æ—¶é…ç½®
OLLAMA_WARMUP_TIMEOUT = 180  # é¢„çƒ­è¶…æ—¶ï¼ˆæ¨¡å‹é¦–æ¬¡åŠ è½½å¯èƒ½å¾ˆæ…¢ï¼‰
OLLAMA_SINGLE_REQUEST_TIMEOUT = 120  # å•æ¡åˆ†ç±»è¶…æ—¶
OLLAMA_BATCH_REQUEST_TIMEOUT = 150  # æ‰¹é‡åˆ†ç±»è¶…æ—¶

# ç»Ÿä¸€çš„ LLM System Promptï¼ˆæ‰€æœ‰æä¾›å•†ä½¿ç”¨ç›¸åŒçš„ç³»ç»Ÿæç¤ºï¼‰
LLM_SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå†…å®¹åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†ç±»ç»“æœã€‚"


class LLMProvider(Enum):
    """æä¾›å•†æšä¸¾"""
    OLLAMA = "ollama"
    OPENAI = "openai"
    AZURE_OPENAI = "azure_openai"


@dataclass
class GPUInfo:
    """GPUä¿¡æ¯"""
    available: bool = False
    gpu_type: str = "none"  # nvidia, amd, apple, qualcomm, none
    gpu_name: str = ""
    vram_mb: int = 0
    driver_version: str = ""
    cuda_available: bool = False
    rocm_available: bool = False
    metal_available: bool = False
    ollama_gpu_supported: bool = False  # Ollamaæ˜¯å¦æ”¯æŒè¯¥GPU


def detect_gpu() -> GPUInfo:
    """
    æ£€æµ‹ç³»ç»ŸGPUä¿¡æ¯
    
    Returns:
        GPUInfo: GPUæ£€æµ‹ç»“æœ
    """
    info = GPUInfo()
    system = platform.system()
    
    # 1. æ£€æµ‹ NVIDIA GPU (CUDA)
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0 and result.stdout.strip():
            parts = result.stdout.strip().split(', ')
            if len(parts) >= 3:
                info.available = True
                info.gpu_type = "nvidia"
                info.gpu_name = parts[0].strip()
                info.vram_mb = int(float(parts[1].strip()))
                info.driver_version = parts[2].strip()
                info.cuda_available = True
                info.ollama_gpu_supported = True
                return info
    except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
        pass
    
    # 2. æ£€æµ‹ AMD GPU (ROCm) - ä»…Linux
    if system == "Linux":
        try:
            result = subprocess.run(['rocm-smi', '--showproductname'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and 'GPU' in result.stdout:
                info.available = True
                info.gpu_type = "amd"
                info.gpu_name = "AMD ROCm GPU"
                info.rocm_available = True
                info.ollama_gpu_supported = True
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 3. æ£€æµ‹ Apple Silicon (Metal)
    if system == "Darwin":
        try:
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                cpu_info = result.stdout.strip()
                if 'Apple' in cpu_info:
                    info.available = True
                    info.gpu_type = "apple"
                    info.gpu_name = cpu_info
                    info.metal_available = True
                    info.ollama_gpu_supported = True
                    return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 4. æ£€æµ‹ Windows æ˜¾å¡ï¼ˆå¯èƒ½æ˜¯ä¸æ”¯æŒçš„GPUï¼‰
    if system == "Windows":
        try:
            result = subprocess.run(
                ['powershell', '-Command', 
                 'Get-WmiObject Win32_VideoController | Select-Object -First 1 Name, AdapterRAM, DriverVersion | ConvertTo-Json'],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0 and result.stdout.strip():
                gpu_data = json.loads(result.stdout)
                gpu_name = gpu_data.get('Name', '')
                
                info.gpu_name = gpu_name
                info.driver_version = gpu_data.get('DriverVersion', '')
                adapter_ram = gpu_data.get('AdapterRAM', 0)
                if adapter_ram:
                    info.vram_mb = int(adapter_ram / (1024 * 1024))
                
                # åˆ¤æ–­GPUç±»å‹
                if 'NVIDIA' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "nvidia"
                    info.cuda_available = True
                    info.ollama_gpu_supported = True
                elif 'AMD' in gpu_name.upper() or 'RADEON' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "amd"
                    info.ollama_gpu_supported = False  # Windowsä¸ŠAMDä¸æ”¯æŒ
                elif 'QUALCOMM' in gpu_name.upper() or 'ADRENO' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "qualcomm"
                    info.ollama_gpu_supported = False  # Qualcommä¸æ”¯æŒ
                elif 'INTEL' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "intel"
                    info.ollama_gpu_supported = False  # Intelé›†æ˜¾ä¸æ”¯æŒ
                
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, json.JSONDecodeError, Exception):
            pass
    
    return info


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    timeout: int = 60
    max_retries: int = 2


@dataclass
class OllamaOptions:
    """Ollamaæ¨ç†é€‰é¡¹ - æ ¹æ®GPUè‡ªé€‚åº”é…ç½®"""
    temperature: float = 0.1
    num_predict: int = 200  # å•æ¡åˆ†ç±»è¾“å‡ºé•¿åº¦
    num_predict_batch: int = 500  # æ‰¹é‡åˆ†ç±»è¾“å‡ºé•¿åº¦ï¼ˆæ¯æ¡çº¦80 tokensï¼‰
    num_ctx: int = 2048
    num_thread: int = 4
    num_gpu: int = 0  # 0è¡¨ç¤ºè‡ªåŠ¨ï¼Œ-1è¡¨ç¤ºç¦ç”¨GPU
    
    @classmethod
    def auto_configure(cls, gpu_info: GPUInfo) -> 'OllamaOptions':
        """æ ¹æ®GPUä¿¡æ¯è‡ªåŠ¨é…ç½®æ¨ç†é€‰é¡¹"""
        options = cls()
        
        if gpu_info and gpu_info.ollama_gpu_supported:
            # GPUåŠ é€Ÿé…ç½® - ä¼˜åŒ–é€Ÿåº¦
            options.num_gpu = 999  # ä½¿ç”¨æ‰€æœ‰GPUå±‚
            options.num_ctx = 4096  # GPUå¯ä»¥å¤„ç†æ›´å¤§ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰
            options.num_predict = 200  # å•æ¡åˆ†ç±»
            options.num_predict_batch = 600  # æ‰¹é‡åˆ†ç±»ï¼ˆ5æ¡*80tokens+ä½™é‡ï¼‰
            options.num_thread = 4  # GPUæ¨¡å¼ä¸‹CPUçº¿ç¨‹ä¸éœ€è¦å¤ªå¤š
        else:
            # CPUæ¨¡å¼ä¼˜åŒ–é…ç½®
            options.num_gpu = 0  # ç¦ç”¨GPU
            options.num_ctx = 2048  # å¢åŠ ä¸Šä¸‹æ–‡ä»¥æ”¯æŒæ‰¹é‡
            options.num_predict = 150  # å•æ¡åˆ†ç±»
            options.num_predict_batch = 500  # æ‰¹é‡åˆ†ç±»
            # æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®çº¿ç¨‹
            try:
                import multiprocessing
                cpu_count = multiprocessing.cpu_count()
                options.num_thread = min(cpu_count, 8)  # æœ€å¤š8çº¿ç¨‹
            except (NotImplementedError, OSError):
                options.num_thread = 4
        
        return options


# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    LLMProvider.OLLAMA: {
        'qwen3:8b': {'name': 'Qwen3 8B', 'description': 'é˜¿é‡Œé€šä¹‰åƒé—®ï¼Œä¸­æ–‡èƒ½åŠ›å¼ºï¼Œæ¨èä½¿ç”¨'},
        'llama3.2:3b': {'name': 'Llama 3.2 3B', 'description': 'Metaè½»é‡æ¨¡å‹ï¼Œé€Ÿåº¦æœ€å¿«'},
        'mistral:7b': {'name': 'Mistral 7B', 'description': 'æ€§èƒ½å‡è¡¡ï¼Œè‹±æ–‡èƒ½åŠ›å¼º'},
    },
    LLMProvider.OPENAI: {
        'gpt-4o-mini': {'name': 'GPT-4o Mini', 'description': 'æ€§ä»·æ¯”é«˜ï¼Œæ¨èä½¿ç”¨'},
        'gpt-4o': {'name': 'GPT-4o', 'description': 'æœ€å¼ºæ€§èƒ½ï¼Œæˆæœ¬è¾ƒé«˜'},
        'gpt-3.5-turbo': {'name': 'GPT-3.5 Turbo', 'description': 'ç»æµå®æƒ '},
    },
    LLMProvider.AZURE_OPENAI: {
        'gpt-4o-mini': {'name': 'GPT-4o Mini (Azure)', 'description': 'Azureéƒ¨ç½²ï¼Œä¼ä¸šçº§å®‰å…¨'},
        'gpt-4o': {'name': 'GPT-4o (Azure)', 'description': 'Azureéƒ¨ç½²ï¼Œæœ€å¼ºæ€§èƒ½'},
        'gpt-4': {'name': 'GPT-4 (Azure)', 'description': 'Azureéƒ¨ç½²ï¼Œç¨³å®šå¯é '},
        'gpt-35-turbo': {'name': 'GPT-3.5 Turbo (Azure)', 'description': 'Azureéƒ¨ç½²ï¼Œç»æµå®æƒ '},
    }
}


class LLMClassifier:
    """LLMå¢å¼ºåˆ†ç±»å™¨"""
    
    def __init__(self, 
                 provider: str = 'ollama',
                 model: str = 'qwen3:8b',
                 api_key: Optional[str] = None,
                 enable_cache: bool = True,
                 max_workers: int = 3,  # é»˜è®¤å¹¶å‘æ•°
                 auto_detect_gpu: bool = True,
                 batch_size: int = 5,  # æ–°å¢æ‰¹é‡åˆ†ç±»å¤§å°
                 azure_endpoint: Optional[str] = None,  # Azure OpenAI ç«¯ç‚¹
                 azure_api_version: Optional[str] = None):  # Azure API ç‰ˆæœ¬
        """
        åˆå§‹åŒ–LLMåˆ†ç±»å™¨
        
        Args:
            provider: æä¾›å•† ('ollama', 'openai', 'azure_openai', 'anthropic')
            model: æ¨¡å‹åç§° (å¯¹äº Azure OpenAIï¼Œè¿™æ˜¯éƒ¨ç½²åç§°)
            api_key: APIå¯†é’¥ï¼ˆOllamaä¸éœ€è¦ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            max_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤5ï¼ŒGPUæ¨¡å¼å¯æ›´é«˜)
            auto_detect_gpu: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹GPUå¹¶ä¼˜åŒ–é…ç½®
            batch_size: æ‰¹é‡åˆ†ç±»æ—¶æ¯æ‰¹çš„æ•°é‡ (ç”¨äºå‡å°‘LLMè°ƒç”¨æ¬¡æ•°)
            azure_endpoint: Azure OpenAI ç«¯ç‚¹ URL (å¦‚ https://xxx.openai.azure.com/)
            azure_api_version: Azure OpenAI API ç‰ˆæœ¬ (å¦‚ 2024-02-15-preview)
        """
        self.provider = LLMProvider(provider)
        self.model = model
        self.api_key = api_key or self._get_api_key()
        self.enable_cache = enable_cache
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # Azure OpenAI ç‰¹æœ‰é…ç½®
        self.azure_endpoint = azure_endpoint or os.getenv('AZURE_OPENAI_ENDPOINT')
        self.azure_api_version = azure_api_version or os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')
        
        # GPUæ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
        self.gpu_info: Optional[GPUInfo] = None
        self.ollama_options: Optional[OllamaOptions] = None
        if auto_detect_gpu and self.provider == LLMProvider.OLLAMA:
            self._setup_gpu_acceleration()
            # GPUæ¨¡å¼ä¸‹å¯ä»¥æé«˜å¹¶å‘æ•°
            if self.gpu_info and self.gpu_info.ollama_gpu_supported:
                self.max_workers = max(max_workers, 6)  # GPUæ¨¡å¼æé«˜å¹¶å‘è‡³6
        
        # ç¼“å­˜
        self.cache: Dict[str, Dict] = {}
        self.cache_file = os.path.join(DATA_CACHE_DIR, 'llm_classification_cache.json')
        self._load_cache()
        
        # è§„åˆ™åˆ†ç±»å™¨ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
        self.rule_classifier = ContentClassifier()
        
        # ç‹¬ç«‹çš„é‡è¦æ€§è¯„ä¼°å™¨ (è§£è€¦åçš„è®¾è®¡)
        self.importance_evaluator = ImportanceEvaluator()
        
        # é™çº§ç­–ç•¥ç®¡ç†å™¨
        self.fallback_strategy = FallbackStrategy()
        
        # æ¨¡å‹é¢„çƒ­çŠ¶æ€
        self.is_warmed_up = False
        self._keep_alive_timer: Optional[threading.Timer] = None
        
        # ç»Ÿè®¡
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'llm_calls': 0,
            'fallback_calls': 0,
            'errors': 0,
            'fallback_details': []  # è®°å½•æ¯æ¡é™çº§çš„è¯¦ç»†ä¿¡æ¯
        }
        
        # HTTP ä¼šè¯å¤ç”¨ï¼ˆæ–°å¢ï¼‰
        self.session = self._create_http_session()
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        self._print_init_info()
    
    def _create_http_session(self) -> requests.Session:
        """åˆ›å»ºé…ç½®å¥½çš„ HTTP ä¼šè¯ï¼ˆè¿æ¥æ± å¤ç”¨ï¼‰"""
        session = requests.Session()
        
        # é…ç½®è¯·æ±‚å¤´
        session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'AI-World-Tracker/1.0'
        })
        
        # é…ç½®è¿æ¥æ± å’Œé‡è¯•ç­–ç•¥
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=3,  # æœ€å¤šé‡è¯•3æ¬¡
            backoff_factor=0.5,  # é‡è¯•é—´éš”: 0.5s, 1s, 2s
            status_forcelist=[429, 500, 502, 503, 504],  # è¿™äº›çŠ¶æ€ç è§¦å‘é‡è¯•
            allowed_methods=["POST", "GET"]  # å…è®¸é‡è¯•çš„æ–¹æ³•
        )
        
        adapter = HTTPAdapter(
            pool_connections=10,  # è¿æ¥æ± å¤§å°
            pool_maxsize=20,  # æœ€å¤§è¿æ¥æ•°
            max_retries=retry_strategy
        )
        
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        return session
    
    def _setup_gpu_acceleration(self):
        """è®¾ç½®GPUåŠ é€Ÿ"""
        self.gpu_info = detect_gpu()
        self.ollama_options = OllamaOptions.auto_configure(self.gpu_info)
    
    def _print_init_info(self):
        """æ‰“å°åˆå§‹åŒ–ä¿¡æ¯"""
        log.dual_ai(t('llm_init_done'))
        log.dual_ai(t('llm_provider', provider=self.provider.value))
        log.dual_ai(t('llm_model_name', model=self.model))
        cache_status = t('llm_cache_enabled') if self.enable_cache else t('llm_cache_disabled')
        log.dual_config(t('llm_cache_status', status=cache_status))
        
        if self.gpu_info:
            if self.gpu_info.ollama_gpu_supported:
                log.dual_success(t('llm_gpu_enabled', gpu_name=self.gpu_info.gpu_name))
                if self.gpu_info.vram_mb:
                    log.dual_info("ğŸ’¾ " + t('llm_vram', vram=self.gpu_info.vram_mb))
            else:
                gpu_name = self.gpu_info.gpu_name or t('llm_no_gpu_detected')
                log.dual_warning(t('llm_cpu_mode', gpu_name=gpu_name))
                if self.ollama_options:
                    log.dual_info("âš™ï¸ " + t('llm_cpu_threads', threads=self.ollama_options.num_thread))
    
    def get_gpu_info(self) -> Optional[GPUInfo]:
        """è·å–GPUä¿¡æ¯"""
        return self.gpu_info
    
    def warmup_model(self) -> bool:
        """
        é¢„çƒ­æ¨¡å‹ï¼šå‘é€ä¸€ä¸ªç®€å•è¯·æ±‚è®©æ¨¡å‹åŠ è½½åˆ°å†…å­˜/æ˜¾å­˜
        
        Returns:
            bool: é¢„çƒ­æ˜¯å¦æˆåŠŸ
        """
        if self.provider != LLMProvider.OLLAMA:
            # äº‘ç«¯APIä¸éœ€è¦é¢„çƒ­
            self.is_warmed_up = True
            return True
        
        if self.is_warmed_up:
            log.dual_info("âœ… " + t('llm_model_warmed'))
            return True
        
        log.dual_ai(t('llm_warming_model', model=self.model))
        start_time = time.time()
        
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æ¥åŠ è½½æ¨¡å‹
            # ä½¿ç”¨ keep_alive å‚æ•°è®©æ¨¡å‹ä¿æŒæ´»è·ƒ
            log.dual_info(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°{'GPU' if self.gpu_info and self.gpu_info.ollama_gpu_supported else 'CPU'}å†…å­˜ï¼Œé¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦1-3åˆ†é’Ÿ...")
            response = self.session.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': 'Hi',  # æœ€ç®€å•çš„prompt
                    'stream': False,
                    'keep_alive': f'{MODEL_KEEP_ALIVE_SECONDS}s',  # ä¿æ´»æ—¶é—´
                    'options': {
                        'num_predict': 1,  # åªç”Ÿæˆ1ä¸ªtoken
                        'num_ctx': 512
                    }
                },
                timeout=OLLAMA_WARMUP_TIMEOUT  # é¦–æ¬¡åŠ è½½å¯èƒ½è¾ƒæ…¢
            )
            
            if response.status_code == 200:
                elapsed = time.time() - start_time
                self.is_warmed_up = True
                log.dual_success(t('llm_warmup_done', time=f'{elapsed:.1f}'))
                log.dual_info("â° " + t('llm_keep_alive', minutes=MODEL_KEEP_ALIVE_SECONDS // 60))
                return True
            else:
                log.dual_error(t('llm_warmup_failed_http', code=response.status_code))
                return False
                
        except Exception as e:
            log.dual_error(t('llm_warmup_failed', error=str(e)))
            return False
    
    def set_keep_alive(self, seconds: int = MODEL_KEEP_ALIVE_SECONDS):
        """
        è®¾ç½®æ¨¡å‹ä¿æ´»æ—¶é—´
        
        Args:
            seconds: ä¿æ´»ç§’æ•°
        """
        if self.provider != LLMProvider.OLLAMA:
            return
        
        try:
            # å‘é€ä¿æ´»è¯·æ±‚
            response = self.session.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': '',  # ç©ºprompt
                    'stream': False,
                    'keep_alive': f'{seconds}s',
                    'options': {'num_predict': 0}
                },
                timeout=10
            )
            
            if response.status_code == 200:
                log.dual_success(t('llm_keepalive_set', minutes=seconds // 60))
                
        except Exception as e:
            log.warning(t('llm_keepalive_failed', error=str(e)))
    
    def unload_model(self):
        """ç«‹å³å¸è½½æ¨¡å‹ï¼ˆé‡Šæ”¾æ˜¾å­˜/å†…å­˜ï¼‰"""
        if self.provider != LLMProvider.OLLAMA:
            return
        
        try:
            response = self.session.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': '',
                    'stream': False,
                    'keep_alive': '0s'  # ç«‹å³å¸è½½
                },
                timeout=10
            )
            
            if response.status_code == 200:
                self.is_warmed_up = False
                log.dual_success(t('llm_model_unloaded', model=self.model))
                
        except Exception as e:
            log.warning(t('llm_unload_failed', error=str(e)))
    
    def cleanup(self):
        """æ¸…ç†èµ„æºï¼ˆä¿å­˜ç¼“å­˜ã€å…³é—­ HTTP ä¼šè¯ï¼‰"""
        # 1. ä¿å­˜ç¼“å­˜
        try:
            self._save_cache()
            log.info("ğŸ’¾ LLM cache saved")
        except Exception as e:
            log.warning(f"Failed to save cache: {e}")
        
        # 2. ä¿å­˜å­¦ä¹ æ•°æ®
        try:
            if hasattr(self, 'evaluator'):
                self.evaluator._save_learning_data()
                log.info("ğŸ’¾ Learning data saved")
        except Exception as e:
            log.warning(f"Failed to save learning data: {e}")
        
        # 3. å…³é—­ HTTP ä¼šè¯
        try:
            if hasattr(self, 'session'):
                self.session.close()
                log.info("ğŸ”Œ HTTP session closed")
        except Exception as e:
            log.warning(f"Failed to close session: {e}")

    def _get_api_key(self) -> Optional[str]:
        """ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥"""
        if self.provider == LLMProvider.OPENAI:
            return os.getenv('OPENAI_API_KEY')
        elif self.provider == LLMProvider.AZURE_OPENAI:
            return os.getenv('AZURE_OPENAI_API_KEY')
        return None
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        if self.provider == LLMProvider.OLLAMA:
            # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
            if not self._check_ollama_service():
                log.error(t('llm_ollama_not_running'))
        elif self.provider == LLMProvider.AZURE_OPENAI:
            if not self.api_key:
                log.error(t('llm_api_key_missing', provider='AZURE_OPENAI'))
            if not self.azure_endpoint:
                log.error(t('llm_azure_endpoint_missing'))
        elif self.provider == LLMProvider.OPENAI:
            if not self.api_key:
                log.error(t('llm_api_key_missing', provider=self.provider.value.upper()))
    
    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            response = self.session.get('http://localhost:11434/api/tags', timeout=5)
            return response.status_code == 200
        except (requests.RequestException, ConnectionError, TimeoutError):
            return False
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        # ç›´æ¥åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶ï¼Œç¡®ä¿ä»é›¶å¼€å§‹
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    loaded_cache = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆå¿…é¡»åŒ…å« classified_by å­—æ®µï¼‰
                if loaded_cache:
                    first_entry = next(iter(loaded_cache.values()), None)
                    if first_entry and 'classified_by' not in first_entry:
                        # æ—§æ ¼å¼ç¼“å­˜ï¼Œåˆ é™¤æ–‡ä»¶
                        os.remove(self.cache_file)
                        log.warning(t('llm_cache_outdated'))
                        self.cache = {}
                        return
                
                self.cache = loaded_cache
                log.dual_data(t('llm_cache_loaded', count=len(self.cache)))
            except Exception as e:
                print(f"âš ï¸ Cache load failed: {e}")
                # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                try:
                    os.remove(self.cache_file)
                except (OSError, PermissionError):
                    pass
                self.cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        if not self.enable_cache:
            return
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(t('llm_cache_save_failed', error=str(e)))
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜ï¼ˆæ–‡ä»¶å’Œå†…å­˜ï¼‰"""
        # æ¸…é™¤å†…å­˜ç¼“å­˜
        self.cache.clear()
        self.stats['cache_hits'] = 0
        
        # åˆ é™¤ç¼“å­˜æ–‡ä»¶
        if os.path.exists(self.cache_file):
            try:
                os.remove(self.cache_file)
                log.dual_success("âœ… LLMåˆ†ç±»ç¼“å­˜å·²æ¸…é™¤ï¼ˆæ–‡ä»¶+å†…å­˜ï¼‰")
            except Exception as e:
                log.error(f"âŒ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def _get_content_hash(self, item: Dict) -> str:
        """è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œï¼ˆä¸å«æ¨¡å‹ä¿¡æ¯ï¼‰"""
        content = f"{item.get('title', '')}|{item.get('summary', '')}|{item.get('source', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_current_model_identifier(self) -> str:
        """è·å–å½“å‰æ¨¡å‹çš„æ ‡è¯†ç¬¦"""
        return f"{self.provider.value}/{self.model}"
    
    def _get_cache_key(self, item: Dict) -> str:
        """
        è·å–ç¼“å­˜keyï¼ˆå¤åˆkeyï¼šå†…å®¹å“ˆå¸Œ + æ¨¡å‹æ ‡è¯†ï¼‰
        
        å¤šæ¨¡å‹ç¼“å­˜å…±å­˜è®¾è®¡ï¼š
        - åŒä¸€å†…å®¹è¢«ä¸åŒæ¨¡å‹åˆ†ç±»åï¼Œä¼šæœ‰å¤šæ¡ç¼“å­˜è®°å½•
        - åˆ‡æ¢æ¨¡å‹æ—¶ï¼Œå¦‚æœè¯¥æ¨¡å‹ä¹‹å‰åˆ†ç±»è¿‡åŒæ ·çš„å†…å®¹ï¼Œå¯ä»¥ç›´æ¥ä»ç¼“å­˜è¯»å–
        - ä¸ä¼šè¦†ç›–å…¶ä»–æ¨¡å‹çš„åˆ†ç±»ç»“æœ
        
        Args:
            item: å†…å®¹é¡¹
            
        Returns:
            æ ¼å¼: "{content_hash}:{model_identifier}"
        """
        content_hash = self._get_content_hash(item)
        model_id = self._get_current_model_identifier()
        return f"{content_hash}:{model_id}"
    
    def _build_classification_prompt(self, item: Dict) -> str:
        """æ„å»ºåˆ†ç±»æç¤ºè¯ï¼ˆä¸æ‰¹é‡åˆ†ç±»è§„åˆ™ç»Ÿä¸€ï¼‰"""
        title = item.get('title', '')[:100]
        summary = item.get('summary', item.get('description', ''))[:300]
        source = item.get('source', '')
        url = item.get('url', '')
        
        # æ£€æµ‹ URL ç±»å‹æç¤ºï¼ˆä¸æ‰¹é‡åˆ†ç±»ç»Ÿä¸€ï¼‰
        url_hints = []
        if 'arxiv.org' in url or '/paper/' in url:
            url_hints.append("[PAPER]")
        if '/podcast/' in url or '/podcasts/' in url:
            url_hints.append("[PODCAST]")
        if '/blog/' in url:
            url_hints.append("[BLOG]")
        
        url_hint_text = f" {' '.join(url_hints)}" if url_hints else ""
        
        prompt = f"""Classify this AI news item. Output ONLY valid JSON.

Title: {title}{url_hint_text}
Summary: {summary}
Source: {source}

IMPORTANT: Use ONLY these exact values for content_type:
- research: Academic papers, scientific studies, technical reports from arxiv/conferences
- product: Product launches, new features, version releases, API announcements
- market: Funding news, investments, company analysis, industry competition (NO quote markers)
- developer: Tools, frameworks, models, open source projects, technical tutorials
- leader: Person's statement with quote markers â˜…â˜…â˜… HIGHEST PRIORITY â˜…â˜…â˜…
- community: Forum discussions, social media trends, community events

â˜…â˜…â˜… LEADER CLASSIFICATION - HIGHEST PRIORITY â˜…â˜…â˜…
Quote marker words (ANY of these in title = leader):
  English: says, said, warns, predicts, believes, stated, told, claims, according to
  Chinese: è¯´, è¡¨ç¤º, ç§°, è®¤ä¸º, æŒ‡å‡º, é€éœ², é¢„æµ‹, è­¦å‘Š

Decision flow:
1. Title contains ANY quote marker word â†’ "leader" (even if about company news)
2. Title format "Person Name: ..." or "äººåï¼š..." â†’ "leader"
3. About famous person but NO quote marker â†’ "market"

Examples:
- "Elon Musk says AI will change work" â†’ leader âœ“ (has "says")
- "Sam Altman predicts AGI timeline" â†’ leader âœ“ (has "predicts")
- "OpenAI CEO warns about AI risks" â†’ leader âœ“ (has "warns")
- "OpenAI launches new model" â†’ product (no quote marker)
- "OpenAI faces competition from Google" â†’ market (no quote marker)

Other rules:
- Items marked [PAPER] â†’ research
- Items marked [PODCAST] â†’ community

â˜…â˜…â˜… AI RELEVANCE SCORING (ai_relevance: 0.0-1.0) - BE STRICT â˜…â˜…â˜…
- 0.9-1.0: Core AI (LLM, deep learning, neural networks, model training, transformers)
- 0.7-0.9: Primary AI (ChatGPT, Claude, Midjourney, AI company core business)
- 0.5-0.7: Partial AI (tech news with explicit AI/ML mention as main topic)
- 0.2-0.5: Weak AI (smart devices without ML, automation without AI)
- 0.0-0.2: Non-AI (completely unrelated to AI)

â˜…â˜…â˜… NON-AI EXAMPLES (score 0.0-0.3) â˜…â˜…â˜…
- Car news: EVs, digital keys, smart cockpit (unless ML-based)
- Hardware: CPUs, GPUs, storage, displays, phones (unless AI chips)
- Software: Regular app updates, OS features (unless AI-powered)
- Gaming: Unless AI NPCs, AI content creation
- Finance: Unless AI company funding or AI trading
- Communication tech: NFC, Bluetooth, UWB, 5G = NOT AI

tech_fields options: LLM, Computer Vision, NLP, Robotics, AI Safety, MLOps, Multimodal, Audio/Speech, Healthcare AI, General AI

Output format (strict JSON, no extra text):
{{"content_type": "TYPE", "confidence": 0.8, "ai_relevance": 0.85, "tech_fields": ["FIELD"], "reasoning": "brief reason"}}"""
        
        return prompt
    
    def _build_batch_prompt(self, items: List[Dict]) -> str:
        """æ„å»ºæ‰¹é‡åˆ†ç±»æç¤ºè¯ï¼ˆä¸å•æ¡åˆ†ç±»è§„åˆ™ç»Ÿä¸€ï¼‰"""
        items_text = []
        for i, item in enumerate(items, 1):
            title = item.get('title', '')[:80]
            summary = item.get('summary', item.get('description', ''))[:120]
            source = item.get('source', '')[:20]
            url = item.get('url', '')
            
            # æ£€æµ‹ URL ç±»å‹æç¤ºï¼ˆä¸å•æ¡åˆ†ç±»ç»Ÿä¸€ï¼‰
            url_type = ""
            if 'arxiv.org' in url or '/paper/' in url:
                url_type = " [PAPER]"
            elif '/podcast/' in url or '/podcasts/' in url:
                url_type = " [PODCAST]"
            elif '/blog/' in url:
                url_type = " [BLOG]"
            
            items_text.append(f"[{i}] {title}{url_type}\n    Summary: {summary}\n    Source: {source}")
        
        all_items = "\n".join(items_text)
        
        prompt = f"""Classify these {len(items)} AI news items. Output ONLY valid JSON, one per line.

Items to classify:
{all_items}

IMPORTANT: Use ONLY these exact values for content_type:
- research: Academic papers, scientific studies, technical reports from arxiv/conferences
- product: Product launches, new features, version releases, API announcements  
- market: Funding news, investments, company analysis, industry competition (NO quote markers)
- developer: Tools, frameworks, models, open source projects, technical tutorials
- leader: Person's statement with quote markers â˜…â˜…â˜… HIGHEST PRIORITY â˜…â˜…â˜…
- community: Forum discussions, social media trends, community events

â˜…â˜…â˜… LEADER CLASSIFICATION - HIGHEST PRIORITY â˜…â˜…â˜…
Quote marker words (ANY of these in title = leader):
  English: says, said, warns, predicts, believes, stated, told, claims, according to
  Chinese: è¯´, è¡¨ç¤º, ç§°, è®¤ä¸º, æŒ‡å‡º, é€éœ², é¢„æµ‹, è­¦å‘Š

Decision flow:
1. Title contains ANY quote marker word â†’ "leader" (even if about company news)
2. Title format "Person Name: ..." or "äººåï¼š..." â†’ "leader"
3. About famous person but NO quote marker â†’ "market"

â˜… LEADER EXAMPLES (classify as leader) â˜…
- "Elon Musk says AI will make work optional" â†’ leader âœ“ (has "says")
- "Sam Altman predicts AGI in 5 years" â†’ leader âœ“ (has "predicts")
- "Jensen Huang believes AI approach human intelligence" â†’ leader âœ“ (has "believes")
- "OpenAI CEO warns about AI risks" â†’ leader âœ“ (has "warns")
- "Bill Gates: AI will transform education" â†’ leader âœ“ (has "Name:" format)

â˜… MARKET EXAMPLES (NO quote markers) â˜…
- "OpenAI declares code red as Google threatens" â†’ market (no quote marker)
- "Sam Altman eyes rocket company" â†’ market (no quote marker)
- "Elon Musk's Grok AI launches new feature" â†’ product (no quote marker)

Other rules:
- Items marked [PAPER] â†’ research
- Items marked [PODCAST] â†’ community
- Items marked [BLOG] â†’ market or developer (based on content)

â˜…â˜…â˜… AI RELEVANCE SCORING (ai_relevance: 0.0-1.0) - BE STRICT â˜…â˜…â˜…
- 0.9-1.0: Core AI (LLM, deep learning, neural networks, model training, transformers, diffusion models)
- 0.7-0.9: Primary AI (ChatGPT, Claude, Midjourney, AI company core business, ML applications)
- 0.5-0.7: Partial AI (tech news with explicit AI/ML mention as main topic)
- 0.2-0.5: Weak AI (smart devices without ML, automation without AI)
- 0.0-0.2: Non-AI (completely unrelated to AI)

â˜…â˜…â˜… NON-AI EXAMPLES (score 0.0-0.3) â˜…â˜…â˜…
- Car news: EVs, digital keys, smart cockpit, autonomous driving sensors (unless ML-based)
- Hardware: CPUs, GPUs (unless AI chips like TPU/NPU), storage, displays, phones
- Software: Regular app updates, OS features (unless AI-powered)
- Gaming: Unless AI NPCs, procedural generation, AI content creation
- Finance: Unless AI company funding or AI trading algorithms
- Communication tech: NFC, Bluetooth, UWB, 5G = NOT AI
- IoT/Smart home: Unless using ML for predictions/recommendations

tech_fields options: LLM, Computer Vision, NLP, Robotics, AI Safety, MLOps, Multimodal, Audio/Speech, Healthcare AI, General AI

Output format - EXACTLY {len(items)} lines starting from id=1:
{{"id":1,"content_type":"TYPE","confidence":0.8,"ai_relevance":0.85,"tech_fields":["FIELD"]}}
{{"id":2,"content_type":"TYPE","confidence":0.8,"ai_relevance":0.85,"tech_fields":["FIELD"]}}
...continue until id={len(items)}

START from id=1, classify ALL {len(items)} items:"""
        
        return prompt
    
    def _call_ollama(self, prompt: str, is_batch: bool = False) -> Tuple[Optional[str], Optional[FallbackReason]]:
        """è°ƒç”¨Ollama API
        
        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. å¯¹äº Qwen3 ç­‰æ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼Œä½¿ç”¨ Chat API + think=false è·å¾—å¿«é€Ÿå“åº”
        2. å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨ Generate API å¹¶è§£æ thinking å­—æ®µï¼ˆå¦‚æœ‰ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼ˆéœ€è¦æ›´å¤šè¾“å‡ºtokensï¼‰
            
        Returns:
            (response_text, error_reason): å“åº”æ–‡æœ¬å’Œé”™è¯¯åŸå› ï¼ˆæˆåŠŸæ—¶ä¸ºNoneï¼‰
        """
        try:
            import requests
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼ˆå¦‚ Qwen3ï¼‰
            use_chat_api = 'qwen3' in self.model.lower()
            
            # ä¿æ´»æ—¶é—´è®¾ç½®
            keep_alive = f'{MODEL_KEEP_ALIVE_SECONDS}s'
            
            if use_chat_api:
                # ä½¿ç”¨ Chat API + think=false å…³é—­æ€è€ƒæ¨¡å¼ï¼Œå¤§å¹…æå‡é€Ÿåº¦
                # æ ¹æ®GPUæ£€æµ‹ç»“æœè‡ªé€‚åº”é…ç½®
                options = self._get_ollama_options(is_batch=is_batch)
                
                response = self.session.post(
                    'http://localhost:11434/api/chat',
                    json={
                        'model': self.model,
                        'messages': [
                            {'role': 'system', 'content': LLM_SYSTEM_PROMPT},
                            {'role': 'user', 'content': prompt}
                        ],
                        'stream': False,
                        'think': False,  # å…³é—­æ€è€ƒæ¨¡å¼ï¼ˆQwen3ä¸“ç”¨ï¼‰
                        'keep_alive': keep_alive,  # ä¿æŒæ¨¡å‹æ´»è·ƒ
                        'options': options
                    },
                    timeout=OLLAMA_BATCH_REQUEST_TIMEOUT if is_batch else OLLAMA_SINGLE_REQUEST_TIMEOUT
                )
                
                if response.status_code == 200:
                    result = response.json()
                    message = result.get('message', {})
                    content = message.get('content', '')
                    return (content, None) if content else (None, FallbackReason.INVALID_RESPONSE)
                elif response.status_code == 429:
                    return (None, FallbackReason.RATE_LIMIT)
                else:
                    return (None, FallbackReason.API_ERROR)
            else:
                # ä½¿ç”¨ Generate APIï¼ˆé€‚ç”¨äºå…¶ä»–æ¨¡å‹ï¼‰
                options = self._get_ollama_options()
                
                # Generate API ä¸æ”¯æŒ system messageï¼Œå°†å…¶æ·»åŠ åˆ° prompt å‰é¢
                full_prompt = f"System: {LLM_SYSTEM_PROMPT}\n\nUser: {prompt}"
                
                response = self.session.post(
                    'http://localhost:11434/api/generate',
                    json={
                        'model': self.model,
                        'prompt': full_prompt,
                        'stream': False,
                        'keep_alive': keep_alive,  # ä¿æŒæ¨¡å‹æ´»è·ƒ
                        'options': options
                    },
                    timeout=OLLAMA_SINGLE_REQUEST_TIMEOUT + 30  # Generate API é€šå¸¸æ›´æ…¢
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # éƒ¨åˆ†æ¨¡å‹ä½¿ç”¨ thinking å­—æ®µå­˜å‚¨æ€è€ƒè¿‡ç¨‹
                    response_text = result.get('response', '')
                    thinking_text = result.get('thinking', '')
                    
                    # å¦‚æœ response ä¸ºç©ºä½† thinking æœ‰å†…å®¹ï¼Œä» thinking ä¸­æå–
                    if not response_text.strip() and thinking_text:
                        return (thinking_text, None)
                    
                    return (response_text, None) if response_text else (None, FallbackReason.INVALID_RESPONSE)
                elif response.status_code == 429:
                    return (None, FallbackReason.RATE_LIMIT)
                else:
                    return (None, FallbackReason.API_ERROR)
            
        except requests.exceptions.Timeout:
            log.dual_warning("â±ï¸ Ollamaè¯·æ±‚è¶…æ—¶ - å¯èƒ½åŸå› : 1)æ¨¡å‹æ­£åœ¨é¦–æ¬¡åŠ è½½ 2)æ˜¾å­˜/å†…å­˜ä¸è¶³ 3)æ‰¹é‡è¯·æ±‚è¿‡å¤§")
            return (None, FallbackReason.TIMEOUT)
        except requests.exceptions.ConnectionError:
            log.dual_error("ğŸ”Œ æ— æ³•è¿æ¥OllamaæœåŠ¡ - è¯·ç¡®è®¤ ollama serve æ­£åœ¨è¿è¡Œ")
            return (None, FallbackReason.CONNECTION_ERROR)
        except Exception as e:
            log.error(t('llm_ollama_failed', error=str(e)))
            return (None, FallbackReason.MODEL_ERROR)
    
    def _get_ollama_options(self, is_batch: bool = False) -> Dict:
        """è·å–Ollamaæ¨ç†é€‰é¡¹ï¼ˆæ ¹æ®GPUè‡ªé€‚åº”é…ç½®ï¼‰
        
        Args:
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼ˆéœ€è¦æ›´å¤šè¾“å‡ºtokensï¼‰
        """
        if self.ollama_options:
            num_predict = self.ollama_options.num_predict_batch if is_batch else self.ollama_options.num_predict
            return {
                'temperature': self.ollama_options.temperature,
                'num_predict': num_predict,
                'num_ctx': self.ollama_options.num_ctx,
                'num_thread': self.ollama_options.num_thread,
                'num_gpu': self.ollama_options.num_gpu
            }
        else:
            # é»˜è®¤é…ç½®
            return {
                'temperature': 0.1,
                'num_predict': 500 if is_batch else 200,
                'num_ctx': 2048,
                'num_thread': 4
            }
    
    def _call_openai(self, prompt: str, is_batch: bool = False) -> Tuple[Optional[str], Optional[FallbackReason]]:
        """è°ƒç”¨OpenAI API
        
        Args:
            prompt: æç¤ºè¯
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼ˆéœ€è¦æ›´å¤šè¾“å‡ºtokensï¼‰
            
        Returns:
            (response, error_reason): å“åº”æ–‡æœ¬å’Œé”™è¯¯åŸå› 
        """
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            
            # æ‰¹é‡æ¨¡å¼éœ€è¦æ›´å¤šè¾“å‡º tokens
            max_tokens = 2000 if is_batch else 300
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": LLM_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            return (content, None) if content else (None, FallbackReason.INVALID_RESPONSE)
            
        except Exception as e:
            log.error(t('llm_openai_failed', error=str(e)))
            return (None, FallbackReason.API_ERROR)
    
    def _call_azure_openai(self, prompt: str, is_batch: bool = False) -> Tuple[Optional[str], Optional[FallbackReason]]:
        """è°ƒç”¨Azure OpenAI API
        
        Azure OpenAI ä½¿ç”¨éƒ¨ç½²åç§°è€Œéæ¨¡å‹åç§°ï¼Œ
        éœ€è¦é…ç½® endpoint å’Œ api_version
        
        Args:
            prompt: æç¤ºè¯
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼ˆéœ€è¦æ›´å¤šè¾“å‡ºtokensï¼‰
            
        Returns:
            (response, error_reason): å“åº”æ–‡æœ¬å’Œé”™è¯¯åŸå› 
        """
        try:
            from openai import AzureOpenAI
            
            # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è·å– Azure ç‰¹å®šå‚æ•°
            endpoint = self.azure_endpoint or os.getenv('AZURE_OPENAI_ENDPOINT')
            api_version = self.azure_api_version or os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')
            
            if not endpoint:
                log.error(t('llm_azure_endpoint_missing'))
                return (None, FallbackReason.API_ERROR)
            
            # ç¡®ä¿ endpoint ä»¥ / ç»“å°¾
            if not endpoint.endswith('/'):
                endpoint = endpoint + '/'
            
            client = AzureOpenAI(
                api_key=self.api_key,
                api_version=api_version,
                azure_endpoint=endpoint
            )
            
            # æ‰¹é‡æ¨¡å¼éœ€è¦æ›´å¤šè¾“å‡º tokens
            max_tokens = 2000 if is_batch else 300
            
            # Azure OpenAI ä½¿ç”¨ deployment_name ä½œä¸º model å‚æ•°
            # æ³¨æ„: self.model å¿…é¡»æ˜¯ Azure ä¸­çš„éƒ¨ç½²åç§°ï¼Œä¸æ˜¯æ¨¡å‹åç§°
            response = client.chat.completions.create(
                model=self.model,  # è¿™é‡Œæ˜¯ Azure éƒ¨ç½²åç§°ï¼Œä¸æ˜¯æ¨¡å‹åå¦‚ gpt-4o
                messages=[
                    {"role": "system", "content": LLM_SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            return (content, None) if content else (None, FallbackReason.INVALID_RESPONSE)
            
        except Exception as e:
            error_msg = str(e)
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯æç¤º
            if '404' in error_msg and 'Resource not found' in error_msg:
                log.error(f"Azure OpenAI 404é”™è¯¯ - è¯·æ£€æŸ¥:")
                log.error(f"  1. Deployment Name '{self.model}' æ˜¯å¦æ­£ç¡® (å¿…é¡»æ˜¯Azureä¸­åˆ›å»ºçš„éƒ¨ç½²åç§°)")
                log.error(f"  2. Endpoint '{self.azure_endpoint}' æ˜¯å¦æ­£ç¡®")
                log.error(f"  3. API Version '{self.azure_api_version}' æ˜¯å¦æ”¯æŒ")
            log.error(t('llm_azure_openai_failed', error=error_msg))
            return (None, FallbackReason.API_ERROR)
    
    def _call_llm(self, prompt: str, is_batch: bool = False) -> Tuple[Optional[str], Optional[FallbackReason]]:
        """è°ƒç”¨LLMï¼ˆæ ¹æ®æä¾›å•†é€‰æ‹©ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼
            
        Returns:
            (response, error_reason): å“åº”æ–‡æœ¬å’Œé”™è¯¯åŸå› ï¼ˆæˆåŠŸæ—¶ä¸ºNoneï¼‰
        """
        if self.provider == LLMProvider.OLLAMA:
            return self._call_ollama(prompt, is_batch=is_batch)
        elif self.provider == LLMProvider.OPENAI:
            return self._call_openai(prompt, is_batch=is_batch)
        elif self.provider == LLMProvider.AZURE_OPENAI:
            return self._call_azure_openai(prompt, is_batch=is_batch)
        return (None, FallbackReason.MODEL_ERROR)
    
    def _parse_llm_response(self, response: str) -> Optional[Dict]:
        """è§£æLLMå“åº”
        
        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. JSONæ ¼å¼: {"content_type": "xxx", ...}
        2. çº¯æ–‡æœ¬æ ¼å¼: ç›´æ¥è¿”å›ç±»åˆ«åç§°ï¼ˆç”¨äº thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
        """
        if not response:
            log.warning("LLMå“åº”ä¸ºç©º")
            return None
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            response = response.strip()
            
            # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if 'content_type' in result:
                    # è§„èŒƒåŒ–å­—æ®µ
                    result['content_type'] = result['content_type'].lower()
                    result['confidence'] = float(result.get('confidence', 0.8))
                    result['ai_relevance'] = float(result.get('ai_relevance', 0.7))  # é»˜è®¤0.7ï¼ˆå‡è®¾å¤§éƒ¨åˆ†é‡‡é›†å†…å®¹æ˜¯AIç›¸å…³ï¼‰
                    result['tech_fields'] = result.get('tech_fields', ['General AI'])
                    result['is_verified'] = result.get('is_verified', True)
                    result['reasoning'] = result.get('reasoning', '')
                    
                    return result
                else:
                    log.warning(f"JSONå“åº”ç¼ºå°‘content_typeå­—æ®µ: {json_str[:100]}")
            
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«ï¼ˆæ”¯æŒ thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
            return self._extract_category_from_text(response)
            
        except json.JSONDecodeError as e:
            log.warning(f"JSONè§£æé”™è¯¯: {e}, å“åº”å†…å®¹: {response[:200] if response else 'None'}")
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«
            return self._extract_category_from_text(response)
        except Exception as e:
            log.warning(t('llm_parse_failed', error=str(e)))
        
        return None
    
    def _extract_category_from_text(self, text: str) -> Optional[Dict]:
        """ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­æå–ç±»åˆ«
        
        ç”¨äºå¤„ç†ä½¿ç”¨ thinking æ¨¡å¼çš„æ¨¡å‹è¾“å‡º
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        # å®šä¹‰ç±»åˆ«å…³é”®è¯æ˜ å°„
        category_keywords = {
            'llm': ['llm', 'large language model', 'language model', 'gpt', 'chatgpt', 'claude', 'gemini'],
            'product': ['product', 'launch', 'release', 'announce', 'new feature'],
            'research': ['research', 'paper', 'study', 'academic', 'arxiv', 'conference'],
            'industry': ['industry', 'business', 'company', 'enterprise', 'market'],
            'tools': ['tool', 'framework', 'library', 'sdk', 'api'],
            'ethics': ['ethics', 'safety', 'regulation', 'policy', 'bias', 'fairness'],
            'vision': ['vision', 'image', 'video', 'computer vision', 'visual'],
            'robotics': ['robot', 'robotics', 'autonomous', 'embodied'],
        }
        
        # é¦–å…ˆæ£€æŸ¥æ–‡æœ¬æœ«å°¾æ˜¯å¦æœ‰æ˜ç¡®çš„ç±»åˆ«åç§°ï¼ˆR1æ¨¡å‹é€šå¸¸åœ¨æœ€åç»™å‡ºç­”æ¡ˆï¼‰
        lines = text.strip().split('\n')
        last_lines = ' '.join(lines[-3:]) if len(lines) >= 3 else text
        
        for category in category_keywords.keys():
            # æ£€æŸ¥æœ€åå‡ è¡Œæ˜¯å¦åŒ…å«æ˜ç¡®çš„ç±»åˆ«åç§°
            if category in last_lines.lower().split():
                return {
                    'content_type': category,
                    'confidence': 0.85,
                    'ai_relevance': 0.7,  # é»˜è®¤ä¸­ç­‰ç›¸å…³æ€§
                    'tech_fields': ['General AI'],
                    'is_verified': True,
                    'reasoning': 'Extracted from LLM thinking output'
                }
        
        # å¦‚æœæœ«å°¾æ²¡æœ‰æ˜ç¡®ç±»åˆ«ï¼Œç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        category_scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            return {
                'content_type': best_category,
                'confidence': min(0.7 + category_scores[best_category] * 0.05, 0.9),
                'ai_relevance': 0.7,  # é»˜è®¤ä¸­ç­‰ç›¸å…³æ€§
                'tech_fields': ['General AI'],
                'is_verified': True,
                'reasoning': f'Inferred from text analysis (score: {category_scores[best_category]})'
            }
        
        return None
    
    def classify_item(self, item: Dict, use_cache: bool = True) -> Dict:
        """
        ä½¿ç”¨LLMåˆ†ç±»å•ä¸ªå†…å®¹é¡¹
        
        Args:
            item: å†…å®¹é¡¹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹ï¼ŒåŒ…å«:
            - content_type: å†…å®¹ç±»å‹
            - confidence: åˆ†ç±»ç½®ä¿¡åº¦
            - importance: å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°
            - importance_breakdown: é‡è¦æ€§åˆ†æ•°æ˜ç»†
            - importance_level: é‡è¦æ€§ç­‰çº§
        """
        self.stats['total_calls'] += 1
        
        classified = item.copy()
        cache_key = self._get_cache_key(item)  # å¤åˆkeyï¼šcontent_hash:model_id
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆå¤šæ¨¡å‹å…±å­˜ï¼škeyå·²åŒ…å«æ¨¡å‹ä¿¡æ¯ï¼Œæ— éœ€é¢å¤–éªŒè¯ï¼‰
        if use_cache and self.enable_cache and cache_key in self.cache:
            cached = self.cache[cache_key]
            self.stats['cache_hits'] += 1
            classified.update(cached)
            classified['from_cache'] = True
            
            # é‡è¦æ€§åˆ†æ•°å§‹ç»ˆé‡æ–°è®¡ç®—ï¼ˆå› ä¸ºæ—¶æ•ˆæ€§ä¼šéšæ—¶é—´å˜åŒ–ï¼‰
            importance, breakdown = self.importance_evaluator.calculate_importance(
                item,
                {'content_type': classified.get('content_type', 'news'), 
                 'confidence': classified.get('confidence', 0.5),
                 'ai_relevance': classified.get('ai_relevance', 0.7)}
            )
            classified['importance'] = importance
            classified['importance_breakdown'] = breakdown
            level, _ = self.importance_evaluator.get_importance_level(importance)
            classified['importance_level'] = level
            
            return classified
        
        # æ£€æŸ¥æ–­è·¯å™¨
        if not self.fallback_strategy.should_use_llm():
            self.stats['fallback_calls'] += 1
            log.dual_warning("âš ï¸ Circuit breaker open, using rule classifier")
            classified = self.rule_classifier.classify_item(item)
            classified['classified_by'] = 'rule:circuit_breaker'
            return classified
        
        # è°ƒç”¨LLMï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        prompt = self._build_classification_prompt(item)
        response, error_reason = self._call_llm_with_fallback(prompt, item)
        
        if response:
            result = self._parse_llm_response(response)
            
            if result:
                self.stats['llm_calls'] += 1
                self.fallback_strategy.record_success()  # è®°å½•æˆåŠŸ
            
            # æ›´æ–°åˆ†ç±»ç»“æœ
            classified['content_type'] = result['content_type']
            classified['confidence'] = result['confidence']
            classified['ai_relevance'] = result.get('ai_relevance', 0.7)  # AIç›¸å…³æ€§è¯„åˆ†
            classified['tech_categories'] = result['tech_fields']
            classified['is_verified'] = result['is_verified']
            classified['llm_reasoning'] = result['reasoning']
            classified['classified_by'] = f"llm:{self.provider.value}/{self.model}"
            classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # ä½¿ç”¨è§„åˆ™åˆ†ç±»å™¨è¡¥å……åœ°åŒºä¿¡æ¯
            classified['region'] = self.rule_classifier.classify_region(item)
            
            # è®¡ç®—å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°å™¨ï¼‰
            importance, importance_breakdown = self.importance_evaluator.calculate_importance(
                item,
                {'content_type': result['content_type'], 'confidence': result['confidence'],
                 'ai_relevance': classified['ai_relevance']}  # ä¼ å…¥AIç›¸å…³æ€§
            )
            classified['importance'] = importance
            classified['importance_breakdown'] = importance_breakdown
            level, _ = self.importance_evaluator.get_importance_level(importance)
            classified['importance_level'] = level
            
            # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå¤šæ¨¡å‹å…±å­˜ï¼šä¸åŒæ¨¡å‹çš„ç»“æœåˆ†åˆ«å­˜å‚¨ï¼‰
            if self.enable_cache:
                self.cache[cache_key] = {
                    'content_type': classified['content_type'],
                    'confidence': classified['confidence'],
                    'ai_relevance': classified['ai_relevance'],  # ç¼“å­˜AIç›¸å…³æ€§
                    'tech_categories': classified['tech_categories'],
                    'is_verified': classified['is_verified'],
                    'llm_reasoning': classified['llm_reasoning'],
                    'region': classified['region'],
                    'classified_by': classified['classified_by']
                    # æ³¨æ„ï¼šimportance ä¸ç¼“å­˜ï¼Œå› ä¸ºæ—¶æ•ˆæ€§åˆ†æ•°éœ€è¦å®æ—¶è®¡ç®—
                }
        # LLMå¤±è´¥ï¼Œæ ¹æ®é”™è¯¯åŸå› æ‰§è¡Œæ™ºèƒ½é™çº§
        self.stats['fallback_calls'] += 1
        self.stats['errors'] += 1
        
        if error_reason:
            self.fallback_strategy.record_error(error_reason)
        
        fallback_reason = error_reason.value if error_reason else 'unknown_error'
        self.stats['fallback_details'].append({
            'title': item.get('title', '')[:50],
            'source': item.get('source', ''),
            'reason': fallback_reason,
            'mode': 'single'
        })
        
        log.warning(t('llm_fallback', title=item.get('title', '')[:30]) + f" ({fallback_reason})")
        classified = self.rule_classifier.classify_item(item)
        classified['classified_by'] = f'rule:fallback:{fallback_reason}'
        
        return classified
    
    def _call_llm_with_fallback(self, prompt: str, item: Dict, max_retries: int = 1) -> Tuple[Optional[str], Optional[FallbackReason]]:
        """å¸¦æ™ºèƒ½é™çº§çš„ LLM è°ƒç”¨
        
        Args:
            prompt: æç¤ºè¯
            item: å†…å®¹é¡¹ï¼ˆç”¨äºé™çº§ç­–ç•¥åˆ¤æ–­ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            (response, error_reason): å“åº”å’Œé”™è¯¯åŸå› 
        """
        for attempt in range(max_retries + 1):
            response, error_reason = self._call_llm(prompt)
            
            if response:
                return (response, None)
            
            if error_reason:
                action = self.fallback_strategy.get_fallback_action(error_reason, item)
                
                if action == 'retry' and attempt < max_retries:
                    log.dual_info(f"ğŸ”„ Retrying LLM call (attempt {attempt + 2}/{max_retries + 1})...")
                    continue
                elif action == 'quick':
                    # å¿«é€Ÿé™çº§ï¼šè¿”å›é”™è¯¯ï¼Œå¤–éƒ¨ä½¿ç”¨ç®€åŒ–è§„åˆ™
                    return (None, error_reason)
                else:
                    # å®Œæ•´é™çº§ï¼šè¿”å›é”™è¯¯ï¼Œå¤–éƒ¨ä½¿ç”¨å®Œæ•´è§„åˆ™åˆ†ç±»
                    return (None, error_reason)
            
            # æœªçŸ¥é”™è¯¯ï¼Œä¸é‡è¯•
            break
        
        return (None, error_reason or FallbackReason.MODEL_ERROR)
    
    def classify_batch(self, items: List[Dict], show_progress: bool = True, 
                       use_batch_api: bool = True) -> List[Dict]:
        """
        æ‰¹é‡åˆ†ç±»ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼‰
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            use_batch_api: æ˜¯å¦ä½¿ç”¨æ‰¹é‡APIï¼ˆä¸€æ¬¡è°ƒç”¨åˆ†ç±»å¤šæ¡ï¼Œæ›´å¿«ï¼‰
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹åˆ—è¡¨
        """
        total = len(items)
        
        # é‡ç½®ç»Ÿè®¡æ•°æ®ï¼ˆæ¯æ¬¡æ‰¹é‡åˆ†ç±»å¼€å§‹æ—¶æ¸…é›¶ï¼Œç¡®ä¿ç»Ÿè®¡åæ˜ å½“å‰æ‰¹æ¬¡ï¼‰
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'llm_calls': 0,
            'fallback_calls': 0,
            'errors': 0,
            'fallback_details': []
        }
        
        # å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œåˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„å†…å®¹
        # å¤šæ¨¡å‹å…±å­˜è®¾è®¡ï¼šç¼“å­˜keyå·²åŒ…å«æ¨¡å‹ä¿¡æ¯ï¼Œæ— éœ€é¢å¤–éªŒè¯æ¨¡å‹åŒ¹é…
        cached_items = []
        uncached_items = []
        uncached_indices = []
        
        current_model = self._get_current_model_identifier()
        
        for i, item in enumerate(items):
            cache_key = self._get_cache_key(item)  # å¤åˆkeyï¼šcontent_hash:model_id
            if self.enable_cache and cache_key in self.cache:
                cached = self.cache[cache_key]
                self.stats['cache_hits'] += 1
                self.stats['total_calls'] += 1
                classified = item.copy()
                classified.update(cached)
                classified['from_cache'] = True
                # ç¡®ä¿æœ‰ classified_by å­—æ®µï¼ˆå…¼å®¹æ—§ç¼“å­˜ï¼‰
                if 'classified_by' not in classified:
                    classified['classified_by'] = f'llm:cached:{current_model}'
                
                # é‡è¦æ€§åˆ†æ•°å§‹ç»ˆé‡æ–°è®¡ç®—ï¼ˆå› ä¸ºæ—¶æ•ˆæ€§ä¼šéšæ—¶é—´å˜åŒ–ï¼‰
                importance, breakdown = self.importance_evaluator.calculate_importance(
                    item,
                    {'content_type': classified.get('content_type', 'news'), 
                     'confidence': classified.get('confidence', 0.5),
                     'ai_relevance': classified.get('ai_relevance', 0.7)}
                )
                classified['importance'] = importance
                classified['importance_breakdown'] = breakdown
                level, _ = self.importance_evaluator.get_importance_level(importance)
                classified['importance_level'] = level
                
                cached_items.append((i, classified))
            else:
                uncached_items.append(item)
                uncached_indices.append(i)
        
        cached_count = len(cached_items)
        uncached_count = len(uncached_items)
        
        log.dual_start(t('llm_batch_start', total=total))
        log.dual_ai(t('llm_batch_info', provider=self.provider.value, model=self.model))
        log.dual_data(t('llm_batch_cache', workers=self.max_workers, cached=cached_count, total=total))
        
        if uncached_count == 0:
            log.dual_success(t('llm_all_cached'))
            cached_items.sort(key=lambda x: x[0])
            return [item for _, item in cached_items]
        
        # æ¨¡å‹é¢„çƒ­ï¼ˆä»…Ollamaä¸”æœªé¢„çƒ­æ—¶ï¼‰
        if self.provider == LLMProvider.OLLAMA and not self.is_warmed_up:
            self.warmup_model()
        
        start_time = time.time()
        classified_uncached = []
        
        # é€‰æ‹©åˆ†ç±»ç­–ç•¥
        # Ollama å’Œ Azure OpenAI éƒ½æ”¯æŒæ‰¹é‡æ¨¡å¼
        if use_batch_api and self.batch_size > 1 and self.provider in (LLMProvider.OLLAMA, LLMProvider.AZURE_OPENAI):
            # æ‰¹é‡APIæ¨¡å¼ï¼šä¸€æ¬¡è°ƒç”¨åˆ†ç±»å¤šæ¡ï¼ˆæ›´å¿«ã€æ›´çœæˆæœ¬ï¼‰
            log.dual_info(t('llm_batch_mode', batch_size=self.batch_size))
            classified_uncached = self._classify_batch_mode(uncached_items, uncached_indices, show_progress)
        else:
            # å¹¶å‘å•æ¡æ¨¡å¼
            log.dual_info(t('llm_concurrent_mode'))
            classified_uncached = self._classify_concurrent_mode(uncached_items, uncached_indices, show_progress)
        
        # åˆå¹¶ç»“æœ
        all_items = cached_items + classified_uncached
        all_items.sort(key=lambda x: x[0])
        result = [item for _, item in all_items]
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache()
        
        # ç»Ÿè®¡
        elapsed = time.time() - start_time
        self._print_stats(elapsed)
        
        return result
    
    def _classify_batch_mode(self, items: List[Dict], indices: List[int], 
                             show_progress: bool) -> List[Tuple[int, Dict]]:
        """æ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼šä¸€æ¬¡LLMè°ƒç”¨å¤„ç†å¤šæ¡å†…å®¹"""
        results = []
        total = len(items)
        total_batches = (total + self.batch_size - 1) // self.batch_size
        
        # åˆ†æ‰¹å¤„ç†
        batch_num = 0
        for batch_start in range(0, total, self.batch_size):
            batch_num += 1
            batch_start_time = time.time()
            batch_end = min(batch_start + self.batch_size, total)
            batch_items = items[batch_start:batch_end]
            batch_indices = indices[batch_start:batch_end]
            
            # æ„å»ºæ‰¹é‡prompt
            prompt = self._build_batch_prompt(batch_items)
            response, error_reason = self._call_llm(prompt, is_batch=True)  # ä½¿ç”¨æ‰¹é‡æ¨¡å¼ï¼ˆæ›´å¤šè¾“å‡ºtokensï¼‰
            batch_results = self._parse_batch_response(response, len(batch_items)) if response else None
            
            # å¤„ç†ç»“æœ
            retry_items = []  # æ”¶é›†éœ€è¦é‡è¯•çš„æ¡ç›®
            retry_indices = []
            
            for i, (item, idx) in enumerate(zip(batch_items, batch_indices)):
                self.stats['total_calls'] += 1
                classified = item.copy()
                
                if batch_results and i < len(batch_results) and batch_results[i]:
                    result = batch_results[i]
                    self.stats['llm_calls'] += 1
                    
                    classified['content_type'] = result.get('content_type', 'market')
                    classified['confidence'] = result.get('confidence', 0.7)
                    classified['ai_relevance'] = result.get('ai_relevance', 0.7)  # AIç›¸å…³æ€§è¯„åˆ†
                    classified['tech_categories'] = result.get('tech_fields', ['General AI'])
                    classified['is_verified'] = result.get('is_verified', True)
                    classified['llm_reasoning'] = result.get('reasoning', '')
                    classified['classified_by'] = f"llm:batch:{self.provider.value}/{self.model}"
                    classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    classified['region'] = self.rule_classifier.classify_region(item)
                    
                    # è®¡ç®—å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°
                    importance, importance_breakdown = self.importance_evaluator.calculate_importance(
                        item,
                        {'content_type': classified['content_type'], 'confidence': classified['confidence'],
                         'ai_relevance': classified['ai_relevance']}  # ä¼ å…¥AIç›¸å…³æ€§
                    )
                    classified['importance'] = importance
                    classified['importance_breakdown'] = importance_breakdown
                    level, _ = self.importance_evaluator.get_importance_level(importance)
                    classified['importance_level'] = level
                    
                    # ç¼“å­˜ï¼ˆå¤šæ¨¡å‹å…±å­˜ï¼šä¸ä¿å­˜importanceï¼Œå› ä¸ºæ—¶æ•ˆæ€§ä¼šå˜åŒ–ï¼‰
                    cache_key = self._get_cache_key(item)
                    if self.enable_cache:
                        self.cache[cache_key] = {
                            'content_type': classified['content_type'],
                            'confidence': classified['confidence'],
                            'ai_relevance': classified['ai_relevance'],  # ç¼“å­˜AIç›¸å…³æ€§
                            'tech_categories': classified['tech_categories'],
                            'is_verified': classified.get('is_verified', True),
                            'llm_reasoning': classified.get('llm_reasoning', ''),
                            'region': classified['region'],
                            'classified_by': classified['classified_by']
                        }
                    results.append((idx, classified))
                else:
                    # æ‰¹é‡è§£æå¤±è´¥ï¼ŒåŠ å…¥é‡è¯•åˆ—è¡¨
                    retry_items.append(item)
                    retry_indices.append(idx)
            
            # å¯¹æ‰¹é‡å¤±è´¥çš„æ¡ç›®è¿›è¡Œå•æ¡é‡è¯•
            if retry_items:
                log.warning(t('llm_batch_retry', count=len(retry_items)))
                for item, idx in zip(retry_items, retry_indices):
                    # å°è¯•å•æ¡ LLM åˆ†ç±»
                    retry_result = self._single_classify_with_llm(item)
                    
                    if retry_result:
                        # å•æ¡é‡è¯•æˆåŠŸ
                        self.stats['llm_calls'] += 1
                        classified = item.copy()
                        classified['content_type'] = retry_result.get('content_type', 'market')
                        classified['confidence'] = retry_result.get('confidence', 0.7)
                        classified['ai_relevance'] = retry_result.get('ai_relevance', 0.7)  # AIç›¸å…³æ€§è¯„åˆ†
                        classified['tech_categories'] = retry_result.get('tech_fields', ['General AI'])
                        classified['is_verified'] = retry_result.get('is_verified', True)
                        classified['llm_reasoning'] = retry_result.get('reasoning', '')
                        classified['classified_by'] = f"llm:retry:{self.provider.value}/{self.model}"
                        classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        classified['region'] = self.rule_classifier.classify_region(item)
                        
                        # è®¡ç®—å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°
                        importance, importance_breakdown = self.importance_evaluator.calculate_importance(
                            item,
                            {'content_type': classified['content_type'], 'confidence': classified['confidence'],
                             'ai_relevance': classified['ai_relevance']}  # ä¼ å…¥AIç›¸å…³æ€§
                        )
                        classified['importance'] = importance
                        classified['importance_breakdown'] = importance_breakdown
                        level, _ = self.importance_evaluator.get_importance_level(importance)
                        classified['importance_level'] = level
                        
                        # ç¼“å­˜ï¼ˆå¤šæ¨¡å‹å…±å­˜ï¼šä¸ä¿å­˜importanceï¼‰
                        cache_key = self._get_cache_key(item)
                        if self.enable_cache:
                            self.cache[cache_key] = {
                                'content_type': classified['content_type'],
                                'confidence': classified['confidence'],
                                'ai_relevance': classified['ai_relevance'],  # ç¼“å­˜AIç›¸å…³æ€§
                                'tech_categories': classified['tech_categories'],
                                'is_verified': classified.get('is_verified', True),
                                'llm_reasoning': classified.get('llm_reasoning', ''),
                                'region': classified['region'],
                                'classified_by': classified['classified_by']
                            }
                        results.append((idx, classified))
                        log.dual_success(t('llm_retry_success', title=item.get('title', '')[:40]))
                    else:
                        # å•æ¡é‡è¯•ä¹Ÿå¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»ï¼ˆè§„åˆ™åˆ†ç±»å·²å†…ç½®é‡è¦æ€§è®¡ç®—ï¼‰
                        self.stats['fallback_calls'] += 1
                        self.stats['fallback_details'].append({
                            'title': item.get('title', '')[:50],
                            'source': item.get('source', ''),
                            'reason': 'æ‰¹é‡+å•æ¡é‡è¯•å‡å¤±è´¥',
                            'mode': 'batch_retry'
                        })
                        classified = self.rule_classifier.classify_item(item)
                        classified['classified_by'] = 'rule:batch_fallback'
                        results.append((idx, classified))
            
            if show_progress:
                completed = min(batch_end, total)
                batch_time = time.time() - batch_start_time
                remaining_batches = total_batches - batch_num
                estimated_remaining = batch_time * remaining_batches
                
                if remaining_batches > 0:
                    log.dual_info(t('llm_progress_eta', completed=completed, total=total, percent=int(completed/total*100), time=f"{batch_time:.1f}", eta=f"{estimated_remaining:.0f}"))
                else:
                    log.dual_info(t('llm_progress', completed=completed, total=total, percent=int(completed/total*100)) + f" | {t('llm_stats_time', time=f'{batch_time:.1f}')}")
        
        return results
    
    def _classify_concurrent_mode(self, items: List[Dict], indices: List[int],
                                   show_progress: bool) -> List[Tuple[int, Dict]]:
        """å¹¶å‘å•æ¡åˆ†ç±»æ¨¡å¼"""
        results = []
        total = len(items)
        last_progress_time = time.time()
        last_progress_count = 0
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.classify_item, item): (i, idx) 
                      for i, (item, idx) in enumerate(zip(items, indices))}
            
            completed = 0
            for future in as_completed(futures):
                try:
                    result = future.result()
                    _, idx = futures[future]
                    results.append((idx, result))
                    completed += 1
                    
                    if show_progress and completed % 5 == 0:
                        current_time = time.time()
                        interval_time = current_time - last_progress_time
                        interval_count = completed - last_progress_count
                        
                        if interval_count > 0 and interval_time > 0:
                            rate = interval_count / interval_time  # æ¡/ç§’
                            remaining = total - completed
                            estimated_remaining = remaining / rate if rate > 0 else 0
                            log.dual_info(t('llm_progress_rate', completed=completed, total=total, percent=int(completed/total*100), rate=f"{rate:.1f}", eta=f"{estimated_remaining:.0f}"))
                        else:
                            log.dual_info(t('llm_progress', completed=completed, total=total, percent=int(completed/total*100)))
                        
                        last_progress_time = current_time
                        last_progress_count = completed
                        
                except Exception as e:
                    log.error(t('llm_task_failed', error=str(e)))
                    self.stats['errors'] += 1
        
        return results
    
    def _single_classify_with_llm(self, item: Dict) -> Optional[Dict]:
        """å¯¹å•æ¡å†…å®¹è¿›è¡Œ LLM åˆ†ç±»ï¼ˆç”¨äºæ‰¹é‡å¤±è´¥åçš„é‡è¯•ï¼‰
        
        Args:
            item: è¦åˆ†ç±»çš„æ¡ç›®
            
        Returns:
            åˆ†ç±»ç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            prompt = self._build_classification_prompt(item)
            response, error_reason = self._call_llm(prompt, is_batch=False)
            
            if not response:
                return None
            
            # è§£æå•æ¡å“åº”
            result = self._parse_single_response(response)
            return result
            
        except Exception as e:
            log.warning(f"Single retry failed: {str(e)}")
            return None
    
    def _parse_single_response(self, response: str) -> Optional[Dict]:
        """è§£æå•æ¡åˆ†ç±»å“åº”"""
        if not response:
            return None
        
        try:
            # é¢„å¤„ç†ï¼šç§»é™¤markdownä»£ç å—æ ‡è®°
            cleaned = response.strip()
            if '```json' in cleaned:
                import re
                json_blocks = re.findall(r'```json?\s*(.*?)\s*```', cleaned, re.DOTALL)
                if json_blocks:
                    cleaned = json_blocks[0]
            elif '```' in cleaned:
                import re
                cleaned = re.sub(r'```\w*\s*', '', cleaned)
                cleaned = cleaned.replace('```', '')
            
            # æŸ¥æ‰¾JSONå¯¹è±¡
            start = cleaned.find('{')
            end = cleaned.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = cleaned[start:end]
                # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                json_str = json_str.replace('ï¼Œ', ',')
                json_str = json_str.replace('"', '"').replace('"', '"')
                
                obj = json.loads(json_str)
                
                content_type = obj.get('content_type', obj.get('type', 'market'))
                if isinstance(content_type, str):
                    content_type = content_type.lower().strip()
                    if '(' in content_type:
                        content_type = content_type.split('(')[0].strip()
                
                # éªŒè¯content_type
                valid_types = ['research', 'product', 'market', 'developer', 'leader', 'community']
                if content_type not in valid_types:
                    type_mapping = {
                        'paper': 'research', 'academic': 'research', 'study': 'research',
                        'release': 'product', 'launch': 'product', 'tool': 'developer',
                        'news': 'market', 'funding': 'market', 'opinion': 'leader',
                        'discussion': 'community', 'trend': 'community'
                    }
                    content_type = type_mapping.get(content_type, 'market')
                
                return {
                    'content_type': content_type,
                    'confidence': float(obj.get('confidence', 0.7)),
                    'tech_fields': obj.get('tech_fields', obj.get('fields', ['General AI'])),
                    'is_verified': obj.get('is_verified', True),
                    'reasoning': obj.get('reasoning', obj.get('reason', ''))
                }
                
        except Exception as e:
            log.warning(f"Parse single response failed: {str(e)}")
        
        return None

    def _parse_batch_response(self, response: str, expected_count: int) -> List[Optional[Dict]]:
        """è§£ææ‰¹é‡åˆ†ç±»å“åº”ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æ”¯æŒå¤šç§LLMè¾“å‡ºæ ¼å¼ï¼š
        1. æ¯è¡Œä¸€ä¸ªJSON
        2. Markdownä»£ç å—åŒ…è£¹çš„JSON
        3. å¸¦åºå·çš„JSONåˆ—è¡¨
        4. JSONæ•°ç»„æ ¼å¼
        """
        results = [None] * expected_count
        
        if not response:
            return results
        
        try:
            # é¢„å¤„ç†ï¼šç§»é™¤markdownä»£ç å—æ ‡è®°
            cleaned = response.strip()
            if '```json' in cleaned:
                # æå–```json ... ```ä¹‹é—´çš„å†…å®¹
                import re
                json_blocks = re.findall(r'```json?\s*(.*?)\s*```', cleaned, re.DOTALL)
                if json_blocks:
                    cleaned = '\n'.join(json_blocks)
            elif '```' in cleaned:
                # ç§»é™¤é€šç”¨ä»£ç å—æ ‡è®°
                cleaned = re.sub(r'```\w*\s*', '', cleaned)
                cleaned = cleaned.replace('```', '')
            
            json_objects = []
            
            # æ–¹æ³•1ï¼šå°è¯•è§£æä¸ºJSONæ•°ç»„
            try:
                arr = json.loads(cleaned)
                if isinstance(arr, list):
                    json_objects = arr
            except json.JSONDecodeError:
                pass
            
            # æ–¹æ³•2ï¼šæŒ‰è¡Œè§£æJSON
            if not json_objects:
                lines = cleaned.split('\n')
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # ç§»é™¤è¡Œé¦–çš„åºå·ï¼ˆå¦‚ "1." æˆ– "[1]"ï¼‰
                    import re
                    line = re.sub(r'^[\[\(]?\d+[\]\)\.:]?\s*', '', line)
                    
                    # æŸ¥æ‰¾JSONå¯¹è±¡
                    start = line.find('{')
                    end = line.rfind('}') + 1
                    
                    if start >= 0 and end > start:
                        json_str = line[start:end]
                        # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                        json_str = json_str.replace('ï¼Œ', ',')  # ä¸­æ–‡é€—å·
                        json_str = json_str.replace('"', '"').replace('"', '"')  # ä¸­æ–‡å¼•å·
                        json_str = json_str.replace(''', "'").replace(''', "'")
                        
                        try:
                            obj = json.loads(json_str)
                            json_objects.append(obj)
                        except json.JSONDecodeError:
                            # å°è¯•æ›´å®½æ¾çš„è§£æ
                            try:
                                # ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®å
                                fixed = re.sub(r'(\w+):', r'"\1":', json_str)
                                obj = json.loads(fixed)
                                json_objects.append(obj)
                            except (json.JSONDecodeError, ValueError):
                                continue
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ‰€æœ‰ç‹¬ç«‹çš„JSONå¯¹è±¡ï¼ˆå¤„ç†å¤šä¸ªJSONåœ¨ä¸€è¡Œçš„æƒ…å†µï¼‰
            if not json_objects:
                import re
                pattern = r'\{[^{}]*\}'
                matches = re.findall(pattern, cleaned)
                for match in matches:
                    try:
                        obj = json.loads(match)
                        json_objects.append(obj)
                    except (json.JSONDecodeError, ValueError):
                        continue
            
            # åŒ¹é…ç»“æœåˆ°å¯¹åº”ç´¢å¼•
            for i, obj in enumerate(json_objects):
                # ä¼˜å…ˆä½¿ç”¨idå­—æ®µ
                idx = obj.get('id')
                if idx is not None:
                    idx = int(idx) - 1  # idä»1å¼€å§‹
                else:
                    # æ²¡æœ‰idå­—æ®µï¼ŒæŒ‰é¡ºåºåˆ†é…
                    idx = i
                
                if 0 <= idx < expected_count and results[idx] is None:
                    content_type = obj.get('content_type', obj.get('type', 'market'))
                    if isinstance(content_type, str):
                        content_type = content_type.lower().strip()
                        # å¤„ç†å¸¦æ‹¬å·çš„æ ¼å¼ï¼Œå¦‚ "developer(tools/models)" -> "developer"
                        if '(' in content_type:
                            content_type = content_type.split('(')[0].strip()
                    
                    # éªŒè¯content_typeæ˜¯å¦æœ‰æ•ˆ
                    valid_types = ['research', 'product', 'market', 'developer', 'leader', 'community']
                    if content_type not in valid_types:
                        # å°è¯•æ˜ å°„
                        type_mapping = {
                            'paper': 'research', 'academic': 'research', 'study': 'research',
                            'papers': 'research', 'releases': 'product',
                            'release': 'product', 'launch': 'product', 'tool': 'developer',
                            'tools': 'developer', 'models': 'developer', 'tools/models': 'developer',
                            'news': 'market', 'funding': 'market', 'investment': 'market',
                            'funding/news': 'market',
                            'opinion': 'leader', 'quote': 'leader', 'insight': 'leader',
                            'opinions': 'leader',
                            'discussion': 'community', 'trend': 'community', 'trends': 'community'
                        }
                        content_type = type_mapping.get(content_type, 'market')
                    
                    results[idx] = {
                        'content_type': content_type,
                        'confidence': float(obj.get('confidence', 0.7)),
                        'ai_relevance': float(obj.get('ai_relevance', 0.7)),  # AIç›¸å…³æ€§è¯„åˆ†
                        'tech_fields': obj.get('tech_fields', obj.get('fields', ['General AI'])),
                        'is_verified': obj.get('is_verified', True),
                        'reasoning': obj.get('reasoning', obj.get('reason', ''))
                    }
            
        except Exception as e:
            log.warning(t('llm_batch_parse_failed', error=str(e)))
        
        return results
    
    def _print_stats(self, elapsed: float):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        log.dual_info(t('llm_stats'))
        log.dual_info(t('llm_stats_total', count=self.stats['total_calls']))
        log.dual_info(t('llm_stats_cached', count=self.stats['cache_hits']) + f" ({self.stats['cache_hits']/max(1,self.stats['total_calls']):.0%})")
        log.dual_info(f"   LLM: {self.stats['llm_calls']}")
        log.dual_info(f"   Fallback: {self.stats['fallback_calls']}")
        log.dual_info(t('llm_stats_failed', count=self.stats['errors']))
        log.dual_info(t('llm_stats_time', time=f"{elapsed:.1f}s"))
        
        if self.stats['llm_calls'] > 0:
            avg_time = elapsed / self.stats['llm_calls']
            log.dual_info(t('llm_stats_avg', time=f"{avg_time:.1f}"))
        
        # æ˜¾ç¤ºé™çº§è¯¦æƒ…
        if self.stats['fallback_details']:
            log.dual_warning(t('llm_fallback_details', count=len(self.stats['fallback_details'])))
            for i, detail in enumerate(self.stats['fallback_details'], 1):
                log.dual_info(t('llm_fallback_item', i=i, mode=detail['mode'], title=detail['title']))
                log.dual_info(t('llm_fallback_source', source=detail['source'], reason=detail['reason']))
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        log.dual_success(t('llm_cache_cleared'))


def select_llm_provider() -> Tuple[str, str]:
    """
    äº¤äº’å¼é€‰æ‹©LLMæä¾›å•†å’Œæ¨¡å‹
    
    Returns:
        (provider, model)
    """
    print("\n" + "="*60)
    print(t('llm_select_provider'))
    print("="*60)
    
    print(t('llm_available_providers'))
    print(t('llm_provider_ollama'))
    print(t('llm_provider_openai'))
    print("  3. Azure OpenAI - ä¼ä¸šçº§äº‘æœåŠ¡ (éœ€è¦Azureè®¢é˜…)")
    
    prompt = "Select provider (1-3) [default: 1]: " if get_language() == 'en' else "è¯·é€‰æ‹©æä¾›å•† (1-3) [é»˜è®¤: 1]: "
    provider_choice = input(f"\n{prompt}").strip() or '1'
    
    provider_map = {'1': 'ollama', '2': 'openai', '3': 'azure_openai'}
    provider = provider_map.get(provider_choice, 'ollama')
    
    # é€‰æ‹©æ¨¡å‹
    print(t('llm_available_models_for', provider=provider.upper()))
    
    provider_enum = LLMProvider(provider)
    models = AVAILABLE_MODELS.get(provider_enum, {})
    
    model_list = list(models.keys())
    for i, (model_id, info) in enumerate(models.items(), 1):
        recommended = " â­" if i == 1 else ""
        print(f"  {i}. {info['name']}{recommended}")
        print(f"     {info['description']}")
    
    model_prompt = f"Select model (1-{len(model_list)}) [default: 1]: " if get_language() == 'en' else f"è¯·é€‰æ‹©æ¨¡å‹ (1-{len(model_list)}) [é»˜è®¤: 1]: "
    model_choice = input(f"\n{model_prompt}").strip() or '1'
    
    try:
        model_idx = int(model_choice) - 1
        model = model_list[model_idx] if 0 <= model_idx < len(model_list) else model_list[0]
    except (ValueError, IndexError):
        model = model_list[0]
    
    log.success(t('llm_selected', provider=provider, model=model))
    
    return provider, model


def get_available_ollama_models() -> List[str]:
    """è·å–æœ¬åœ°å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
    try:
        import requests
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
    except (requests.RequestException, ConnectionError, TimeoutError, json.JSONDecodeError):
        pass
    return []


def check_ollama_status() -> Dict:
    """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«æ›´å¤šè¯Šæ–­ä¿¡æ¯ï¼‰"""
    result = {
        'running': False,
        'models': [],
        'recommended': None,
        'loaded_models': [],  # å½“å‰å·²åŠ è½½åˆ°å†…å­˜çš„æ¨¡å‹
        'gpu_info': None
    }
    
    try:
        import requests
        
        # 1. æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            result['running'] = True
            data = response.json()
            result['models'] = [model['name'] for model in data.get('models', [])]
            
            # æ¨èæ¨¡å‹ä¼˜å…ˆçº§
            preferred = ['qwen3:8b', 'llama3.2:3b', 'mistral:7b']
            for model in preferred:
                if model in result['models']:
                    result['recommended'] = model
                    break
            
            if not result['recommended'] and result['models']:
                result['recommended'] = result['models'][0]
        
        # 2. æ£€æŸ¥å½“å‰å·²åŠ è½½çš„æ¨¡å‹ï¼ˆollama ps ç­‰æ•ˆï¼‰
        try:
            ps_response = requests.get('http://localhost:11434/api/ps', timeout=5)
            if ps_response.status_code == 200:
                ps_data = ps_response.json()
                loaded = ps_data.get('models', [])
                result['loaded_models'] = [m.get('name', '') for m in loaded]
                
                # å¦‚æœæœ‰æ¨¡å‹å·²åŠ è½½ï¼Œä¼˜å…ˆæ¨èå·²åŠ è½½çš„æ¨¡å‹ï¼ˆé¿å…é‡æ–°åŠ è½½ï¼‰
                for loaded_model in result['loaded_models']:
                    if loaded_model in result['models']:
                        result['recommended'] = loaded_model
                        result['model_preloaded'] = True
                        break
        except Exception:
            pass  # ps API å¯èƒ½ä¸å¯ç”¨ï¼Œå¿½ç•¥
        
        # 3. æ£€æµ‹GPUçŠ¶æ€
        gpu_info = detect_gpu()
        if gpu_info:
            result['gpu_info'] = {
                'available': gpu_info.available,
                'type': gpu_info.gpu_type,
                'name': gpu_info.gpu_name,
                'ollama_supported': gpu_info.ollama_gpu_supported
            }
                
    except requests.exceptions.ConnectionError:
        result['error'] = 'connection_refused'
        result['error_message'] = 'OllamaæœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ ollama serve'
    except requests.exceptions.Timeout:
        result['error'] = 'timeout'
        result['error_message'] = 'OllamaæœåŠ¡å“åº”è¶…æ—¶'
    except Exception as e:
        result['error'] = str(e)
    
    return result


# ä¾¿æ·å‡½æ•°
def create_llm_classifier(auto_select: bool = False) -> LLMClassifier:
    """
    åˆ›å»ºLLMåˆ†ç±»å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        auto_select: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
        
    Returns:
        LLMClassifierå®ä¾‹
    """
    if auto_select:
        # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollama
        status = check_ollama_status()
        if status['running'] and status['recommended']:
            return LLMClassifier(provider='ollama', model=status['recommended'])
        
        # æ£€æŸ¥OpenAI
        if os.getenv('OPENAI_API_KEY'):
            return LLMClassifier(provider='openai', model='gpt-4o-mini')
        
        # æ£€æŸ¥Azure OpenAI
        if os.getenv('AZURE_OPENAI_API_KEY') and os.getenv('AZURE_OPENAI_ENDPOINT'):
            return LLMClassifier(provider='azure_openai', model='gpt-4o-mini')
        
        log.error(t('llm_no_service'))
        return None
    else:
        # äº¤äº’å¼é€‰æ‹©
        provider, model = select_llm_provider()
        return LLMClassifier(provider=provider, model=model)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("="*60)
    print(t('llm_test_title'))
    print("="*60)
    
    # æ£€æŸ¥OllamaçŠ¶æ€
    status = check_ollama_status()
    status_text = t('llm_ollama_running_yes') if status['running'] else t('llm_ollama_running_no')
    log.info(t('llm_ollama_status', status=status_text), emoji="ğŸ”")
    if status['models']:
        log.info(t('llm_available_models', models=', '.join(status['models'])), emoji="ğŸ“¦")
        log.info(t('llm_recommended_model', model=status['recommended']), emoji="â­")
    
    # åˆ›å»ºåˆ†ç±»å™¨
    if status['running']:
        classifier = LLMClassifier(
            provider='ollama',
            model=status['recommended'] or 'qwen3:8b'
        )
        
        # æµ‹è¯•åˆ†ç±»
        test_item = {
            'title': 'OpenAI officially launches GPT-4o with new features',
            'summary': 'OpenAI announces the general availability of GPT-4o model with improved capabilities',
            'source': 'TechCrunch'
        }
        
        log.info(t('llm_test_content', title=test_item['title']), emoji="ğŸ§ª")
        result = classifier.classify_item(test_item)
        
        log.info(t('llm_test_result'), emoji="ğŸ“‹")
        log.info(t('llm_test_type', type=result.get('content_type')), emoji="  ")
        log.info(t('llm_test_confidence', confidence=f"{result.get('confidence', 0):.1%}"), emoji="  ")
        log.info(t('llm_test_tech', tech=result.get('tech_categories')), emoji="  ")
        log.info(t('llm_test_verified', verified=result.get('is_verified')), emoji="  ")
        log.info(t('llm_test_reasoning', reasoning=result.get('llm_reasoning')), emoji="  ")
    else:
        log.warning(t('llm_start_ollama'))

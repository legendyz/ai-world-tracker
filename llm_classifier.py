"""
LLMå¢å¼ºåˆ†ç±»å™¨ - LLM Classifier
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†ç±»

æ”¯æŒçš„æä¾›å•†:
- Ollama (æœ¬åœ°): Qwen3:8b, Qwen3:4b, Llama3.2:3b
- OpenAI: GPT-4o-mini, GPT-4o
- Anthropic: Claude-3-Haiku, Claude-3-Sonnet

åŠŸèƒ½ç‰¹æ€§:
- å¤šæä¾›å•†æ”¯æŒï¼Œçµæ´»åˆ‡æ¢
- MD5å†…å®¹ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
- è‡ªåŠ¨é™çº§åˆ°è§„åˆ™åˆ†ç±»
- å¹¶å‘å¤„ç†åŠ é€Ÿ
- è¯¦ç»†çš„åˆ†ç±»æ¨ç†
- GPUè‡ªåŠ¨æ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
"""

import os
import json
import hashlib
import time
import subprocess
import platform
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum

# å¯¼å…¥è§„åˆ™åˆ†ç±»å™¨ä½œä¸ºå¤‡ä»½
from content_classifier import ContentClassifier


class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    OLLAMA = "ollama"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class GPUInfo:
    """GPUä¿¡æ¯"""
    available: bool = False
    gpu_type: str = "none"  # nvidia, amd, apple, qualcomm, none
    gpu_name: str = ""
    vram_mb: int = 0
    driver_version: str = ""
    cuda_available: bool = False
    rocm_available: bool = False
    metal_available: bool = False
    ollama_gpu_supported: bool = False  # Ollamaæ˜¯å¦æ”¯æŒè¯¥GPU


def detect_gpu() -> GPUInfo:
    """
    æ£€æµ‹ç³»ç»ŸGPUä¿¡æ¯
    
    Returns:
        GPUInfo: GPUæ£€æµ‹ç»“æœ
    """
    info = GPUInfo()
    system = platform.system()
    
    # 1. æ£€æµ‹ NVIDIA GPU (CUDA)
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0 and result.stdout.strip():
            parts = result.stdout.strip().split(', ')
            if len(parts) >= 3:
                info.available = True
                info.gpu_type = "nvidia"
                info.gpu_name = parts[0].strip()
                info.vram_mb = int(float(parts[1].strip()))
                info.driver_version = parts[2].strip()
                info.cuda_available = True
                info.ollama_gpu_supported = True
                return info
    except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
        pass
    
    # 2. æ£€æµ‹ AMD GPU (ROCm) - ä»…Linux
    if system == "Linux":
        try:
            result = subprocess.run(['rocm-smi', '--showproductname'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and 'GPU' in result.stdout:
                info.available = True
                info.gpu_type = "amd"
                info.gpu_name = "AMD ROCm GPU"
                info.rocm_available = True
                info.ollama_gpu_supported = True
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 3. æ£€æµ‹ Apple Silicon (Metal)
    if system == "Darwin":
        try:
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                cpu_info = result.stdout.strip()
                if 'Apple' in cpu_info:
                    info.available = True
                    info.gpu_type = "apple"
                    info.gpu_name = cpu_info
                    info.metal_available = True
                    info.ollama_gpu_supported = True
                    return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 4. æ£€æµ‹ Windows æ˜¾å¡ï¼ˆå¯èƒ½æ˜¯ä¸æ”¯æŒçš„GPUï¼‰
    if system == "Windows":
        try:
            result = subprocess.run(
                ['powershell', '-Command', 
                 'Get-WmiObject Win32_VideoController | Select-Object -First 1 Name, AdapterRAM, DriverVersion | ConvertTo-Json'],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0 and result.stdout.strip():
                gpu_data = json.loads(result.stdout)
                gpu_name = gpu_data.get('Name', '')
                
                info.gpu_name = gpu_name
                info.driver_version = gpu_data.get('DriverVersion', '')
                adapter_ram = gpu_data.get('AdapterRAM', 0)
                if adapter_ram:
                    info.vram_mb = int(adapter_ram / (1024 * 1024))
                
                # åˆ¤æ–­GPUç±»å‹
                if 'NVIDIA' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "nvidia"
                    info.cuda_available = True
                    info.ollama_gpu_supported = True
                elif 'AMD' in gpu_name.upper() or 'RADEON' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "amd"
                    info.ollama_gpu_supported = False  # Windowsä¸ŠAMDä¸æ”¯æŒ
                elif 'QUALCOMM' in gpu_name.upper() or 'ADRENO' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "qualcomm"
                    info.ollama_gpu_supported = False  # Qualcommä¸æ”¯æŒ
                elif 'INTEL' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "intel"
                    info.ollama_gpu_supported = False  # Intelé›†æ˜¾ä¸æ”¯æŒ
                
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, json.JSONDecodeError, Exception):
            pass
    
    return info


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    timeout: int = 60
    max_retries: int = 2


@dataclass
class OllamaOptions:
    """Ollamaæ¨ç†é€‰é¡¹ - æ ¹æ®GPUè‡ªé€‚åº”é…ç½®"""
    temperature: float = 0.1
    num_predict: int = 200
    num_ctx: int = 2048
    num_thread: int = 4
    num_gpu: int = 0  # 0è¡¨ç¤ºè‡ªåŠ¨ï¼Œ-1è¡¨ç¤ºç¦ç”¨GPU
    
    @classmethod
    def auto_configure(cls, gpu_info: GPUInfo) -> 'OllamaOptions':
        """æ ¹æ®GPUä¿¡æ¯è‡ªåŠ¨é…ç½®æ¨ç†é€‰é¡¹"""
        options = cls()
        
        if gpu_info.ollama_gpu_supported:
            # GPUåŠ é€Ÿé…ç½®
            options.num_gpu = 999  # ä½¿ç”¨æ‰€æœ‰GPUå±‚
            options.num_ctx = 4096  # GPUå¯ä»¥å¤„ç†æ›´å¤§ä¸Šä¸‹æ–‡
            options.num_predict = 300
            options.num_thread = 4  # GPUæ¨¡å¼ä¸‹CPUçº¿ç¨‹ä¸éœ€è¦å¤ªå¤š
        else:
            # CPUæ¨¡å¼ä¼˜åŒ–é…ç½®
            options.num_gpu = 0  # ç¦ç”¨GPU
            options.num_ctx = 1024  # å‡å°‘ä¸Šä¸‹æ–‡ä»¥æå‡é€Ÿåº¦
            options.num_predict = 200
            # æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®çº¿ç¨‹
            try:
                import multiprocessing
                cpu_count = multiprocessing.cpu_count()
                options.num_thread = min(cpu_count, 8)  # æœ€å¤š8çº¿ç¨‹
            except:
                options.num_thread = 4
        
        return options


# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    LLMProvider.OLLAMA: {
        'qwen3:8b': {'name': 'Qwen3 8B', 'description': 'é˜¿é‡Œé€šä¹‰åƒé—®ï¼Œä¸­æ–‡èƒ½åŠ›å¼ºï¼Œæ¨èä½¿ç”¨'},
        'qwen3:4b': {'name': 'Qwen3 4B', 'description': 'è½»é‡æ¨¡å‹ï¼ŒCPUå‹å¥½'},
        'llama3.2:3b': {'name': 'Llama 3.2 3B', 'description': 'Metaè½»é‡æ¨¡å‹ï¼Œé€Ÿåº¦æœ€å¿«'},
        'mistral:7b': {'name': 'Mistral 7B', 'description': 'æ€§èƒ½å‡è¡¡ï¼Œè‹±æ–‡èƒ½åŠ›å¼º'},
    },
    LLMProvider.OPENAI: {
        'gpt-4o-mini': {'name': 'GPT-4o Mini', 'description': 'æ€§ä»·æ¯”é«˜ï¼Œæ¨èä½¿ç”¨'},
        'gpt-4o': {'name': 'GPT-4o', 'description': 'æœ€å¼ºæ€§èƒ½ï¼Œæˆæœ¬è¾ƒé«˜'},
        'gpt-3.5-turbo': {'name': 'GPT-3.5 Turbo', 'description': 'ç»æµå®æƒ '},
    },
    LLMProvider.ANTHROPIC: {
        'claude-3-haiku-20240307': {'name': 'Claude 3 Haiku', 'description': 'å¿«é€Ÿå“åº”ï¼Œæˆæœ¬ä½'},
        'claude-3-sonnet-20240229': {'name': 'Claude 3 Sonnet', 'description': 'å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬'},
        'claude-3-5-sonnet-20241022': {'name': 'Claude 3.5 Sonnet', 'description': 'æœ€æ–°æœ€å¼º'},
    }
}


class LLMClassifier:
    """LLMå¢å¼ºåˆ†ç±»å™¨"""
    
    def __init__(self, 
                 provider: str = 'ollama',
                 model: str = 'qwen3:8b',
                 api_key: Optional[str] = None,
                 enable_cache: bool = True,
                 max_workers: int = 3,
                 auto_detect_gpu: bool = True):
        """
        åˆå§‹åŒ–LLMåˆ†ç±»å™¨
        
        Args:
            provider: æä¾›å•† ('ollama', 'openai', 'anthropic')
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼ˆOllamaä¸éœ€è¦ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            max_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            auto_detect_gpu: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹GPUå¹¶ä¼˜åŒ–é…ç½®
        """
        self.provider = LLMProvider(provider)
        self.model = model
        self.api_key = api_key or self._get_api_key()
        self.enable_cache = enable_cache
        self.max_workers = max_workers
        
        # GPUæ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
        self.gpu_info: Optional[GPUInfo] = None
        self.ollama_options: Optional[OllamaOptions] = None
        if auto_detect_gpu and self.provider == LLMProvider.OLLAMA:
            self._setup_gpu_acceleration()
        
        # ç¼“å­˜
        self.cache: Dict[str, Dict] = {}
        self.cache_file = 'llm_classification_cache.json'
        self._load_cache()
        
        # è§„åˆ™åˆ†ç±»å™¨ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
        self.rule_classifier = ContentClassifier()
        
        # ç»Ÿè®¡
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'llm_calls': 0,
            'fallback_calls': 0,
            'errors': 0
        }
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        self._print_init_info()
    
    def _setup_gpu_acceleration(self):
        """è®¾ç½®GPUåŠ é€Ÿ"""
        self.gpu_info = detect_gpu()
        self.ollama_options = OllamaOptions.auto_configure(self.gpu_info)
    
    def _print_init_info(self):
        """æ‰“å°åˆå§‹åŒ–ä¿¡æ¯"""
        print(f"ğŸ¤– LLMåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æä¾›å•†: {self.provider.value}")
        print(f"   æ¨¡å‹: {self.model}")
        print(f"   ç¼“å­˜: {'å¯ç”¨' if self.enable_cache else 'ç¦ç”¨'}")
        
        if self.gpu_info:
            if self.gpu_info.ollama_gpu_supported:
                print(f"   ğŸš€ GPUåŠ é€Ÿ: å¯ç”¨ ({self.gpu_info.gpu_name})")
                if self.gpu_info.vram_mb:
                    print(f"   æ˜¾å­˜: {self.gpu_info.vram_mb} MB")
            else:
                print(f"   ğŸ’» è¿è¡Œæ¨¡å¼: CPU ({self.gpu_info.gpu_name or 'æœªæ£€æµ‹åˆ°GPU'})")
                if self.ollama_options:
                    print(f"   CPUçº¿ç¨‹: {self.ollama_options.num_thread}")
    
    def get_gpu_info(self) -> Optional[GPUInfo]:
        """è·å–GPUä¿¡æ¯"""
        return self.gpu_info
    
    def _get_api_key(self) -> Optional[str]:
        """ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥"""
        if self.provider == LLMProvider.OPENAI:
            return os.getenv('OPENAI_API_KEY')
        elif self.provider == LLMProvider.ANTHROPIC:
            return os.getenv('ANTHROPIC_API_KEY')
        return None
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        if self.provider == LLMProvider.OLLAMA:
            # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
            if not self._check_ollama_service():
                print("âš ï¸ OllamaæœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨: ollama serve")
        elif self.provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:
            if not self.api_key:
                print(f"âš ï¸ æœªè®¾ç½® {self.provider.value.upper()}_API_KEY ç¯å¢ƒå˜é‡")
    
    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            import requests
            response = requests.get('http://localhost:11434/api/tags', timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if not self.enable_cache:
            return
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                print(f"ğŸ“¦ å·²åŠ è½½ {len(self.cache)} æ¡ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            self.cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        if not self.enable_cache:
            return
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _get_content_hash(self, item: Dict) -> str:
        """è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œ"""
        content = f"{item.get('title', '')}|{item.get('summary', '')}|{item.get('source', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _build_classification_prompt(self, item: Dict) -> str:
        """æ„å»ºåˆ†ç±»æç¤ºè¯"""
        title = item.get('title', '')
        summary = item.get('summary', item.get('description', ''))[:500]
        source = item.get('source', '')
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªAIå†…å®¹åˆ†ç±»ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å†…å®¹å¹¶è¿›è¡Œåˆ†ç±»ã€‚

ã€å¾…åˆ†ç±»å†…å®¹ã€‘
æ ‡é¢˜: {title}
æ‘˜è¦: {summary}
æ¥æº: {source}

ã€åˆ†ç±»ä»»åŠ¡ã€‘
1. å†…å®¹ç±»å‹ (content_type): ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ª
   - research: å­¦æœ¯ç ”ç©¶ã€è®ºæ–‡ã€æŠ€æœ¯æŠ¥å‘Š
   - product: äº§å“å‘å¸ƒã€åŠŸèƒ½æ›´æ–°ã€æ–°æœåŠ¡ä¸Šçº¿
   - market: èèµ„ã€æ”¶è´­ã€è¡Œä¸šåˆ†æã€æ”¿ç­–æ³•è§„
   - developer: å¼€æºé¡¹ç›®ã€SDKã€å¼€å‘å·¥å…·ã€æ•™ç¨‹
   - leader: AIé¢†è¢–è¨€è®ºã€é‡‡è®¿ã€æ¼”è®²
   - community: ç¤¾åŒºè®¨è®ºã€çƒ­ç‚¹è¯é¢˜

2. ç½®ä¿¡åº¦ (confidence): 0.0-1.0ä¹‹é—´çš„æ•°å€¼

3. æŠ€æœ¯é¢†åŸŸ (tech_fields): å¯å¤šé€‰
   - NLP, Computer Vision, Generative AI, Reinforcement Learning, MLOps, AI Ethics, General AI

4. å†…å®¹å¯ä¿¡åº¦ (is_verified): true/falseï¼Œæ˜¯å¦ä¸ºå¯ä¿¡å†…å®¹ï¼ˆéè°£è¨€/ä¼ é—»ï¼‰

5. åˆ†ç±»ç†ç”± (reasoning): ç®€çŸ­è¯´æ˜åˆ†ç±»åŸå› ï¼ˆ20å­—ä»¥å†…ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹:
{{"content_type": "xxx", "confidence": 0.xx, "tech_fields": ["xxx"], "is_verified": true, "reasoning": "xxx"}}"""
        
        return prompt
    
    def _call_ollama(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Ollama API
        
        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. å¯¹äº Qwen3 ç­‰æ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼Œä½¿ç”¨ Chat API + think=false è·å¾—å¿«é€Ÿå“åº”
        2. å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨ Generate API å¹¶è§£æ thinking å­—æ®µï¼ˆå¦‚æœ‰ï¼‰
        """
        try:
            import requests
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼ˆå¦‚ Qwen3ï¼‰
            use_chat_api = 'qwen3' in self.model.lower()
            
            if use_chat_api:
                # ä½¿ç”¨ Chat API + think=false å…³é—­æ€è€ƒæ¨¡å¼ï¼Œå¤§å¹…æå‡é€Ÿåº¦
                # æ ¹æ®GPUæ£€æµ‹ç»“æœè‡ªé€‚åº”é…ç½®
                options = self._get_ollama_options()
                
                response = requests.post(
                    'http://localhost:11434/api/chat',
                    json={
                        'model': self.model,
                        'messages': [
                            {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå†…å®¹åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†ç±»ç»“æœã€‚'},
                            {'role': 'user', 'content': prompt}
                        ],
                        'stream': False,
                        'think': False,  # å…³é—­æ€è€ƒæ¨¡å¼
                        'options': options
                    },
                    timeout=60 if self.gpu_info and self.gpu_info.ollama_gpu_supported else 90
                )
                
                if response.status_code == 200:
                    result = response.json()
                    message = result.get('message', {})
                    return message.get('content', '')
            else:
                # ä½¿ç”¨ Generate APIï¼ˆé€‚ç”¨äºå…¶ä»–æ¨¡å‹ï¼‰
                options = self._get_ollama_options()
                
                response = requests.post(
                    'http://localhost:11434/api/generate',
                    json={
                        'model': self.model,
                        'prompt': prompt,
                        'stream': False,
                        'options': options
                    },
                    timeout=120 if self.gpu_info and self.gpu_info.ollama_gpu_supported else 180
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # éƒ¨åˆ†æ¨¡å‹ä½¿ç”¨ thinking å­—æ®µå­˜å‚¨æ€è€ƒè¿‡ç¨‹
                    response_text = result.get('response', '')
                    thinking_text = result.get('thinking', '')
                    
                    # å¦‚æœ response ä¸ºç©ºä½† thinking æœ‰å†…å®¹ï¼Œä» thinking ä¸­æå–
                    if not response_text.strip() and thinking_text:
                        return thinking_text
                    
                    return response_text
            
            print(f"âš ï¸ Ollama APIé”™è¯¯: {response.status_code}")
            return None
                
        except Exception as e:
            print(f"âš ï¸ Ollamaè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _get_ollama_options(self) -> Dict:
        """è·å–Ollamaæ¨ç†é€‰é¡¹ï¼ˆæ ¹æ®GPUè‡ªé€‚åº”é…ç½®ï¼‰"""
        if self.ollama_options:
            return {
                'temperature': self.ollama_options.temperature,
                'num_predict': self.ollama_options.num_predict,
                'num_ctx': self.ollama_options.num_ctx,
                'num_thread': self.ollama_options.num_thread,
                'num_gpu': self.ollama_options.num_gpu
            }
        else:
            # é»˜è®¤é…ç½®
            return {
                'temperature': 0.1,
                'num_predict': 200,
                'num_ctx': 1024,
                'num_thread': 4
            }
    
    def _call_openai(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨OpenAI API"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå†…å®¹åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†ç±»ç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=300
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âš ï¸ OpenAIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _call_anthropic(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Anthropic API"""
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=self.api_key)
            response = client.messages.create(
                model=self.model,
                max_tokens=300,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.content[0].text
            
        except Exception as e:
            print(f"âš ï¸ Anthropicè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _call_llm(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨LLMï¼ˆæ ¹æ®æä¾›å•†é€‰æ‹©ï¼‰"""
        if self.provider == LLMProvider.OLLAMA:
            return self._call_ollama(prompt)
        elif self.provider == LLMProvider.OPENAI:
            return self._call_openai(prompt)
        elif self.provider == LLMProvider.ANTHROPIC:
            return self._call_anthropic(prompt)
        return None
    
    def _parse_llm_response(self, response: str) -> Optional[Dict]:
        """è§£æLLMå“åº”
        
        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. JSONæ ¼å¼: {"content_type": "xxx", ...}
        2. çº¯æ–‡æœ¬æ ¼å¼: ç›´æ¥è¿”å›ç±»åˆ«åç§°ï¼ˆç”¨äº thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
        """
        if not response:
            return None
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            response = response.strip()
            
            # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if 'content_type' in result:
                    # è§„èŒƒåŒ–å­—æ®µ
                    result['content_type'] = result['content_type'].lower()
                    result['confidence'] = float(result.get('confidence', 0.8))
                    result['tech_fields'] = result.get('tech_fields', ['General AI'])
                    result['is_verified'] = result.get('is_verified', True)
                    result['reasoning'] = result.get('reasoning', '')
                    
                    return result
            
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«ï¼ˆæ”¯æŒ thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
            return self._extract_category_from_text(response)
            
        except json.JSONDecodeError as e:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«
            return self._extract_category_from_text(response)
        except Exception as e:
            print(f"âš ï¸ å“åº”è§£æå¤±è´¥: {e}")
        
        return None
    
    def _extract_category_from_text(self, text: str) -> Optional[Dict]:
        """ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­æå–ç±»åˆ«
        
        ç”¨äºå¤„ç†ä½¿ç”¨ thinking æ¨¡å¼çš„æ¨¡å‹è¾“å‡º
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        # å®šä¹‰ç±»åˆ«å…³é”®è¯æ˜ å°„
        category_keywords = {
            'llm': ['llm', 'large language model', 'language model', 'gpt', 'chatgpt', 'claude', 'gemini'],
            'product': ['product', 'launch', 'release', 'announce', 'new feature'],
            'research': ['research', 'paper', 'study', 'academic', 'arxiv', 'conference'],
            'industry': ['industry', 'business', 'company', 'enterprise', 'market'],
            'tools': ['tool', 'framework', 'library', 'sdk', 'api'],
            'ethics': ['ethics', 'safety', 'regulation', 'policy', 'bias', 'fairness'],
            'vision': ['vision', 'image', 'video', 'computer vision', 'visual'],
            'robotics': ['robot', 'robotics', 'autonomous', 'embodied'],
        }
        
        # é¦–å…ˆæ£€æŸ¥æ–‡æœ¬æœ«å°¾æ˜¯å¦æœ‰æ˜ç¡®çš„ç±»åˆ«åç§°ï¼ˆR1æ¨¡å‹é€šå¸¸åœ¨æœ€åç»™å‡ºç­”æ¡ˆï¼‰
        lines = text.strip().split('\n')
        last_lines = ' '.join(lines[-3:]) if len(lines) >= 3 else text
        
        for category in category_keywords.keys():
            # æ£€æŸ¥æœ€åå‡ è¡Œæ˜¯å¦åŒ…å«æ˜ç¡®çš„ç±»åˆ«åç§°
            if category in last_lines.lower().split():
                return {
                    'content_type': category,
                    'confidence': 0.85,
                    'tech_fields': ['General AI'],
                    'is_verified': True,
                    'reasoning': 'Extracted from LLM thinking output'
                }
        
        # å¦‚æœæœ«å°¾æ²¡æœ‰æ˜ç¡®ç±»åˆ«ï¼Œç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        category_scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            return {
                'content_type': best_category,
                'confidence': min(0.7 + category_scores[best_category] * 0.05, 0.9),
                'tech_fields': ['General AI'],
                'is_verified': True,
                'reasoning': f'Inferred from text analysis (score: {category_scores[best_category]})'
            }
        
        return None
    
    def classify_item(self, item: Dict, use_cache: bool = True) -> Dict:
        """
        ä½¿ç”¨LLMåˆ†ç±»å•ä¸ªå†…å®¹é¡¹
        
        Args:
            item: å†…å®¹é¡¹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹
        """
        self.stats['total_calls'] += 1
        
        classified = item.copy()
        content_hash = self._get_content_hash(item)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and self.enable_cache and content_hash in self.cache:
            self.stats['cache_hits'] += 1
            cached = self.cache[content_hash]
            classified.update(cached)
            classified['from_cache'] = True
            return classified
        
        # è°ƒç”¨LLM
        prompt = self._build_classification_prompt(item)
        response = self._call_llm(prompt)
        result = self._parse_llm_response(response)
        
        if result:
            self.stats['llm_calls'] += 1
            
            # æ›´æ–°åˆ†ç±»ç»“æœ
            classified['content_type'] = result['content_type']
            classified['confidence'] = result['confidence']
            classified['tech_categories'] = result['tech_fields']
            classified['is_verified'] = result['is_verified']
            classified['llm_reasoning'] = result['reasoning']
            classified['classified_by'] = f"llm:{self.provider.value}/{self.model}"
            classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # ä½¿ç”¨è§„åˆ™åˆ†ç±»å™¨è¡¥å……åœ°åŒºä¿¡æ¯
            classified['region'] = self.rule_classifier.classify_region(item)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.enable_cache:
                self.cache[content_hash] = {
                    'content_type': classified['content_type'],
                    'confidence': classified['confidence'],
                    'tech_categories': classified['tech_categories'],
                    'is_verified': classified['is_verified'],
                    'llm_reasoning': classified['llm_reasoning'],
                    'region': classified['region']
                }
        else:
            # LLMå¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»
            self.stats['fallback_calls'] += 1
            self.stats['errors'] += 1
            
            print(f"âš ï¸ LLMåˆ†ç±»å¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»: {item.get('title', '')[:30]}...")
            classified = self.rule_classifier.classify_item(item)
            classified['classified_by'] = 'rule:fallback'
        
        return classified
    
    def classify_batch(self, items: List[Dict], show_progress: bool = True) -> List[Dict]:
        """
        æ‰¹é‡åˆ†ç±»
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹åˆ—è¡¨
        """
        total = len(items)
        print(f"\nğŸ¤– å¼€å§‹LLMæ‰¹é‡åˆ†ç±» ({total} æ¡å†…å®¹)")
        print(f"   æä¾›å•†: {self.provider.value} | æ¨¡å‹: {self.model}")
        print(f"   å¹¶å‘æ•°: {self.max_workers} | ç¼“å­˜: {'å¯ç”¨' if self.enable_cache else 'ç¦ç”¨'}")
        
        start_time = time.time()
        classified_items = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.classify_item, item): i for i, item in enumerate(items)}
            
            completed = 0
            for future in as_completed(futures):
                try:
                    result = future.result()
                    classified_items.append((futures[future], result))
                    completed += 1
                    
                    if show_progress and completed % 5 == 0:
                        print(f"   è¿›åº¦: {completed}/{total} ({completed/total:.0%})")
                        
                except Exception as e:
                    print(f"âš ï¸ åˆ†ç±»ä»»åŠ¡å¤±è´¥: {e}")
                    self.stats['errors'] += 1
        
        # æŒ‰åŸé¡ºåºæ’åº
        classified_items.sort(key=lambda x: x[0])
        classified_items = [item for _, item in classified_items]
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache()
        
        # ç»Ÿè®¡
        elapsed = time.time() - start_time
        self._print_stats(elapsed)
        
        return classified_items
    
    def _print_stats(self, elapsed: float):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚: {self.stats['total_calls']}")
        print(f"   ç¼“å­˜å‘½ä¸­: {self.stats['cache_hits']} ({self.stats['cache_hits']/max(1,self.stats['total_calls']):.0%})")
        print(f"   LLMè°ƒç”¨: {self.stats['llm_calls']}")
        print(f"   è§„åˆ™é™çº§: {self.stats['fallback_calls']}")
        print(f"   é”™è¯¯æ•°: {self.stats['errors']}")
        print(f"   è€—æ—¶: {elapsed:.1f}ç§’")
        
        if self.stats['llm_calls'] > 0:
            avg_time = elapsed / self.stats['llm_calls']
            print(f"   å¹³å‡æ¯æ¡: {avg_time:.1f}ç§’")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        print("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")


def select_llm_provider() -> Tuple[str, str]:
    """
    äº¤äº’å¼é€‰æ‹©LLMæä¾›å•†å’Œæ¨¡å‹
    
    Returns:
        (provider, model)
    """
    print("\n" + "="*60)
    print("ğŸ¤– é€‰æ‹©LLMæä¾›å•†")
    print("="*60)
    
    print("\nå¯ç”¨çš„æä¾›å•†:")
    print("  1. Ollama (æœ¬åœ°å…è´¹) â­ æ¨è")
    print("  2. OpenAI (éœ€è¦APIå¯†é’¥)")
    print("  3. Anthropic (éœ€è¦APIå¯†é’¥)")
    
    provider_choice = input("\nè¯·é€‰æ‹©æä¾›å•† (1-3) [é»˜è®¤: 1]: ").strip() or '1'
    
    provider_map = {'1': 'ollama', '2': 'openai', '3': 'anthropic'}
    provider = provider_map.get(provider_choice, 'ollama')
    
    # é€‰æ‹©æ¨¡å‹
    print(f"\nå¯ç”¨çš„ {provider.upper()} æ¨¡å‹:")
    
    provider_enum = LLMProvider(provider)
    models = AVAILABLE_MODELS.get(provider_enum, {})
    
    model_list = list(models.keys())
    for i, (model_id, info) in enumerate(models.items(), 1):
        recommended = " â­" if i == 1 else ""
        print(f"  {i}. {info['name']}{recommended}")
        print(f"     {info['description']}")
    
    model_choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(model_list)}) [é»˜è®¤: 1]: ").strip() or '1'
    
    try:
        model_idx = int(model_choice) - 1
        model = model_list[model_idx] if 0 <= model_idx < len(model_list) else model_list[0]
    except:
        model = model_list[0]
    
    print(f"\nâœ… å·²é€‰æ‹©: {provider} / {model}")
    
    return provider, model


def get_available_ollama_models() -> List[str]:
    """è·å–æœ¬åœ°å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
    try:
        import requests
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
    except:
        pass
    return []


def check_ollama_status() -> Dict:
    """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
    result = {
        'running': False,
        'models': [],
        'recommended': None
    }
    
    try:
        import requests
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            result['running'] = True
            data = response.json()
            result['models'] = [model['name'] for model in data.get('models', [])]
            
            # æ¨èæ¨¡å‹ä¼˜å…ˆçº§
            preferred = ['qwen3:8b', 'qwen3:4b', 'llama3.2:3b', 'mistral:7b']
            for model in preferred:
                if model in result['models']:
                    result['recommended'] = model
                    break
            
            if not result['recommended'] and result['models']:
                result['recommended'] = result['models'][0]
                
    except Exception as e:
        result['error'] = str(e)
    
    return result


# ä¾¿æ·å‡½æ•°
def create_llm_classifier(auto_select: bool = False) -> LLMClassifier:
    """
    åˆ›å»ºLLMåˆ†ç±»å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        auto_select: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
        
    Returns:
        LLMClassifierå®ä¾‹
    """
    if auto_select:
        # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollama
        status = check_ollama_status()
        if status['running'] and status['recommended']:
            return LLMClassifier(provider='ollama', model=status['recommended'])
        
        # æ£€æŸ¥OpenAI
        if os.getenv('OPENAI_API_KEY'):
            return LLMClassifier(provider='openai', model='gpt-4o-mini')
        
        # æ£€æŸ¥Anthropic
        if os.getenv('ANTHROPIC_API_KEY'):
            return LLMClassifier(provider='anthropic', model='claude-3-haiku-20240307')
        
        print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„LLMæœåŠ¡ï¼Œå°†ä½¿ç”¨è§„åˆ™åˆ†ç±»")
        return None
    else:
        # äº¤äº’å¼é€‰æ‹©
        provider, model = select_llm_provider()
        return LLMClassifier(provider=provider, model=model)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("="*60)
    print("LLMåˆ†ç±»å™¨æµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥OllamaçŠ¶æ€
    status = check_ollama_status()
    print(f"\nOllamaçŠ¶æ€: {'è¿è¡Œä¸­ âœ…' if status['running'] else 'æœªè¿è¡Œ âŒ'}")
    if status['models']:
        print(f"å¯ç”¨æ¨¡å‹: {', '.join(status['models'])}")
        print(f"æ¨èæ¨¡å‹: {status['recommended']}")
    
    # åˆ›å»ºåˆ†ç±»å™¨
    if status['running']:
        classifier = LLMClassifier(
            provider='ollama',
            model=status['recommended'] or 'qwen3:8b'
        )
        
        # æµ‹è¯•åˆ†ç±»
        test_item = {
            'title': 'OpenAI officially launches GPT-4o with new features',
            'summary': 'OpenAI announces the general availability of GPT-4o model with improved capabilities',
            'source': 'TechCrunch'
        }
        
        print(f"\næµ‹è¯•å†…å®¹: {test_item['title']}")
        result = classifier.classify_item(test_item)
        
        print(f"\nåˆ†ç±»ç»“æœ:")
        print(f"  ç±»å‹: {result.get('content_type')}")
        print(f"  ç½®ä¿¡åº¦: {result.get('confidence', 0):.1%}")
        print(f"  æŠ€æœ¯é¢†åŸŸ: {result.get('tech_categories')}")
        print(f"  å¯ä¿¡: {result.get('is_verified')}")
        print(f"  ç†ç”±: {result.get('llm_reasoning')}")
    else:
        print("\nâš ï¸ è¯·å…ˆå¯åŠ¨OllamaæœåŠ¡: ollama serve")

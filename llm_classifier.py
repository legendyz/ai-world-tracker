"""
LLMå¢å¼ºåˆ†ç±»å™¨ - LLM Classifier
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†ç±»

æ”¯æŒçš„æä¾›å•†:
- Ollama (æœ¬åœ°): Qwen3:8b, Qwen3:4b, Llama3.2:3b
- OpenAI: GPT-4o-mini, GPT-4o
- Anthropic: Claude-3-Haiku, Claude-3-Sonnet

åŠŸèƒ½ç‰¹æ€§:
- å¤šæä¾›å•†æ”¯æŒï¼Œçµæ´»åˆ‡æ¢
- MD5å†…å®¹ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
- è‡ªåŠ¨é™çº§åˆ°è§„åˆ™åˆ†ç±»
- å¹¶å‘å¤„ç†åŠ é€Ÿ
- è¯¦ç»†çš„åˆ†ç±»æ¨ç†
- GPUè‡ªåŠ¨æ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
"""

import os
import json
import hashlib
import time
import subprocess
import platform
import threading
import re
import requests
import yaml
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum

# å¯¼å…¥è§„åˆ™åˆ†ç±»å™¨ä½œä¸ºå¤‡ä»½
from content_classifier import ContentClassifier
from logger import get_log_helper

# å¯¼å…¥å›½é™…åŒ–æ¨¡å—
try:
    from i18n import t, get_language
except ImportError:
    def t(key, **kwargs): return key
    def get_language(): return 'zh'

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('llm_classifier')

# åŠ è½½ç¼“å­˜ç›®å½•é…ç½®
def _get_cache_dir():
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    cache_dir = 'data/cache'
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                cache_dir = cfg.get('data', {}).get('cache_dir', cache_dir)
    except Exception:
        pass
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir

DATA_CACHE_DIR = _get_cache_dir()

# æ¨¡å‹ä¿æ´»æ—¶é—´ï¼ˆç§’ï¼‰
MODEL_KEEP_ALIVE_SECONDS = 5 * 60  # 5åˆ†é’Ÿ


class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    OLLAMA = "ollama"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class GPUInfo:
    """GPUä¿¡æ¯"""
    available: bool = False
    gpu_type: str = "none"  # nvidia, amd, apple, qualcomm, none
    gpu_name: str = ""
    vram_mb: int = 0
    driver_version: str = ""
    cuda_available: bool = False
    rocm_available: bool = False
    metal_available: bool = False
    ollama_gpu_supported: bool = False  # Ollamaæ˜¯å¦æ”¯æŒè¯¥GPU


def detect_gpu() -> GPUInfo:
    """
    æ£€æµ‹ç³»ç»ŸGPUä¿¡æ¯
    
    Returns:
        GPUInfo: GPUæ£€æµ‹ç»“æœ
    """
    info = GPUInfo()
    system = platform.system()
    
    # 1. æ£€æµ‹ NVIDIA GPU (CUDA)
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0 and result.stdout.strip():
            parts = result.stdout.strip().split(', ')
            if len(parts) >= 3:
                info.available = True
                info.gpu_type = "nvidia"
                info.gpu_name = parts[0].strip()
                info.vram_mb = int(float(parts[1].strip()))
                info.driver_version = parts[2].strip()
                info.cuda_available = True
                info.ollama_gpu_supported = True
                return info
    except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
        pass
    
    # 2. æ£€æµ‹ AMD GPU (ROCm) - ä»…Linux
    if system == "Linux":
        try:
            result = subprocess.run(['rocm-smi', '--showproductname'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and 'GPU' in result.stdout:
                info.available = True
                info.gpu_type = "amd"
                info.gpu_name = "AMD ROCm GPU"
                info.rocm_available = True
                info.ollama_gpu_supported = True
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 3. æ£€æµ‹ Apple Silicon (Metal)
    if system == "Darwin":
        try:
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                cpu_info = result.stdout.strip()
                if 'Apple' in cpu_info:
                    info.available = True
                    info.gpu_type = "apple"
                    info.gpu_name = cpu_info
                    info.metal_available = True
                    info.ollama_gpu_supported = True
                    return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 4. æ£€æµ‹ Windows æ˜¾å¡ï¼ˆå¯èƒ½æ˜¯ä¸æ”¯æŒçš„GPUï¼‰
    if system == "Windows":
        try:
            result = subprocess.run(
                ['powershell', '-Command', 
                 'Get-WmiObject Win32_VideoController | Select-Object -First 1 Name, AdapterRAM, DriverVersion | ConvertTo-Json'],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0 and result.stdout.strip():
                gpu_data = json.loads(result.stdout)
                gpu_name = gpu_data.get('Name', '')
                
                info.gpu_name = gpu_name
                info.driver_version = gpu_data.get('DriverVersion', '')
                adapter_ram = gpu_data.get('AdapterRAM', 0)
                if adapter_ram:
                    info.vram_mb = int(adapter_ram / (1024 * 1024))
                
                # åˆ¤æ–­GPUç±»å‹
                if 'NVIDIA' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "nvidia"
                    info.cuda_available = True
                    info.ollama_gpu_supported = True
                elif 'AMD' in gpu_name.upper() or 'RADEON' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "amd"
                    info.ollama_gpu_supported = False  # Windowsä¸ŠAMDä¸æ”¯æŒ
                elif 'QUALCOMM' in gpu_name.upper() or 'ADRENO' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "qualcomm"
                    info.ollama_gpu_supported = False  # Qualcommä¸æ”¯æŒ
                elif 'INTEL' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "intel"
                    info.ollama_gpu_supported = False  # Intelé›†æ˜¾ä¸æ”¯æŒ
                
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, json.JSONDecodeError, Exception):
            pass
    
    return info


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    timeout: int = 60
    max_retries: int = 2


@dataclass
class OllamaOptions:
    """Ollamaæ¨ç†é€‰é¡¹ - æ ¹æ®GPUè‡ªé€‚åº”é…ç½®"""
    temperature: float = 0.1
    num_predict: int = 200  # å•æ¡åˆ†ç±»è¾“å‡ºé•¿åº¦
    num_predict_batch: int = 500  # æ‰¹é‡åˆ†ç±»è¾“å‡ºé•¿åº¦ï¼ˆæ¯æ¡çº¦80 tokensï¼‰
    num_ctx: int = 2048
    num_thread: int = 4
    num_gpu: int = 0  # 0è¡¨ç¤ºè‡ªåŠ¨ï¼Œ-1è¡¨ç¤ºç¦ç”¨GPU
    
    @classmethod
    def auto_configure(cls, gpu_info: GPUInfo) -> 'OllamaOptions':
        """æ ¹æ®GPUä¿¡æ¯è‡ªåŠ¨é…ç½®æ¨ç†é€‰é¡¹"""
        options = cls()
        
        if gpu_info.ollama_gpu_supported:
            # GPUåŠ é€Ÿé…ç½® - ä¼˜åŒ–é€Ÿåº¦
            options.num_gpu = 999  # ä½¿ç”¨æ‰€æœ‰GPUå±‚
            options.num_ctx = 4096  # GPUå¯ä»¥å¤„ç†æ›´å¤§ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰
            options.num_predict = 200  # å•æ¡åˆ†ç±»
            options.num_predict_batch = 600  # æ‰¹é‡åˆ†ç±»ï¼ˆ5æ¡*80tokens+ä½™é‡ï¼‰
            options.num_thread = 4  # GPUæ¨¡å¼ä¸‹CPUçº¿ç¨‹ä¸éœ€è¦å¤ªå¤š
        else:
            # CPUæ¨¡å¼ä¼˜åŒ–é…ç½®
            options.num_gpu = 0  # ç¦ç”¨GPU
            options.num_ctx = 2048  # å¢åŠ ä¸Šä¸‹æ–‡ä»¥æ”¯æŒæ‰¹é‡
            options.num_predict = 150  # å•æ¡åˆ†ç±»
            options.num_predict_batch = 500  # æ‰¹é‡åˆ†ç±»
            # æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®çº¿ç¨‹
            try:
                import multiprocessing
                cpu_count = multiprocessing.cpu_count()
                options.num_thread = min(cpu_count, 8)  # æœ€å¤š8çº¿ç¨‹
            except (NotImplementedError, OSError):
                options.num_thread = 4
        
        return options


# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    LLMProvider.OLLAMA: {
        'qwen3:8b': {'name': 'Qwen3 8B', 'description': 'é˜¿é‡Œé€šä¹‰åƒé—®ï¼Œä¸­æ–‡èƒ½åŠ›å¼ºï¼Œæ¨èä½¿ç”¨'},
        'qwen3:4b': {'name': 'Qwen3 4B', 'description': 'è½»é‡æ¨¡å‹ï¼ŒCPUå‹å¥½'},
        'llama3.2:3b': {'name': 'Llama 3.2 3B', 'description': 'Metaè½»é‡æ¨¡å‹ï¼Œé€Ÿåº¦æœ€å¿«'},
        'mistral:7b': {'name': 'Mistral 7B', 'description': 'æ€§èƒ½å‡è¡¡ï¼Œè‹±æ–‡èƒ½åŠ›å¼º'},
    },
    LLMProvider.OPENAI: {
        'gpt-4o-mini': {'name': 'GPT-4o Mini', 'description': 'æ€§ä»·æ¯”é«˜ï¼Œæ¨èä½¿ç”¨'},
        'gpt-4o': {'name': 'GPT-4o', 'description': 'æœ€å¼ºæ€§èƒ½ï¼Œæˆæœ¬è¾ƒé«˜'},
        'gpt-3.5-turbo': {'name': 'GPT-3.5 Turbo', 'description': 'ç»æµå®æƒ '},
    },
    LLMProvider.ANTHROPIC: {
        'claude-3-haiku-20240307': {'name': 'Claude 3 Haiku', 'description': 'å¿«é€Ÿå“åº”ï¼Œæˆæœ¬ä½'},
        'claude-3-sonnet-20240229': {'name': 'Claude 3 Sonnet', 'description': 'å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬'},
        'claude-3-5-sonnet-20241022': {'name': 'Claude 3.5 Sonnet', 'description': 'æœ€æ–°æœ€å¼º'},
    }
}


class LLMClassifier:
    """LLMå¢å¼ºåˆ†ç±»å™¨"""
    
    def __init__(self, 
                 provider: str = 'ollama',
                 model: str = 'qwen3:8b',
                 api_key: Optional[str] = None,
                 enable_cache: bool = True,
                 max_workers: int = 3,  # é»˜è®¤å¹¶å‘æ•°
                 auto_detect_gpu: bool = True,
                 batch_size: int = 5):  # æ–°å¢æ‰¹é‡åˆ†ç±»å¤§å°
        """
        åˆå§‹åŒ–LLMåˆ†ç±»å™¨
        
        Args:
            provider: æä¾›å•† ('ollama', 'openai', 'anthropic')
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼ˆOllamaä¸éœ€è¦ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            max_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤5ï¼ŒGPUæ¨¡å¼å¯æ›´é«˜)
            auto_detect_gpu: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹GPUå¹¶ä¼˜åŒ–é…ç½®
            batch_size: æ‰¹é‡åˆ†ç±»æ—¶æ¯æ‰¹çš„æ•°é‡ (ç”¨äºå‡å°‘LLMè°ƒç”¨æ¬¡æ•°)
        """
        self.provider = LLMProvider(provider)
        self.model = model
        self.api_key = api_key or self._get_api_key()
        self.enable_cache = enable_cache
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # GPUæ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
        self.gpu_info: Optional[GPUInfo] = None
        self.ollama_options: Optional[OllamaOptions] = None
        if auto_detect_gpu and self.provider == LLMProvider.OLLAMA:
            self._setup_gpu_acceleration()
            # GPUæ¨¡å¼ä¸‹å¯ä»¥æé«˜å¹¶å‘æ•°
            if self.gpu_info and self.gpu_info.ollama_gpu_supported:
                self.max_workers = max(max_workers, 6)  # GPUæ¨¡å¼æé«˜å¹¶å‘è‡³6
        
        # ç¼“å­˜
        self.cache: Dict[str, Dict] = {}
        self.cache_file = os.path.join(DATA_CACHE_DIR, 'llm_classification_cache.json')
        self._load_cache()
        
        # è§„åˆ™åˆ†ç±»å™¨ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
        self.rule_classifier = ContentClassifier()
        
        # æ¨¡å‹é¢„çƒ­çŠ¶æ€
        self.is_warmed_up = False
        self._keep_alive_timer: Optional[threading.Timer] = None
        
        # ç»Ÿè®¡
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'llm_calls': 0,
            'fallback_calls': 0,
            'errors': 0,
            'fallback_details': []  # è®°å½•æ¯æ¡é™çº§çš„è¯¦ç»†ä¿¡æ¯
        }
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        self._print_init_info()
    
    def _setup_gpu_acceleration(self):
        """è®¾ç½®GPUåŠ é€Ÿ"""
        self.gpu_info = detect_gpu()
        self.ollama_options = OllamaOptions.auto_configure(self.gpu_info)
    
    def _print_init_info(self):
        """æ‰“å°åˆå§‹åŒ–ä¿¡æ¯"""
        log.ai(t('llm_init_done'))
        log.ai(t('llm_provider', provider=self.provider.value))
        log.ai(t('llm_model_name', model=self.model))
        cache_status = t('llm_cache_enabled') if self.enable_cache else t('llm_cache_disabled')
        log.config(t('llm_cache_status', status=cache_status))
        
        if self.gpu_info:
            if self.gpu_info.ollama_gpu_supported:
                log.success(t('llm_gpu_enabled', gpu_name=self.gpu_info.gpu_name))
                if self.gpu_info.vram_mb:
                    log.info(t('llm_vram', vram=self.gpu_info.vram_mb), emoji="ğŸ’¾")
            else:
                gpu_name = self.gpu_info.gpu_name or t('llm_no_gpu_detected')
                log.warning(t('llm_cpu_mode', gpu_name=gpu_name))
                if self.ollama_options:
                    log.info(t('llm_cpu_threads', threads=self.ollama_options.num_thread), emoji="âš™ï¸")
    
    def get_gpu_info(self) -> Optional[GPUInfo]:
        """è·å–GPUä¿¡æ¯"""
        return self.gpu_info
    
    def warmup_model(self) -> bool:
        """
        é¢„çƒ­æ¨¡å‹ï¼šå‘é€ä¸€ä¸ªç®€å•è¯·æ±‚è®©æ¨¡å‹åŠ è½½åˆ°å†…å­˜/æ˜¾å­˜
        
        Returns:
            bool: é¢„çƒ­æ˜¯å¦æˆåŠŸ
        """
        if self.provider != LLMProvider.OLLAMA:
            # äº‘ç«¯APIä¸éœ€è¦é¢„çƒ­
            self.is_warmed_up = True
            return True
        
        if self.is_warmed_up:
            log.info(t('llm_model_warmed'), emoji="âœ…")
            return True
        
        log.start(t('llm_warming_model', model=self.model))
        start_time = time.time()
        
        try:
            import requests
            
            # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æ¥åŠ è½½æ¨¡å‹
            # ä½¿ç”¨ keep_alive å‚æ•°è®©æ¨¡å‹ä¿æŒæ´»è·ƒ
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': 'Hi',  # æœ€ç®€å•çš„prompt
                    'stream': False,
                    'keep_alive': f'{MODEL_KEEP_ALIVE_SECONDS}s',  # ä¿æ´»æ—¶é—´
                    'options': {
                        'num_predict': 1,  # åªç”Ÿæˆ1ä¸ªtoken
                        'num_ctx': 512
                    }
                },
                timeout=120  # é¦–æ¬¡åŠ è½½å¯èƒ½è¾ƒæ…¢
            )
            
            if response.status_code == 200:
                elapsed = time.time() - start_time
                self.is_warmed_up = True
                log.success(t('llm_warmup_done', time=f'{elapsed:.1f}'))
                log.info(t('llm_keep_alive', minutes=MODEL_KEEP_ALIVE_SECONDS // 60), emoji="â°")
                return True
            else:
                log.error(t('llm_warmup_failed_http', code=response.status_code))
                return False
                
        except Exception as e:
            log.error(t('llm_warmup_failed', error=str(e)))
            return False
    
    def set_keep_alive(self, seconds: int = MODEL_KEEP_ALIVE_SECONDS):
        """
        è®¾ç½®æ¨¡å‹ä¿æ´»æ—¶é—´
        
        Args:
            seconds: ä¿æ´»ç§’æ•°
        """
        if self.provider != LLMProvider.OLLAMA:
            return
        
        try:
            import requests
            
            # å‘é€ä¿æ´»è¯·æ±‚
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': '',  # ç©ºprompt
                    'stream': False,
                    'keep_alive': f'{seconds}s',
                    'options': {'num_predict': 0}
                },
                timeout=10
            )
            
            if response.status_code == 200:
                log.success(t('llm_keepalive_set', minutes=seconds // 60))
                
        except Exception as e:
            log.warning(t('llm_keepalive_failed', error=str(e)))
    
    def unload_model(self):
        """ç«‹å³å¸è½½æ¨¡å‹ï¼ˆé‡Šæ”¾æ˜¾å­˜/å†…å­˜ï¼‰"""
        if self.provider != LLMProvider.OLLAMA:
            return
        
        try:
            import requests
            
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': '',
                    'stream': False,
                    'keep_alive': '0s'  # ç«‹å³å¸è½½
                },
                timeout=10
            )
            
            if response.status_code == 200:
                self.is_warmed_up = False
                log.success(t('llm_model_unloaded', model=self.model))
                
        except Exception as e:
            log.warning(t('llm_unload_failed', error=str(e)))

    def _get_api_key(self) -> Optional[str]:
        """ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥"""
        if self.provider == LLMProvider.OPENAI:
            return os.getenv('OPENAI_API_KEY')
        elif self.provider == LLMProvider.ANTHROPIC:
            return os.getenv('ANTHROPIC_API_KEY')
        return None
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        if self.provider == LLMProvider.OLLAMA:
            # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
            if not self._check_ollama_service():
                log.error(t('llm_ollama_not_running'))
        elif self.provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:
            if not self.api_key:
                log.error(t('llm_api_key_missing', provider=self.provider.value.upper()))
    
    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            response = requests.get('http://localhost:11434/api/tags', timeout=5)
            return response.status_code == 200
        except (requests.RequestException, ConnectionError, TimeoutError):
            return False
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        # ç›´æ¥åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶ï¼Œç¡®ä¿ä»é›¶å¼€å§‹
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    loaded_cache = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆå¿…é¡»åŒ…å« classified_by å­—æ®µï¼‰
                if loaded_cache:
                    first_entry = next(iter(loaded_cache.values()), None)
                    if first_entry and 'classified_by' not in first_entry:
                        # æ—§æ ¼å¼ç¼“å­˜ï¼Œåˆ é™¤æ–‡ä»¶
                        os.remove(self.cache_file)
                        log.warning(t('llm_cache_outdated'))
                        self.cache = {}
                        return
                
                self.cache = loaded_cache
                log.data(t('llm_cache_loaded', count=len(self.cache)))
            except Exception as e:
                print(f"âš ï¸ Cache load failed: {e}")
                # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                try:
                    os.remove(self.cache_file)
                except (OSError, PermissionError):
                    pass
                self.cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        if not self.enable_cache:
            return
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(t('llm_cache_save_failed', error=str(e)))
    
    def _get_content_hash(self, item: Dict) -> str:
        """è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œ"""
        content = f"{item.get('title', '')}|{item.get('summary', '')}|{item.get('source', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _build_classification_prompt(self, item: Dict) -> str:
        """æ„å»ºåˆ†ç±»æç¤ºè¯ï¼ˆç²¾ç®€ç‰ˆï¼Œå‡å°‘tokenæ¶ˆè€—ï¼‰"""
        title = item.get('title', '')[:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
        summary = item.get('summary', item.get('description', ''))[:300]  # å‡å°‘æ‘˜è¦é•¿åº¦
        source = item.get('source', '')
        url = item.get('url', '')
        
        # æ£€æµ‹ URL ä¸­çš„å†…å®¹ç±»å‹æç¤º
        url_hints = []
        if '/podcast/' in url or '/podcasts/' in url:
            url_hints.append("URL indicates PODCAST content")
        if '/paper/' in url or 'arxiv.org' in url:
            url_hints.append("URL indicates RESEARCH content")
        if '/blog/' in url:
            url_hints.append("URL indicates BLOG/ANALYSIS content")
        
        url_hint_text = f"\nURLæç¤º: {', '.join(url_hints)}" if url_hints else ""
        
        # ç²¾ç®€ç‰ˆpromptï¼Œå¤§å¹…å‡å°‘tokenï¼Œä½†ä¿æŒåˆ†ç±»å‡†ç¡®æ€§
        prompt = f"""åˆ†ç±»AIå†…å®¹ã€‚è¾“å‡ºJSONæ ¼å¼ã€‚

æ ‡é¢˜: {title}
æ‘˜è¦: {summary}
æ¥æº: {source}{url_hint_text}

ç±»å‹é€‰é¡¹:
- research: å­¦æœ¯è®ºæ–‡ã€ç§‘ç ”æŠ¥å‘Š
- product: äº§å“å‘å¸ƒã€åŠŸèƒ½æ›´æ–°
- market: èèµ„æ–°é—»ã€è¡Œä¸šåˆ†æã€å…¬å¸åŠ¨æ€ã€æ’­å®¢èŠ‚ç›®
- developer: å¼€æºå·¥å…·ã€æŠ€æœ¯æ¡†æ¶
- leader: çŸ¥åäººå£«ç›´æ¥å‘è¨€ï¼ˆå¿…é¡»å«å¼•è¯­æ ‡è®°å¦‚"è¯´"ã€"è¡¨ç¤º"ã€"ç§°"ã€"warns"ã€"says"ï¼‰
- community: ç¤¾åŒºè®¨è®ºã€è¶‹åŠ¿è¯é¢˜

â˜… leaderåˆ¤æ–­æ ¸å¿ƒè§„åˆ™:
1. å¿…é¡»æœ‰ç›´æ¥å¼•è¯­æ ‡è®°ï¼š"XXXè¯´"ã€"XXXè¡¨ç¤º"ã€"XXX warns"ã€"XXX says"ã€"æ®XXXç§°"
2. æ²¡æœ‰å¼•è¯­æ ‡è®°çš„å…¬å¸/äººç‰©æ–°é—» â†’ market
3. æ ‡é¢˜æ ¼å¼å¦‚"Sam Altman: ..."æˆ–"[äººå]: ..."é€šå¸¸æ˜¯leader

è¾“å‡ºæ ¼å¼(ä¸¥æ ¼JSON):
{{"content_type": "ç±»å‹", "confidence": 0.8, "tech_fields": ["é¢†åŸŸ"], "reasoning": "åŸå› "}}"""
        
        return prompt
    
    def _build_batch_prompt(self, items: List[Dict]) -> str:
        """æ„å»ºæ‰¹é‡åˆ†ç±»æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæé«˜è§£ææˆåŠŸç‡ï¼‰"""
        items_text = []
        for i, item in enumerate(items, 1):
            title = item.get('title', '')[:80]
            summary = item.get('summary', item.get('description', ''))[:120]
            source = item.get('source', '')[:20]
            url = item.get('url', '')
            
            # æ£€æµ‹ URL ç±»å‹æç¤º
            url_type = ""
            if 'arxiv.org' in url or '/paper/' in url:
                url_type = " [PAPER]"
            
            items_text.append(f"[{i}] {title}{url_type}\n    Summary: {summary}\n    Source: {source}")
        
        all_items = "\n".join(items_text)
        
        # ä½¿ç”¨æ›´æ˜ç¡®çš„æ ¼å¼æŒ‡ä»¤å’Œåˆ†ç±»æ ‡å‡†
        prompt = f"""Classify these {len(items)} AI news items. Output ONLY valid JSON, one per line.

Items to classify:
{all_items}

IMPORTANT: Use ONLY these exact values for content_type:
- research: Academic papers, scientific studies, technical reports from arxiv/conferences
- product: Product launches, new features, version releases, API announcements  
- market: Funding news, investments, company analysis, industry competition, podcasts, general news
- developer: Tools, frameworks, models, open source projects, technical tutorials
- leader: Contains DIRECT SPEECH from a notable person (must have quote markers)
- community: Forum discussions, social media trends, community events

â˜…â˜…â˜… LEADER CLASSIFICATION - CRITICAL RULES â˜…â˜…â˜…
Only classify as "leader" when ALL conditions are met:
1. Contains quote markers like: "said", "says", "warns", "believes", "stated", "according to", "è¡¨ç¤º", "ç§°", "è¯´"
2. Title format like "Person Name: ..." or "[Name]: ..." indicates leader
3. If NO quote markers found -> classify as "market" even if about famous people

Examples:
- "Sam Altman says AI will change everything" -> leader (has "says")
- "OpenAI's code red response to Google" -> market (no quote marker)
- "Sam Altman: We need to move fast" -> leader (has "Name:" format)

Other rules:
- Items marked [PAPER] -> research

tech_fields options: LLM, Computer Vision, NLP, Robotics, AI Safety, MLOps, Multimodal, Audio/Speech, Healthcare AI, General AI

Output format - EXACTLY {len(items)} lines starting from id=1:
{{"id":1,"content_type":"TYPE","confidence":0.8,"tech_fields":["FIELD"]}}
{{"id":2,"content_type":"TYPE","confidence":0.8,"tech_fields":["FIELD"]}}
...continue until id={len(items)}

START from id=1, classify ALL {len(items)} items:"""
        
        return prompt
    
    def _call_ollama(self, prompt: str, is_batch: bool = False) -> Optional[str]:
        """è°ƒç”¨Ollama API
        
        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. å¯¹äº Qwen3 ç­‰æ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼Œä½¿ç”¨ Chat API + think=false è·å¾—å¿«é€Ÿå“åº”
        2. å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨ Generate API å¹¶è§£æ thinking å­—æ®µï¼ˆå¦‚æœ‰ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼ˆéœ€è¦æ›´å¤šè¾“å‡ºtokensï¼‰
        """
        try:
            import requests
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼ˆå¦‚ Qwen3ï¼‰
            use_chat_api = 'qwen3' in self.model.lower()
            
            # ä¿æ´»æ—¶é—´è®¾ç½®
            keep_alive = f'{MODEL_KEEP_ALIVE_SECONDS}s'
            
            if use_chat_api:
                # ä½¿ç”¨ Chat API + think=false å…³é—­æ€è€ƒæ¨¡å¼ï¼Œå¤§å¹…æå‡é€Ÿåº¦
                # æ ¹æ®GPUæ£€æµ‹ç»“æœè‡ªé€‚åº”é…ç½®
                options = self._get_ollama_options(is_batch=is_batch)
                
                response = requests.post(
                    'http://localhost:11434/api/chat',
                    json={
                        'model': self.model,
                        'messages': [
                            {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå†…å®¹åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†ç±»ç»“æœã€‚'},
                            {'role': 'user', 'content': prompt}
                        ],
                        'stream': False,
                        'think': False,  # å…³é—­æ€è€ƒæ¨¡å¼
                        'keep_alive': keep_alive,  # ä¿æŒæ¨¡å‹æ´»è·ƒ
                        'options': options
                    },
                    timeout=90 if is_batch else (60 if self.gpu_info and self.gpu_info.ollama_gpu_supported else 90)
                )
                
                if response.status_code == 200:
                    result = response.json()
                    message = result.get('message', {})
                    return message.get('content', '')
            else:
                # ä½¿ç”¨ Generate APIï¼ˆé€‚ç”¨äºå…¶ä»–æ¨¡å‹ï¼‰
                options = self._get_ollama_options()
                
                response = requests.post(
                    'http://localhost:11434/api/generate',
                    json={
                        'model': self.model,
                        'prompt': prompt,
                        'stream': False,
                        'keep_alive': keep_alive,  # ä¿æŒæ¨¡å‹æ´»è·ƒ
                        'options': options
                    },
                    timeout=120 if self.gpu_info and self.gpu_info.ollama_gpu_supported else 180
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # éƒ¨åˆ†æ¨¡å‹ä½¿ç”¨ thinking å­—æ®µå­˜å‚¨æ€è€ƒè¿‡ç¨‹
                    response_text = result.get('response', '')
                    thinking_text = result.get('thinking', '')
                    
                    # å¦‚æœ response ä¸ºç©ºä½† thinking æœ‰å†…å®¹ï¼Œä» thinking ä¸­æå–
                    if not response_text.strip() and thinking_text:
                        return thinking_text
                    
                    return response_text
            
            log.error(t('llm_ollama_error', code=response.status_code))
            return None
                
        except Exception as e:
            log.error(t('llm_ollama_failed', error=str(e)))
            return None
    
    def _get_ollama_options(self, is_batch: bool = False) -> Dict:
        """è·å–Ollamaæ¨ç†é€‰é¡¹ï¼ˆæ ¹æ®GPUè‡ªé€‚åº”é…ç½®ï¼‰
        
        Args:
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼ˆéœ€è¦æ›´å¤šè¾“å‡ºtokensï¼‰
        """
        if self.ollama_options:
            num_predict = self.ollama_options.num_predict_batch if is_batch else self.ollama_options.num_predict
            return {
                'temperature': self.ollama_options.temperature,
                'num_predict': num_predict,
                'num_ctx': self.ollama_options.num_ctx,
                'num_thread': self.ollama_options.num_thread,
                'num_gpu': self.ollama_options.num_gpu
            }
        else:
            # é»˜è®¤é…ç½®
            return {
                'temperature': 0.1,
                'num_predict': 500 if is_batch else 200,
                'num_ctx': 2048,
                'num_thread': 4
            }
    
    def _call_openai(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨OpenAI API"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå†…å®¹åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†ç±»ç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=300
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            log.error(t('llm_openai_failed', error=str(e)))
            return None
    
    def _call_anthropic(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Anthropic API"""
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=self.api_key)
            response = client.messages.create(
                model=self.model,
                max_tokens=300,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.content[0].text
            
        except Exception as e:
            log.error(t('llm_anthropic_failed', error=str(e)))
            return None
    
    def _call_llm(self, prompt: str, is_batch: bool = False) -> Optional[str]:
        """è°ƒç”¨LLMï¼ˆæ ¹æ®æä¾›å•†é€‰æ‹©ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            is_batch: æ˜¯å¦ä¸ºæ‰¹é‡åˆ†ç±»æ¨¡å¼
        """
        if self.provider == LLMProvider.OLLAMA:
            return self._call_ollama(prompt, is_batch=is_batch)
        elif self.provider == LLMProvider.OPENAI:
            return self._call_openai(prompt)
        elif self.provider == LLMProvider.ANTHROPIC:
            return self._call_anthropic(prompt)
        return None
    
    def _parse_llm_response(self, response: str) -> Optional[Dict]:
        """è§£æLLMå“åº”
        
        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. JSONæ ¼å¼: {"content_type": "xxx", ...}
        2. çº¯æ–‡æœ¬æ ¼å¼: ç›´æ¥è¿”å›ç±»åˆ«åç§°ï¼ˆç”¨äº thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
        """
        if not response:
            return None
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            response = response.strip()
            
            # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if 'content_type' in result:
                    # è§„èŒƒåŒ–å­—æ®µ
                    result['content_type'] = result['content_type'].lower()
                    result['confidence'] = float(result.get('confidence', 0.8))
                    result['tech_fields'] = result.get('tech_fields', ['General AI'])
                    result['is_verified'] = result.get('is_verified', True)
                    result['reasoning'] = result.get('reasoning', '')
                    
                    return result
            
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«ï¼ˆæ”¯æŒ thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
            return self._extract_category_from_text(response)
            
        except json.JSONDecodeError as e:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«
            return self._extract_category_from_text(response)
        except Exception as e:
            log.warning(t('llm_parse_failed', error=str(e)))
        
        return None
    
    def _extract_category_from_text(self, text: str) -> Optional[Dict]:
        """ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­æå–ç±»åˆ«
        
        ç”¨äºå¤„ç†ä½¿ç”¨ thinking æ¨¡å¼çš„æ¨¡å‹è¾“å‡º
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        # å®šä¹‰ç±»åˆ«å…³é”®è¯æ˜ å°„
        category_keywords = {
            'llm': ['llm', 'large language model', 'language model', 'gpt', 'chatgpt', 'claude', 'gemini'],
            'product': ['product', 'launch', 'release', 'announce', 'new feature'],
            'research': ['research', 'paper', 'study', 'academic', 'arxiv', 'conference'],
            'industry': ['industry', 'business', 'company', 'enterprise', 'market'],
            'tools': ['tool', 'framework', 'library', 'sdk', 'api'],
            'ethics': ['ethics', 'safety', 'regulation', 'policy', 'bias', 'fairness'],
            'vision': ['vision', 'image', 'video', 'computer vision', 'visual'],
            'robotics': ['robot', 'robotics', 'autonomous', 'embodied'],
        }
        
        # é¦–å…ˆæ£€æŸ¥æ–‡æœ¬æœ«å°¾æ˜¯å¦æœ‰æ˜ç¡®çš„ç±»åˆ«åç§°ï¼ˆR1æ¨¡å‹é€šå¸¸åœ¨æœ€åç»™å‡ºç­”æ¡ˆï¼‰
        lines = text.strip().split('\n')
        last_lines = ' '.join(lines[-3:]) if len(lines) >= 3 else text
        
        for category in category_keywords.keys():
            # æ£€æŸ¥æœ€åå‡ è¡Œæ˜¯å¦åŒ…å«æ˜ç¡®çš„ç±»åˆ«åç§°
            if category in last_lines.lower().split():
                return {
                    'content_type': category,
                    'confidence': 0.85,
                    'tech_fields': ['General AI'],
                    'is_verified': True,
                    'reasoning': 'Extracted from LLM thinking output'
                }
        
        # å¦‚æœæœ«å°¾æ²¡æœ‰æ˜ç¡®ç±»åˆ«ï¼Œç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        category_scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            return {
                'content_type': best_category,
                'confidence': min(0.7 + category_scores[best_category] * 0.05, 0.9),
                'tech_fields': ['General AI'],
                'is_verified': True,
                'reasoning': f'Inferred from text analysis (score: {category_scores[best_category]})'
            }
        
        return None
    
    def classify_item(self, item: Dict, use_cache: bool = True) -> Dict:
        """
        ä½¿ç”¨LLMåˆ†ç±»å•ä¸ªå†…å®¹é¡¹
        
        Args:
            item: å†…å®¹é¡¹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹
        """
        self.stats['total_calls'] += 1
        
        classified = item.copy()
        content_hash = self._get_content_hash(item)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and self.enable_cache and content_hash in self.cache:
            self.stats['cache_hits'] += 1
            cached = self.cache[content_hash]
            classified.update(cached)
            classified['from_cache'] = True
            return classified
        
        # è°ƒç”¨LLM
        prompt = self._build_classification_prompt(item)
        response = self._call_llm(prompt)
        result = self._parse_llm_response(response)
        
        if result:
            self.stats['llm_calls'] += 1
            
            # æ›´æ–°åˆ†ç±»ç»“æœ
            classified['content_type'] = result['content_type']
            classified['confidence'] = result['confidence']
            classified['tech_categories'] = result['tech_fields']
            classified['is_verified'] = result['is_verified']
            classified['llm_reasoning'] = result['reasoning']
            classified['classified_by'] = f"llm:{self.provider.value}/{self.model}"
            classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # ä½¿ç”¨è§„åˆ™åˆ†ç±»å™¨è¡¥å……åœ°åŒºä¿¡æ¯
            classified['region'] = self.rule_classifier.classify_region(item)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.enable_cache:
                self.cache[content_hash] = {
                    'content_type': classified['content_type'],
                    'confidence': classified['confidence'],
                    'tech_categories': classified['tech_categories'],
                    'is_verified': classified['is_verified'],
                    'llm_reasoning': classified['llm_reasoning'],
                    'region': classified['region'],
                    'classified_by': classified['classified_by']  # ä¿å­˜åˆ†ç±»æ¥æº
                }
        else:
            # LLMå¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»
            self.stats['fallback_calls'] += 1
            self.stats['errors'] += 1
            fallback_reason = 'LLMå“åº”è§£æå¤±è´¥'
            self.stats['fallback_details'].append({
                'title': item.get('title', '')[:50],
                'source': item.get('source', ''),
                'reason': fallback_reason,
                'mode': 'single'
            })
            
            log.warning(t('llm_fallback', title=item.get('title', '')[:30]))
            classified = self.rule_classifier.classify_item(item)
            classified['classified_by'] = 'rule:fallback'
        
        return classified
    
    def classify_batch(self, items: List[Dict], show_progress: bool = True, 
                       use_batch_api: bool = True) -> List[Dict]:
        """
        æ‰¹é‡åˆ†ç±»ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼‰
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            use_batch_api: æ˜¯å¦ä½¿ç”¨æ‰¹é‡APIï¼ˆä¸€æ¬¡è°ƒç”¨åˆ†ç±»å¤šæ¡ï¼Œæ›´å¿«ï¼‰
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹åˆ—è¡¨
        """
        total = len(items)
        
        # å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œåˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„å†…å®¹
        cached_items = []
        uncached_items = []
        uncached_indices = []
        
        for i, item in enumerate(items):
            content_hash = self._get_content_hash(item)
            if self.enable_cache and content_hash in self.cache:
                self.stats['cache_hits'] += 1
                self.stats['total_calls'] += 1
                classified = item.copy()
                classified.update(self.cache[content_hash])
                classified['from_cache'] = True
                # ç¡®ä¿æœ‰ classified_by å­—æ®µï¼ˆå…¼å®¹æ—§ç¼“å­˜ï¼‰
                if 'classified_by' not in classified:
                    classified['classified_by'] = 'llm:cached'
                cached_items.append((i, classified))
            else:
                uncached_items.append(item)
                uncached_indices.append(i)
        
        cached_count = len(cached_items)
        uncached_count = len(uncached_items)
        
        log.start(t('llm_batch_start', total=total))
        log.ai(t('llm_batch_info', provider=self.provider.value, model=self.model))
        log.data(t('llm_batch_cache', workers=self.max_workers, cached=cached_count, total=total))
        
        if uncached_count == 0:
            log.success(t('llm_all_cached'))
            cached_items.sort(key=lambda x: x[0])
            return [item for _, item in cached_items]
        
        # æ¨¡å‹é¢„çƒ­ï¼ˆä»…Ollamaä¸”æœªé¢„çƒ­æ—¶ï¼‰
        if self.provider == LLMProvider.OLLAMA and not self.is_warmed_up:
            self.warmup_model()
        
        start_time = time.time()
        classified_uncached = []
        
        # é€‰æ‹©åˆ†ç±»ç­–ç•¥
        if use_batch_api and self.batch_size > 1 and self.provider == LLMProvider.OLLAMA:
            # æ‰¹é‡APIæ¨¡å¼ï¼šä¸€æ¬¡è°ƒç”¨åˆ†ç±»å¤šæ¡ï¼ˆæ›´å¿«ï¼‰
            log.info(t('llm_batch_mode', batch_size=self.batch_size), emoji="ğŸ“¦")
            classified_uncached = self._classify_batch_mode(uncached_items, uncached_indices, show_progress)
        else:
            # å¹¶å‘å•æ¡æ¨¡å¼
            log.info(t('llm_concurrent_mode'), emoji="âš¡")
            classified_uncached = self._classify_concurrent_mode(uncached_items, uncached_indices, show_progress)
        
        # åˆå¹¶ç»“æœ
        all_items = cached_items + classified_uncached
        all_items.sort(key=lambda x: x[0])
        result = [item for _, item in all_items]
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache()
        
        # ç»Ÿè®¡
        elapsed = time.time() - start_time
        self._print_stats(elapsed)
        
        return result
    
    def _classify_batch_mode(self, items: List[Dict], indices: List[int], 
                             show_progress: bool) -> List[Tuple[int, Dict]]:
        """æ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼šä¸€æ¬¡LLMè°ƒç”¨å¤„ç†å¤šæ¡å†…å®¹"""
        results = []
        total = len(items)
        total_batches = (total + self.batch_size - 1) // self.batch_size
        
        # åˆ†æ‰¹å¤„ç†
        batch_num = 0
        for batch_start in range(0, total, self.batch_size):
            batch_num += 1
            batch_start_time = time.time()
            batch_end = min(batch_start + self.batch_size, total)
            batch_items = items[batch_start:batch_end]
            batch_indices = indices[batch_start:batch_end]
            
            # æ„å»ºæ‰¹é‡prompt
            prompt = self._build_batch_prompt(batch_items)
            response = self._call_llm(prompt, is_batch=True)  # ä½¿ç”¨æ‰¹é‡æ¨¡å¼ï¼ˆæ›´å¤šè¾“å‡ºtokensï¼‰
            batch_results = self._parse_batch_response(response, len(batch_items))
            
            # å¤„ç†ç»“æœ
            retry_items = []  # æ”¶é›†éœ€è¦é‡è¯•çš„æ¡ç›®
            retry_indices = []
            
            for i, (item, idx) in enumerate(zip(batch_items, batch_indices)):
                self.stats['total_calls'] += 1
                classified = item.copy()
                
                if batch_results and i < len(batch_results) and batch_results[i]:
                    result = batch_results[i]
                    self.stats['llm_calls'] += 1
                    
                    classified['content_type'] = result.get('content_type', 'market')
                    classified['confidence'] = result.get('confidence', 0.7)
                    classified['tech_categories'] = result.get('tech_fields', ['General AI'])
                    classified['is_verified'] = result.get('is_verified', True)
                    classified['llm_reasoning'] = result.get('reasoning', '')
                    classified['classified_by'] = f"llm:batch:{self.provider.value}/{self.model}"
                    classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    classified['region'] = self.rule_classifier.classify_region(item)
                    
                    # ç¼“å­˜
                    content_hash = self._get_content_hash(item)
                    if self.enable_cache:
                        self.cache[content_hash] = {
                            'content_type': classified['content_type'],
                            'confidence': classified['confidence'],
                            'tech_categories': classified['tech_categories'],
                            'is_verified': classified.get('is_verified', True),
                            'llm_reasoning': classified.get('llm_reasoning', ''),
                            'region': classified['region'],
                            'classified_by': classified['classified_by']  # ä¿å­˜åˆ†ç±»æ¥æº
                        }
                    results.append((idx, classified))
                else:
                    # æ‰¹é‡è§£æå¤±è´¥ï¼ŒåŠ å…¥é‡è¯•åˆ—è¡¨
                    retry_items.append(item)
                    retry_indices.append(idx)
            
            # å¯¹æ‰¹é‡å¤±è´¥çš„æ¡ç›®è¿›è¡Œå•æ¡é‡è¯•
            if retry_items:
                log.warning(t('llm_batch_retry', count=len(retry_items)))
                for item, idx in zip(retry_items, retry_indices):
                    # å°è¯•å•æ¡ LLM åˆ†ç±»
                    retry_result = self._single_classify_with_llm(item)
                    
                    if retry_result:
                        # å•æ¡é‡è¯•æˆåŠŸ
                        self.stats['llm_calls'] += 1
                        classified = item.copy()
                        classified['content_type'] = retry_result.get('content_type', 'market')
                        classified['confidence'] = retry_result.get('confidence', 0.7)
                        classified['tech_categories'] = retry_result.get('tech_fields', ['General AI'])
                        classified['is_verified'] = retry_result.get('is_verified', True)
                        classified['llm_reasoning'] = retry_result.get('reasoning', '')
                        classified['classified_by'] = f"llm:retry:{self.provider.value}/{self.model}"
                        classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        classified['region'] = self.rule_classifier.classify_region(item)
                        
                        # ç¼“å­˜
                        content_hash = self._get_content_hash(item)
                        if self.enable_cache:
                            self.cache[content_hash] = {
                                'content_type': classified['content_type'],
                                'confidence': classified['confidence'],
                                'tech_categories': classified['tech_categories'],
                                'is_verified': classified.get('is_verified', True),
                                'llm_reasoning': classified.get('llm_reasoning', ''),
                                'region': classified['region'],
                                'classified_by': classified['classified_by']
                            }
                        results.append((idx, classified))
                        log.success(t('llm_retry_success', title=item.get('title', '')[:40]))
                    else:
                        # å•æ¡é‡è¯•ä¹Ÿå¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»
                        self.stats['fallback_calls'] += 1
                        self.stats['fallback_details'].append({
                            'title': item.get('title', '')[:50],
                            'source': item.get('source', ''),
                            'reason': 'æ‰¹é‡+å•æ¡é‡è¯•å‡å¤±è´¥',
                            'mode': 'batch_retry'
                        })
                        classified = self.rule_classifier.classify_item(item)
                        classified['classified_by'] = 'rule:batch_fallback'
                        results.append((idx, classified))
            
            if show_progress:
                completed = min(batch_end, total)
                batch_time = time.time() - batch_start_time
                remaining_batches = total_batches - batch_num
                estimated_remaining = batch_time * remaining_batches
                
                if remaining_batches > 0:
                    log.info(t('llm_progress_eta', completed=completed, total=total, percent=int(completed/total*100), time=f"{batch_time:.1f}", eta=f"{estimated_remaining:.0f}"), emoji="â³")
                else:
                    log.info(t('llm_progress', completed=completed, total=total, percent=int(completed/total*100)) + f" | {t('llm_stats_time', time=f'{batch_time:.1f}')}", emoji="âœ…")
        
        return results
    
    def _classify_concurrent_mode(self, items: List[Dict], indices: List[int],
                                   show_progress: bool) -> List[Tuple[int, Dict]]:
        """å¹¶å‘å•æ¡åˆ†ç±»æ¨¡å¼"""
        results = []
        total = len(items)
        last_progress_time = time.time()
        last_progress_count = 0
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.classify_item, item): (i, idx) 
                      for i, (item, idx) in enumerate(zip(items, indices))}
            
            completed = 0
            for future in as_completed(futures):
                try:
                    result = future.result()
                    _, idx = futures[future]
                    results.append((idx, result))
                    completed += 1
                    
                    if show_progress and completed % 5 == 0:
                        current_time = time.time()
                        interval_time = current_time - last_progress_time
                        interval_count = completed - last_progress_count
                        
                        if interval_count > 0 and interval_time > 0:
                            rate = interval_count / interval_time  # æ¡/ç§’
                            remaining = total - completed
                            estimated_remaining = remaining / rate if rate > 0 else 0
                            log.info(t('llm_progress_rate', completed=completed, total=total, percent=int(completed/total*100), rate=f"{rate:.1f}", eta=f"{estimated_remaining:.0f}"), emoji="â³")
                        else:
                            log.info(t('llm_progress', completed=completed, total=total, percent=int(completed/total*100)), emoji="ğŸ“Š")
                        
                        last_progress_time = current_time
                        last_progress_count = completed
                        
                except Exception as e:
                    log.error(t('llm_task_failed', error=str(e)))
                    self.stats['errors'] += 1
        
        return results
    
    def _single_classify_with_llm(self, item: Dict) -> Optional[Dict]:
        """å¯¹å•æ¡å†…å®¹è¿›è¡Œ LLM åˆ†ç±»ï¼ˆç”¨äºæ‰¹é‡å¤±è´¥åçš„é‡è¯•ï¼‰
        
        Args:
            item: è¦åˆ†ç±»çš„æ¡ç›®
            
        Returns:
            åˆ†ç±»ç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            prompt = self._build_classification_prompt(item)
            response = self._call_llm(prompt, is_batch=False)
            
            if not response:
                return None
            
            # è§£æå•æ¡å“åº”
            result = self._parse_single_response(response)
            return result
            
        except Exception as e:
            log.warning(f"Single retry failed: {str(e)}")
            return None
    
    def _parse_single_response(self, response: str) -> Optional[Dict]:
        """è§£æå•æ¡åˆ†ç±»å“åº”"""
        if not response:
            return None
        
        try:
            # é¢„å¤„ç†ï¼šç§»é™¤markdownä»£ç å—æ ‡è®°
            cleaned = response.strip()
            if '```json' in cleaned:
                import re
                json_blocks = re.findall(r'```json?\s*(.*?)\s*```', cleaned, re.DOTALL)
                if json_blocks:
                    cleaned = json_blocks[0]
            elif '```' in cleaned:
                import re
                cleaned = re.sub(r'```\w*\s*', '', cleaned)
                cleaned = cleaned.replace('```', '')
            
            # æŸ¥æ‰¾JSONå¯¹è±¡
            start = cleaned.find('{')
            end = cleaned.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = cleaned[start:end]
                # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                json_str = json_str.replace('ï¼Œ', ',')
                json_str = json_str.replace('"', '"').replace('"', '"')
                
                obj = json.loads(json_str)
                
                content_type = obj.get('content_type', obj.get('type', 'market'))
                if isinstance(content_type, str):
                    content_type = content_type.lower().strip()
                    if '(' in content_type:
                        content_type = content_type.split('(')[0].strip()
                
                # éªŒè¯content_type
                valid_types = ['research', 'product', 'market', 'developer', 'leader', 'community']
                if content_type not in valid_types:
                    type_mapping = {
                        'paper': 'research', 'academic': 'research', 'study': 'research',
                        'release': 'product', 'launch': 'product', 'tool': 'developer',
                        'news': 'market', 'funding': 'market', 'opinion': 'leader',
                        'discussion': 'community', 'trend': 'community'
                    }
                    content_type = type_mapping.get(content_type, 'market')
                
                return {
                    'content_type': content_type,
                    'confidence': float(obj.get('confidence', 0.7)),
                    'tech_fields': obj.get('tech_fields', obj.get('fields', ['General AI'])),
                    'is_verified': obj.get('is_verified', True),
                    'reasoning': obj.get('reasoning', obj.get('reason', ''))
                }
                
        except Exception as e:
            log.warning(f"Parse single response failed: {str(e)}")
        
        return None

    def _parse_batch_response(self, response: str, expected_count: int) -> List[Optional[Dict]]:
        """è§£ææ‰¹é‡åˆ†ç±»å“åº”ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        æ”¯æŒå¤šç§LLMè¾“å‡ºæ ¼å¼ï¼š
        1. æ¯è¡Œä¸€ä¸ªJSON
        2. Markdownä»£ç å—åŒ…è£¹çš„JSON
        3. å¸¦åºå·çš„JSONåˆ—è¡¨
        4. JSONæ•°ç»„æ ¼å¼
        """
        results = [None] * expected_count
        
        if not response:
            return results
        
        try:
            # é¢„å¤„ç†ï¼šç§»é™¤markdownä»£ç å—æ ‡è®°
            cleaned = response.strip()
            if '```json' in cleaned:
                # æå–```json ... ```ä¹‹é—´çš„å†…å®¹
                import re
                json_blocks = re.findall(r'```json?\s*(.*?)\s*```', cleaned, re.DOTALL)
                if json_blocks:
                    cleaned = '\n'.join(json_blocks)
            elif '```' in cleaned:
                # ç§»é™¤é€šç”¨ä»£ç å—æ ‡è®°
                cleaned = re.sub(r'```\w*\s*', '', cleaned)
                cleaned = cleaned.replace('```', '')
            
            json_objects = []
            
            # æ–¹æ³•1ï¼šå°è¯•è§£æä¸ºJSONæ•°ç»„
            try:
                arr = json.loads(cleaned)
                if isinstance(arr, list):
                    json_objects = arr
            except json.JSONDecodeError:
                pass
            
            # æ–¹æ³•2ï¼šæŒ‰è¡Œè§£æJSON
            if not json_objects:
                lines = cleaned.split('\n')
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # ç§»é™¤è¡Œé¦–çš„åºå·ï¼ˆå¦‚ "1." æˆ– "[1]"ï¼‰
                    import re
                    line = re.sub(r'^[\[\(]?\d+[\]\)\.:]?\s*', '', line)
                    
                    # æŸ¥æ‰¾JSONå¯¹è±¡
                    start = line.find('{')
                    end = line.rfind('}') + 1
                    
                    if start >= 0 and end > start:
                        json_str = line[start:end]
                        # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                        json_str = json_str.replace('ï¼Œ', ',')  # ä¸­æ–‡é€—å·
                        json_str = json_str.replace('"', '"').replace('"', '"')  # ä¸­æ–‡å¼•å·
                        json_str = json_str.replace(''', "'").replace(''', "'")
                        
                        try:
                            obj = json.loads(json_str)
                            json_objects.append(obj)
                        except json.JSONDecodeError:
                            # å°è¯•æ›´å®½æ¾çš„è§£æ
                            try:
                                # ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®å
                                fixed = re.sub(r'(\w+):', r'"\1":', json_str)
                                obj = json.loads(fixed)
                                json_objects.append(obj)
                            except (json.JSONDecodeError, ValueError):
                                continue
            
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾æ‰€æœ‰ç‹¬ç«‹çš„JSONå¯¹è±¡ï¼ˆå¤„ç†å¤šä¸ªJSONåœ¨ä¸€è¡Œçš„æƒ…å†µï¼‰
            if not json_objects:
                import re
                pattern = r'\{[^{}]*\}'
                matches = re.findall(pattern, cleaned)
                for match in matches:
                    try:
                        obj = json.loads(match)
                        json_objects.append(obj)
                    except (json.JSONDecodeError, ValueError):
                        continue
            
            # åŒ¹é…ç»“æœåˆ°å¯¹åº”ç´¢å¼•
            for i, obj in enumerate(json_objects):
                # ä¼˜å…ˆä½¿ç”¨idå­—æ®µ
                idx = obj.get('id')
                if idx is not None:
                    idx = int(idx) - 1  # idä»1å¼€å§‹
                else:
                    # æ²¡æœ‰idå­—æ®µï¼ŒæŒ‰é¡ºåºåˆ†é…
                    idx = i
                
                if 0 <= idx < expected_count and results[idx] is None:
                    content_type = obj.get('content_type', obj.get('type', 'market'))
                    if isinstance(content_type, str):
                        content_type = content_type.lower().strip()
                        # å¤„ç†å¸¦æ‹¬å·çš„æ ¼å¼ï¼Œå¦‚ "developer(tools/models)" -> "developer"
                        if '(' in content_type:
                            content_type = content_type.split('(')[0].strip()
                    
                    # éªŒè¯content_typeæ˜¯å¦æœ‰æ•ˆ
                    valid_types = ['research', 'product', 'market', 'developer', 'leader', 'community']
                    if content_type not in valid_types:
                        # å°è¯•æ˜ å°„
                        type_mapping = {
                            'paper': 'research', 'academic': 'research', 'study': 'research',
                            'papers': 'research', 'releases': 'product',
                            'release': 'product', 'launch': 'product', 'tool': 'developer',
                            'tools': 'developer', 'models': 'developer', 'tools/models': 'developer',
                            'news': 'market', 'funding': 'market', 'investment': 'market',
                            'funding/news': 'market',
                            'opinion': 'leader', 'quote': 'leader', 'insight': 'leader',
                            'opinions': 'leader',
                            'discussion': 'community', 'trend': 'community', 'trends': 'community'
                        }
                        content_type = type_mapping.get(content_type, 'market')
                    
                    results[idx] = {
                        'content_type': content_type,
                        'confidence': float(obj.get('confidence', 0.7)),
                        'tech_fields': obj.get('tech_fields', obj.get('fields', ['General AI'])),
                        'is_verified': obj.get('is_verified', True),
                        'reasoning': obj.get('reasoning', obj.get('reason', ''))
                    }
            
        except Exception as e:
            log.warning(t('llm_batch_parse_failed', error=str(e)))
        
        return results
    
    def _print_stats(self, elapsed: float):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        log.info(t('llm_stats'), emoji="ğŸ“Š")
        log.info(t('llm_stats_total', count=self.stats['total_calls']), emoji="  ")
        log.info(t('llm_stats_cached', count=self.stats['cache_hits']) + f" ({self.stats['cache_hits']/max(1,self.stats['total_calls']):.0%})", emoji="  ")
        log.info(f"   LLM: {self.stats['llm_calls']}", emoji="  ")
        log.info(f"   Fallback: {self.stats['fallback_calls']}", emoji="  ")
        log.info(t('llm_stats_failed', count=self.stats['errors']), emoji="  ")
        log.info(t('llm_stats_time', time=f"{elapsed:.1f}s"), emoji="  ")
        
        if self.stats['llm_calls'] > 0:
            avg_time = elapsed / self.stats['llm_calls']
            log.info(t('llm_stats_avg', time=f"{avg_time:.1f}"), emoji="  ")
        
        # æ˜¾ç¤ºé™çº§è¯¦æƒ…
        if self.stats['fallback_details']:
            log.warning(t('llm_fallback_details', count=len(self.stats['fallback_details'])))
            for i, detail in enumerate(self.stats['fallback_details'], 1):
                log.info(t('llm_fallback_item', i=i, mode=detail['mode'], title=detail['title']), emoji="  ")
                log.info(t('llm_fallback_source', source=detail['source'], reason=detail['reason']), emoji="  ")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        log.success(t('llm_cache_cleared'))


def select_llm_provider() -> Tuple[str, str]:
    """
    äº¤äº’å¼é€‰æ‹©LLMæä¾›å•†å’Œæ¨¡å‹
    
    Returns:
        (provider, model)
    """
    print("\n" + "="*60)
    print(t('llm_select_provider'))
    print("="*60)
    
    print(t('llm_available_providers'))
    print(t('llm_provider_ollama'))
    print(t('llm_provider_openai'))
    print(t('llm_provider_anthropic'))
    
    prompt = "Select provider (1-3) [default: 1]: " if get_language() == 'en' else "è¯·é€‰æ‹©æä¾›å•† (1-3) [é»˜è®¤: 1]: "
    provider_choice = input(f"\n{prompt}").strip() or '1'
    
    provider_map = {'1': 'ollama', '2': 'openai', '3': 'anthropic'}
    provider = provider_map.get(provider_choice, 'ollama')
    
    # é€‰æ‹©æ¨¡å‹
    print(t('llm_available_models_for', provider=provider.upper()))
    
    provider_enum = LLMProvider(provider)
    models = AVAILABLE_MODELS.get(provider_enum, {})
    
    model_list = list(models.keys())
    for i, (model_id, info) in enumerate(models.items(), 1):
        recommended = " â­" if i == 1 else ""
        print(f"  {i}. {info['name']}{recommended}")
        print(f"     {info['description']}")
    
    model_prompt = f"Select model (1-{len(model_list)}) [default: 1]: " if get_language() == 'en' else f"è¯·é€‰æ‹©æ¨¡å‹ (1-{len(model_list)}) [é»˜è®¤: 1]: "
    model_choice = input(f"\n{model_prompt}").strip() or '1'
    
    try:
        model_idx = int(model_choice) - 1
        model = model_list[model_idx] if 0 <= model_idx < len(model_list) else model_list[0]
    except (ValueError, IndexError):
        model = model_list[0]
    
    log.success(t('llm_selected', provider=provider, model=model))
    
    return provider, model


def get_available_ollama_models() -> List[str]:
    """è·å–æœ¬åœ°å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
    try:
        import requests
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
    except (requests.RequestException, ConnectionError, TimeoutError, json.JSONDecodeError):
        pass
    return []


def check_ollama_status() -> Dict:
    """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
    result = {
        'running': False,
        'models': [],
        'recommended': None
    }
    
    try:
        import requests
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            result['running'] = True
            data = response.json()
            result['models'] = [model['name'] for model in data.get('models', [])]
            
            # æ¨èæ¨¡å‹ä¼˜å…ˆçº§
            preferred = ['qwen3:8b', 'qwen3:4b', 'llama3.2:3b', 'mistral:7b']
            for model in preferred:
                if model in result['models']:
                    result['recommended'] = model
                    break
            
            if not result['recommended'] and result['models']:
                result['recommended'] = result['models'][0]
                
    except Exception as e:
        result['error'] = str(e)
    
    return result


# ä¾¿æ·å‡½æ•°
def create_llm_classifier(auto_select: bool = False) -> LLMClassifier:
    """
    åˆ›å»ºLLMåˆ†ç±»å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        auto_select: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
        
    Returns:
        LLMClassifierå®ä¾‹
    """
    if auto_select:
        # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollama
        status = check_ollama_status()
        if status['running'] and status['recommended']:
            return LLMClassifier(provider='ollama', model=status['recommended'])
        
        # æ£€æŸ¥OpenAI
        if os.getenv('OPENAI_API_KEY'):
            return LLMClassifier(provider='openai', model='gpt-4o-mini')
        
        # æ£€æŸ¥Anthropic
        if os.getenv('ANTHROPIC_API_KEY'):
            return LLMClassifier(provider='anthropic', model='claude-3-haiku-20240307')
        
        log.error(t('llm_no_service'))
        return None
    else:
        # äº¤äº’å¼é€‰æ‹©
        provider, model = select_llm_provider()
        return LLMClassifier(provider=provider, model=model)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("="*60)
    print(t('llm_test_title'))
    print("="*60)
    
    # æ£€æŸ¥OllamaçŠ¶æ€
    status = check_ollama_status()
    status_text = t('llm_ollama_running_yes') if status['running'] else t('llm_ollama_running_no')
    log.info(t('llm_ollama_status', status=status_text), emoji="ğŸ”")
    if status['models']:
        log.info(t('llm_available_models', models=', '.join(status['models'])), emoji="ğŸ“¦")
        log.info(t('llm_recommended_model', model=status['recommended']), emoji="â­")
    
    # åˆ›å»ºåˆ†ç±»å™¨
    if status['running']:
        classifier = LLMClassifier(
            provider='ollama',
            model=status['recommended'] or 'qwen3:8b'
        )
        
        # æµ‹è¯•åˆ†ç±»
        test_item = {
            'title': 'OpenAI officially launches GPT-4o with new features',
            'summary': 'OpenAI announces the general availability of GPT-4o model with improved capabilities',
            'source': 'TechCrunch'
        }
        
        log.info(t('llm_test_content', title=test_item['title']), emoji="ğŸ§ª")
        result = classifier.classify_item(test_item)
        
        log.info(t('llm_test_result'), emoji="ğŸ“‹")
        log.info(t('llm_test_type', type=result.get('content_type')), emoji="  ")
        log.info(t('llm_test_confidence', confidence=f"{result.get('confidence', 0):.1%}"), emoji="  ")
        log.info(t('llm_test_tech', tech=result.get('tech_categories')), emoji="  ")
        log.info(t('llm_test_verified', verified=result.get('is_verified')), emoji="  ")
        log.info(t('llm_test_reasoning', reasoning=result.get('llm_reasoning')), emoji="  ")
    else:
        log.warning(t('llm_start_ollama'))

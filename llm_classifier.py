"""
LLMå¢å¼ºåˆ†ç±»å™¨ - LLM Classifier
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†ç±»

æ”¯æŒçš„æä¾›å•†:
- Ollama (æœ¬åœ°): Qwen3:8b, Qwen3:4b, Llama3.2:3b
- OpenAI: GPT-4o-mini, GPT-4o
- Anthropic: Claude-3-Haiku, Claude-3-Sonnet

åŠŸèƒ½ç‰¹æ€§:
- å¤šæä¾›å•†æ”¯æŒï¼Œçµæ´»åˆ‡æ¢
- MD5å†…å®¹ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
- è‡ªåŠ¨é™çº§åˆ°è§„åˆ™åˆ†ç±»
- å¹¶å‘å¤„ç†åŠ é€Ÿ
- è¯¦ç»†çš„åˆ†ç±»æ¨ç†
- GPUè‡ªåŠ¨æ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
"""

import os
import json
import hashlib
import time
import subprocess
import platform
import threading
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum

# å¯¼å…¥è§„åˆ™åˆ†ç±»å™¨ä½œä¸ºå¤‡ä»½
from content_classifier import ContentClassifier


# æ¨¡å‹ä¿æ´»æ—¶é—´ï¼ˆç§’ï¼‰
MODEL_KEEP_ALIVE_SECONDS = 5 * 60  # 5åˆ†é’Ÿ


class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    OLLAMA = "ollama"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class GPUInfo:
    """GPUä¿¡æ¯"""
    available: bool = False
    gpu_type: str = "none"  # nvidia, amd, apple, qualcomm, none
    gpu_name: str = ""
    vram_mb: int = 0
    driver_version: str = ""
    cuda_available: bool = False
    rocm_available: bool = False
    metal_available: bool = False
    ollama_gpu_supported: bool = False  # Ollamaæ˜¯å¦æ”¯æŒè¯¥GPU


def detect_gpu() -> GPUInfo:
    """
    æ£€æµ‹ç³»ç»ŸGPUä¿¡æ¯
    
    Returns:
        GPUInfo: GPUæ£€æµ‹ç»“æœ
    """
    info = GPUInfo()
    system = platform.system()
    
    # 1. æ£€æµ‹ NVIDIA GPU (CUDA)
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0 and result.stdout.strip():
            parts = result.stdout.strip().split(', ')
            if len(parts) >= 3:
                info.available = True
                info.gpu_type = "nvidia"
                info.gpu_name = parts[0].strip()
                info.vram_mb = int(float(parts[1].strip()))
                info.driver_version = parts[2].strip()
                info.cuda_available = True
                info.ollama_gpu_supported = True
                return info
    except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
        pass
    
    # 2. æ£€æµ‹ AMD GPU (ROCm) - ä»…Linux
    if system == "Linux":
        try:
            result = subprocess.run(['rocm-smi', '--showproductname'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0 and 'GPU' in result.stdout:
                info.available = True
                info.gpu_type = "amd"
                info.gpu_name = "AMD ROCm GPU"
                info.rocm_available = True
                info.ollama_gpu_supported = True
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 3. æ£€æµ‹ Apple Silicon (Metal)
    if system == "Darwin":
        try:
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                cpu_info = result.stdout.strip()
                if 'Apple' in cpu_info:
                    info.available = True
                    info.gpu_type = "apple"
                    info.gpu_name = cpu_info
                    info.metal_available = True
                    info.ollama_gpu_supported = True
                    return info
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass
    
    # 4. æ£€æµ‹ Windows æ˜¾å¡ï¼ˆå¯èƒ½æ˜¯ä¸æ”¯æŒçš„GPUï¼‰
    if system == "Windows":
        try:
            result = subprocess.run(
                ['powershell', '-Command', 
                 'Get-WmiObject Win32_VideoController | Select-Object -First 1 Name, AdapterRAM, DriverVersion | ConvertTo-Json'],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0 and result.stdout.strip():
                gpu_data = json.loads(result.stdout)
                gpu_name = gpu_data.get('Name', '')
                
                info.gpu_name = gpu_name
                info.driver_version = gpu_data.get('DriverVersion', '')
                adapter_ram = gpu_data.get('AdapterRAM', 0)
                if adapter_ram:
                    info.vram_mb = int(adapter_ram / (1024 * 1024))
                
                # åˆ¤æ–­GPUç±»å‹
                if 'NVIDIA' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "nvidia"
                    info.cuda_available = True
                    info.ollama_gpu_supported = True
                elif 'AMD' in gpu_name.upper() or 'RADEON' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "amd"
                    info.ollama_gpu_supported = False  # Windowsä¸ŠAMDä¸æ”¯æŒ
                elif 'QUALCOMM' in gpu_name.upper() or 'ADRENO' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "qualcomm"
                    info.ollama_gpu_supported = False  # Qualcommä¸æ”¯æŒ
                elif 'INTEL' in gpu_name.upper():
                    info.available = True
                    info.gpu_type = "intel"
                    info.ollama_gpu_supported = False  # Intelé›†æ˜¾ä¸æ”¯æŒ
                
                return info
        except (FileNotFoundError, subprocess.TimeoutExpired, json.JSONDecodeError, Exception):
            pass
    
    return info


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    timeout: int = 60
    max_retries: int = 2


@dataclass
class OllamaOptions:
    """Ollamaæ¨ç†é€‰é¡¹ - æ ¹æ®GPUè‡ªé€‚åº”é…ç½®"""
    temperature: float = 0.1
    num_predict: int = 150  # å‡å°‘è¾“å‡ºé•¿åº¦ï¼ŒåŠ å¿«å“åº”
    num_ctx: int = 2048
    num_thread: int = 4
    num_gpu: int = 0  # 0è¡¨ç¤ºè‡ªåŠ¨ï¼Œ-1è¡¨ç¤ºç¦ç”¨GPU
    
    @classmethod
    def auto_configure(cls, gpu_info: GPUInfo) -> 'OllamaOptions':
        """æ ¹æ®GPUä¿¡æ¯è‡ªåŠ¨é…ç½®æ¨ç†é€‰é¡¹"""
        options = cls()
        
        if gpu_info.ollama_gpu_supported:
            # GPUåŠ é€Ÿé…ç½® - ä¼˜åŒ–é€Ÿåº¦
            options.num_gpu = 999  # ä½¿ç”¨æ‰€æœ‰GPUå±‚
            options.num_ctx = 4096  # GPUå¯ä»¥å¤„ç†æ›´å¤§ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰
            options.num_predict = 150  # å‡å°‘è¾“å‡ºï¼ŒåŠ å¿«é€Ÿåº¦
            options.num_thread = 4  # GPUæ¨¡å¼ä¸‹CPUçº¿ç¨‹ä¸éœ€è¦å¤ªå¤š
        else:
            # CPUæ¨¡å¼ä¼˜åŒ–é…ç½® - ç‰ºç‰²è´¨é‡æ¢é€Ÿåº¦
            options.num_gpu = 0  # ç¦ç”¨GPU
            options.num_ctx = 1024  # å‡å°‘ä¸Šä¸‹æ–‡ä»¥æå‡é€Ÿåº¦
            options.num_predict = 100  # CPUæ¨¡å¼æ›´çŸ­è¾“å‡º
            # æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®çº¿ç¨‹
            try:
                import multiprocessing
                cpu_count = multiprocessing.cpu_count()
                options.num_thread = min(cpu_count, 8)  # æœ€å¤š8çº¿ç¨‹
            except:
                options.num_thread = 4
        
        return options


# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    LLMProvider.OLLAMA: {
        'qwen3:8b': {'name': 'Qwen3 8B', 'description': 'é˜¿é‡Œé€šä¹‰åƒé—®ï¼Œä¸­æ–‡èƒ½åŠ›å¼ºï¼Œæ¨èä½¿ç”¨'},
        'qwen3:4b': {'name': 'Qwen3 4B', 'description': 'è½»é‡æ¨¡å‹ï¼ŒCPUå‹å¥½'},
        'llama3.2:3b': {'name': 'Llama 3.2 3B', 'description': 'Metaè½»é‡æ¨¡å‹ï¼Œé€Ÿåº¦æœ€å¿«'},
        'mistral:7b': {'name': 'Mistral 7B', 'description': 'æ€§èƒ½å‡è¡¡ï¼Œè‹±æ–‡èƒ½åŠ›å¼º'},
    },
    LLMProvider.OPENAI: {
        'gpt-4o-mini': {'name': 'GPT-4o Mini', 'description': 'æ€§ä»·æ¯”é«˜ï¼Œæ¨èä½¿ç”¨'},
        'gpt-4o': {'name': 'GPT-4o', 'description': 'æœ€å¼ºæ€§èƒ½ï¼Œæˆæœ¬è¾ƒé«˜'},
        'gpt-3.5-turbo': {'name': 'GPT-3.5 Turbo', 'description': 'ç»æµå®æƒ '},
    },
    LLMProvider.ANTHROPIC: {
        'claude-3-haiku-20240307': {'name': 'Claude 3 Haiku', 'description': 'å¿«é€Ÿå“åº”ï¼Œæˆæœ¬ä½'},
        'claude-3-sonnet-20240229': {'name': 'Claude 3 Sonnet', 'description': 'å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬'},
        'claude-3-5-sonnet-20241022': {'name': 'Claude 3.5 Sonnet', 'description': 'æœ€æ–°æœ€å¼º'},
    }
}


class LLMClassifier:
    """LLMå¢å¼ºåˆ†ç±»å™¨"""
    
    def __init__(self, 
                 provider: str = 'ollama',
                 model: str = 'qwen3:8b',
                 api_key: Optional[str] = None,
                 enable_cache: bool = True,
                 max_workers: int = 3,  # é»˜è®¤å¹¶å‘æ•°
                 auto_detect_gpu: bool = True,
                 batch_size: int = 5):  # æ–°å¢æ‰¹é‡åˆ†ç±»å¤§å°
        """
        åˆå§‹åŒ–LLMåˆ†ç±»å™¨
        
        Args:
            provider: æä¾›å•† ('ollama', 'openai', 'anthropic')
            model: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼ˆOllamaä¸éœ€è¦ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            max_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤5ï¼ŒGPUæ¨¡å¼å¯æ›´é«˜)
            auto_detect_gpu: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹GPUå¹¶ä¼˜åŒ–é…ç½®
            batch_size: æ‰¹é‡åˆ†ç±»æ—¶æ¯æ‰¹çš„æ•°é‡ (ç”¨äºå‡å°‘LLMè°ƒç”¨æ¬¡æ•°)
        """
        self.provider = LLMProvider(provider)
        self.model = model
        self.api_key = api_key or self._get_api_key()
        self.enable_cache = enable_cache
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # GPUæ£€æµ‹ä¸è‡ªé€‚åº”é…ç½®
        self.gpu_info: Optional[GPUInfo] = None
        self.ollama_options: Optional[OllamaOptions] = None
        if auto_detect_gpu and self.provider == LLMProvider.OLLAMA:
            self._setup_gpu_acceleration()
            # GPUæ¨¡å¼ä¸‹å¯ä»¥æé«˜å¹¶å‘æ•°
            if self.gpu_info and self.gpu_info.ollama_gpu_supported:
                self.max_workers = max(max_workers, 6)  # GPUæ¨¡å¼æé«˜å¹¶å‘è‡³6
        
        # ç¼“å­˜
        self.cache: Dict[str, Dict] = {}
        self.cache_file = 'llm_classification_cache.json'
        self._load_cache()
        
        # è§„åˆ™åˆ†ç±»å™¨ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
        self.rule_classifier = ContentClassifier()
        
        # æ¨¡å‹é¢„çƒ­çŠ¶æ€
        self.is_warmed_up = False
        self._keep_alive_timer: Optional[threading.Timer] = None
        
        # ç»Ÿè®¡
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'llm_calls': 0,
            'fallback_calls': 0,
            'errors': 0
        }
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        self._print_init_info()
    
    def _setup_gpu_acceleration(self):
        """è®¾ç½®GPUåŠ é€Ÿ"""
        self.gpu_info = detect_gpu()
        self.ollama_options = OllamaOptions.auto_configure(self.gpu_info)
    
    def _print_init_info(self):
        """æ‰“å°åˆå§‹åŒ–ä¿¡æ¯"""
        print(f"ğŸ¤– LLMåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æä¾›å•†: {self.provider.value}")
        print(f"   æ¨¡å‹: {self.model}")
        print(f"   ç¼“å­˜: {'å¯ç”¨' if self.enable_cache else 'ç¦ç”¨'}")
        
        if self.gpu_info:
            if self.gpu_info.ollama_gpu_supported:
                print(f"   ğŸš€ GPUåŠ é€Ÿ: å¯ç”¨ ({self.gpu_info.gpu_name})")
                if self.gpu_info.vram_mb:
                    print(f"   æ˜¾å­˜: {self.gpu_info.vram_mb} MB")
            else:
                print(f"   ğŸ’» è¿è¡Œæ¨¡å¼: CPU ({self.gpu_info.gpu_name or 'æœªæ£€æµ‹åˆ°GPU'})")
                if self.ollama_options:
                    print(f"   CPUçº¿ç¨‹: {self.ollama_options.num_thread}")
    
    def get_gpu_info(self) -> Optional[GPUInfo]:
        """è·å–GPUä¿¡æ¯"""
        return self.gpu_info
    
    def warmup_model(self) -> bool:
        """
        é¢„çƒ­æ¨¡å‹ï¼šå‘é€ä¸€ä¸ªç®€å•è¯·æ±‚è®©æ¨¡å‹åŠ è½½åˆ°å†…å­˜/æ˜¾å­˜
        
        Returns:
            bool: é¢„çƒ­æ˜¯å¦æˆåŠŸ
        """
        if self.provider != LLMProvider.OLLAMA:
            # äº‘ç«¯APIä¸éœ€è¦é¢„çƒ­
            self.is_warmed_up = True
            return True
        
        if self.is_warmed_up:
            print("   âœ“ æ¨¡å‹å·²é¢„çƒ­")
            return True
        
        print(f"ğŸ”¥ æ­£åœ¨é¢„çƒ­æ¨¡å‹ {self.model}...")
        start_time = time.time()
        
        try:
            import requests
            
            # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æ¥åŠ è½½æ¨¡å‹
            # ä½¿ç”¨ keep_alive å‚æ•°è®©æ¨¡å‹ä¿æŒæ´»è·ƒ
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': 'Hi',  # æœ€ç®€å•çš„prompt
                    'stream': False,
                    'keep_alive': f'{MODEL_KEEP_ALIVE_SECONDS}s',  # ä¿æ´»æ—¶é—´
                    'options': {
                        'num_predict': 1,  # åªç”Ÿæˆ1ä¸ªtoken
                        'num_ctx': 512
                    }
                },
                timeout=120  # é¦–æ¬¡åŠ è½½å¯èƒ½è¾ƒæ…¢
            )
            
            if response.status_code == 200:
                elapsed = time.time() - start_time
                self.is_warmed_up = True
                print(f"   âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ (è€—æ—¶ {elapsed:.1f}s)")
                print(f"   â° æ¨¡å‹å°†ä¿æŒæ´»è·ƒ {MODEL_KEEP_ALIVE_SECONDS // 60} åˆ†é’Ÿ")
                return True
            else:
                print(f"   âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"   âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
            return False
    
    def set_keep_alive(self, seconds: int = MODEL_KEEP_ALIVE_SECONDS):
        """
        è®¾ç½®æ¨¡å‹ä¿æ´»æ—¶é—´
        
        Args:
            seconds: ä¿æ´»ç§’æ•°
        """
        if self.provider != LLMProvider.OLLAMA:
            return
        
        try:
            import requests
            
            # å‘é€ä¿æ´»è¯·æ±‚
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': '',  # ç©ºprompt
                    'stream': False,
                    'keep_alive': f'{seconds}s',
                    'options': {'num_predict': 0}
                },
                timeout=10
            )
            
            if response.status_code == 200:
                print(f"   â° æ¨¡å‹ä¿æ´»æ—¶é—´å·²è®¾ç½®ä¸º {seconds // 60} åˆ†é’Ÿ")
                
        except Exception as e:
            print(f"   âš ï¸ è®¾ç½®ä¿æ´»å¤±è´¥: {e}")
    
    def unload_model(self):
        """ç«‹å³å¸è½½æ¨¡å‹ï¼ˆé‡Šæ”¾æ˜¾å­˜/å†…å­˜ï¼‰"""
        if self.provider != LLMProvider.OLLAMA:
            return
        
        try:
            import requests
            
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': self.model,
                    'prompt': '',
                    'stream': False,
                    'keep_alive': '0s'  # ç«‹å³å¸è½½
                },
                timeout=10
            )
            
            if response.status_code == 200:
                self.is_warmed_up = False
                print(f"   ğŸ”» æ¨¡å‹ {self.model} å·²å¸è½½")
                
        except Exception as e:
            print(f"   âš ï¸ å¸è½½æ¨¡å‹å¤±è´¥: {e}")

    def _get_api_key(self) -> Optional[str]:
        """ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥"""
        if self.provider == LLMProvider.OPENAI:
            return os.getenv('OPENAI_API_KEY')
        elif self.provider == LLMProvider.ANTHROPIC:
            return os.getenv('ANTHROPIC_API_KEY')
        return None
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        if self.provider == LLMProvider.OLLAMA:
            # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
            if not self._check_ollama_service():
                print("âš ï¸ OllamaæœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨: ollama serve")
        elif self.provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:
            if not self.api_key:
                print(f"âš ï¸ æœªè®¾ç½® {self.provider.value.upper()}_API_KEY ç¯å¢ƒå˜é‡")
    
    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            import requests
            response = requests.get('http://localhost:11434/api/tags', timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        if not self.enable_cache:
            return
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                print(f"ğŸ“¦ å·²åŠ è½½ {len(self.cache)} æ¡ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            self.cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        if not self.enable_cache:
            return
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _get_content_hash(self, item: Dict) -> str:
        """è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œ"""
        content = f"{item.get('title', '')}|{item.get('summary', '')}|{item.get('source', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _build_classification_prompt(self, item: Dict) -> str:
        """æ„å»ºåˆ†ç±»æç¤ºè¯ï¼ˆç²¾ç®€ç‰ˆï¼Œå‡å°‘tokenæ¶ˆè€—ï¼‰"""
        title = item.get('title', '')[:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
        summary = item.get('summary', item.get('description', ''))[:300]  # å‡å°‘æ‘˜è¦é•¿åº¦
        source = item.get('source', '')
        
        # ç²¾ç®€ç‰ˆpromptï¼Œå¤§å¹…å‡å°‘token
        prompt = f"""åˆ†ç±»AIå†…å®¹ã€‚è¾“å‡ºJSONæ ¼å¼ã€‚

æ ‡é¢˜: {title}
æ‘˜è¦: {summary}
æ¥æº: {source}

ç±»å‹é€‰é¡¹: research(è®ºæ–‡ç ”ç©¶), product(äº§å“å‘å¸ƒ), market(å¸‚åœºèèµ„), developer(å¼€æºå·¥å…·), leader(é¢†è¢–è¨€è®º), community(ç¤¾åŒºè®¨è®º)

è¾“å‡ºæ ¼å¼(ä¸¥æ ¼JSON):
{{"content_type": "ç±»å‹", "confidence": 0.8, "tech_fields": ["é¢†åŸŸ"], "reasoning": "åŸå› "}}"""
        
        return prompt
    
    def _build_batch_prompt(self, items: List[Dict]) -> str:
        """æ„å»ºæ‰¹é‡åˆ†ç±»æç¤ºè¯ï¼ˆä¸€æ¬¡å¤„ç†å¤šæ¡ï¼‰"""
        items_text = []
        for i, item in enumerate(items, 1):
            title = item.get('title', '')[:80]
            summary = item.get('summary', item.get('description', ''))[:150]
            items_text.append(f"[{i}] {title}\n    {summary}")
        
        all_items = "\n".join(items_text)
        
        prompt = f"""æ‰¹é‡åˆ†ç±»ä»¥ä¸‹{len(items)}æ¡AIå†…å®¹ã€‚æ¯æ¡è¾“å‡ºä¸€è¡ŒJSONã€‚

{all_items}

ç±»å‹: research/product/market/developer/leader/community

è¾“å‡º{len(items)}è¡ŒJSON(æ¯è¡Œå¯¹åº”ä¸€æ¡,æŒ‰é¡ºåº):
{{"id": 1, "content_type": "ç±»å‹", "confidence": 0.8, "tech_fields": ["é¢†åŸŸ"]}}"""
        
        return prompt
    
    def _call_ollama(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Ollama API
        
        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. å¯¹äº Qwen3 ç­‰æ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼Œä½¿ç”¨ Chat API + think=false è·å¾—å¿«é€Ÿå“åº”
        2. å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨ Generate API å¹¶è§£æ thinking å­—æ®µï¼ˆå¦‚æœ‰ï¼‰
        """
        try:
            import requests
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ”¯æŒ think å‚æ•°çš„æ¨¡å‹ï¼ˆå¦‚ Qwen3ï¼‰
            use_chat_api = 'qwen3' in self.model.lower()
            
            # ä¿æ´»æ—¶é—´è®¾ç½®
            keep_alive = f'{MODEL_KEEP_ALIVE_SECONDS}s'
            
            if use_chat_api:
                # ä½¿ç”¨ Chat API + think=false å…³é—­æ€è€ƒæ¨¡å¼ï¼Œå¤§å¹…æå‡é€Ÿåº¦
                # æ ¹æ®GPUæ£€æµ‹ç»“æœè‡ªé€‚åº”é…ç½®
                options = self._get_ollama_options()
                
                response = requests.post(
                    'http://localhost:11434/api/chat',
                    json={
                        'model': self.model,
                        'messages': [
                            {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå†…å®¹åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†ç±»ç»“æœã€‚'},
                            {'role': 'user', 'content': prompt}
                        ],
                        'stream': False,
                        'think': False,  # å…³é—­æ€è€ƒæ¨¡å¼
                        'keep_alive': keep_alive,  # ä¿æŒæ¨¡å‹æ´»è·ƒ
                        'options': options
                    },
                    timeout=60 if self.gpu_info and self.gpu_info.ollama_gpu_supported else 90
                )
                
                if response.status_code == 200:
                    result = response.json()
                    message = result.get('message', {})
                    return message.get('content', '')
            else:
                # ä½¿ç”¨ Generate APIï¼ˆé€‚ç”¨äºå…¶ä»–æ¨¡å‹ï¼‰
                options = self._get_ollama_options()
                
                response = requests.post(
                    'http://localhost:11434/api/generate',
                    json={
                        'model': self.model,
                        'prompt': prompt,
                        'stream': False,
                        'keep_alive': keep_alive,  # ä¿æŒæ¨¡å‹æ´»è·ƒ
                        'options': options
                    },
                    timeout=120 if self.gpu_info and self.gpu_info.ollama_gpu_supported else 180
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # éƒ¨åˆ†æ¨¡å‹ä½¿ç”¨ thinking å­—æ®µå­˜å‚¨æ€è€ƒè¿‡ç¨‹
                    response_text = result.get('response', '')
                    thinking_text = result.get('thinking', '')
                    
                    # å¦‚æœ response ä¸ºç©ºä½† thinking æœ‰å†…å®¹ï¼Œä» thinking ä¸­æå–
                    if not response_text.strip() and thinking_text:
                        return thinking_text
                    
                    return response_text
            
            print(f"âš ï¸ Ollama APIé”™è¯¯: {response.status_code}")
            return None
                
        except Exception as e:
            print(f"âš ï¸ Ollamaè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _get_ollama_options(self) -> Dict:
        """è·å–Ollamaæ¨ç†é€‰é¡¹ï¼ˆæ ¹æ®GPUè‡ªé€‚åº”é…ç½®ï¼‰"""
        if self.ollama_options:
            return {
                'temperature': self.ollama_options.temperature,
                'num_predict': self.ollama_options.num_predict,
                'num_ctx': self.ollama_options.num_ctx,
                'num_thread': self.ollama_options.num_thread,
                'num_gpu': self.ollama_options.num_gpu
            }
        else:
            # é»˜è®¤é…ç½®
            return {
                'temperature': 0.1,
                'num_predict': 200,
                'num_ctx': 1024,
                'num_thread': 4
            }
    
    def _call_openai(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨OpenAI API"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå†…å®¹åˆ†ç±»åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†ç±»ç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=300
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âš ï¸ OpenAIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _call_anthropic(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Anthropic API"""
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=self.api_key)
            response = client.messages.create(
                model=self.model,
                max_tokens=300,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.content[0].text
            
        except Exception as e:
            print(f"âš ï¸ Anthropicè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _call_llm(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨LLMï¼ˆæ ¹æ®æä¾›å•†é€‰æ‹©ï¼‰"""
        if self.provider == LLMProvider.OLLAMA:
            return self._call_ollama(prompt)
        elif self.provider == LLMProvider.OPENAI:
            return self._call_openai(prompt)
        elif self.provider == LLMProvider.ANTHROPIC:
            return self._call_anthropic(prompt)
        return None
    
    def _parse_llm_response(self, response: str) -> Optional[Dict]:
        """è§£æLLMå“åº”
        
        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. JSONæ ¼å¼: {"content_type": "xxx", ...}
        2. çº¯æ–‡æœ¬æ ¼å¼: ç›´æ¥è¿”å›ç±»åˆ«åç§°ï¼ˆç”¨äº thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
        """
        if not response:
            return None
        
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            response = response.strip()
            
            # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if 'content_type' in result:
                    # è§„èŒƒåŒ–å­—æ®µ
                    result['content_type'] = result['content_type'].lower()
                    result['confidence'] = float(result.get('confidence', 0.8))
                    result['tech_fields'] = result.get('tech_fields', ['General AI'])
                    result['is_verified'] = result.get('is_verified', True)
                    result['reasoning'] = result.get('reasoning', '')
                    
                    return result
            
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«ï¼ˆæ”¯æŒ thinking æ¨¡å¼çš„æ¨¡å‹ï¼‰
            return self._extract_category_from_text(response)
            
        except json.JSONDecodeError as e:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ç±»åˆ«
            return self._extract_category_from_text(response)
        except Exception as e:
            print(f"âš ï¸ å“åº”è§£æå¤±è´¥: {e}")
        
        return None
    
    def _extract_category_from_text(self, text: str) -> Optional[Dict]:
        """ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­æå–ç±»åˆ«
        
        ç”¨äºå¤„ç†ä½¿ç”¨ thinking æ¨¡å¼çš„æ¨¡å‹è¾“å‡º
        """
        if not text:
            return None
        
        text_lower = text.lower()
        
        # å®šä¹‰ç±»åˆ«å…³é”®è¯æ˜ å°„
        category_keywords = {
            'llm': ['llm', 'large language model', 'language model', 'gpt', 'chatgpt', 'claude', 'gemini'],
            'product': ['product', 'launch', 'release', 'announce', 'new feature'],
            'research': ['research', 'paper', 'study', 'academic', 'arxiv', 'conference'],
            'industry': ['industry', 'business', 'company', 'enterprise', 'market'],
            'tools': ['tool', 'framework', 'library', 'sdk', 'api'],
            'ethics': ['ethics', 'safety', 'regulation', 'policy', 'bias', 'fairness'],
            'vision': ['vision', 'image', 'video', 'computer vision', 'visual'],
            'robotics': ['robot', 'robotics', 'autonomous', 'embodied'],
        }
        
        # é¦–å…ˆæ£€æŸ¥æ–‡æœ¬æœ«å°¾æ˜¯å¦æœ‰æ˜ç¡®çš„ç±»åˆ«åç§°ï¼ˆR1æ¨¡å‹é€šå¸¸åœ¨æœ€åç»™å‡ºç­”æ¡ˆï¼‰
        lines = text.strip().split('\n')
        last_lines = ' '.join(lines[-3:]) if len(lines) >= 3 else text
        
        for category in category_keywords.keys():
            # æ£€æŸ¥æœ€åå‡ è¡Œæ˜¯å¦åŒ…å«æ˜ç¡®çš„ç±»åˆ«åç§°
            if category in last_lines.lower().split():
                return {
                    'content_type': category,
                    'confidence': 0.85,
                    'tech_fields': ['General AI'],
                    'is_verified': True,
                    'reasoning': 'Extracted from LLM thinking output'
                }
        
        # å¦‚æœæœ«å°¾æ²¡æœ‰æ˜ç¡®ç±»åˆ«ï¼Œç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        category_scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            return {
                'content_type': best_category,
                'confidence': min(0.7 + category_scores[best_category] * 0.05, 0.9),
                'tech_fields': ['General AI'],
                'is_verified': True,
                'reasoning': f'Inferred from text analysis (score: {category_scores[best_category]})'
            }
        
        return None
    
    def classify_item(self, item: Dict, use_cache: bool = True) -> Dict:
        """
        ä½¿ç”¨LLMåˆ†ç±»å•ä¸ªå†…å®¹é¡¹
        
        Args:
            item: å†…å®¹é¡¹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹
        """
        self.stats['total_calls'] += 1
        
        classified = item.copy()
        content_hash = self._get_content_hash(item)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache and self.enable_cache and content_hash in self.cache:
            self.stats['cache_hits'] += 1
            cached = self.cache[content_hash]
            classified.update(cached)
            classified['from_cache'] = True
            return classified
        
        # è°ƒç”¨LLM
        prompt = self._build_classification_prompt(item)
        response = self._call_llm(prompt)
        result = self._parse_llm_response(response)
        
        if result:
            self.stats['llm_calls'] += 1
            
            # æ›´æ–°åˆ†ç±»ç»“æœ
            classified['content_type'] = result['content_type']
            classified['confidence'] = result['confidence']
            classified['tech_categories'] = result['tech_fields']
            classified['is_verified'] = result['is_verified']
            classified['llm_reasoning'] = result['reasoning']
            classified['classified_by'] = f"llm:{self.provider.value}/{self.model}"
            classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # ä½¿ç”¨è§„åˆ™åˆ†ç±»å™¨è¡¥å……åœ°åŒºä¿¡æ¯
            classified['region'] = self.rule_classifier.classify_region(item)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.enable_cache:
                self.cache[content_hash] = {
                    'content_type': classified['content_type'],
                    'confidence': classified['confidence'],
                    'tech_categories': classified['tech_categories'],
                    'is_verified': classified['is_verified'],
                    'llm_reasoning': classified['llm_reasoning'],
                    'region': classified['region']
                }
        else:
            # LLMå¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»
            self.stats['fallback_calls'] += 1
            self.stats['errors'] += 1
            
            print(f"âš ï¸ LLMåˆ†ç±»å¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»: {item.get('title', '')[:30]}...")
            classified = self.rule_classifier.classify_item(item)
            classified['classified_by'] = 'rule:fallback'
        
        return classified
    
    def classify_batch(self, items: List[Dict], show_progress: bool = True, 
                       use_batch_api: bool = True) -> List[Dict]:
        """
        æ‰¹é‡åˆ†ç±»ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼‰
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            use_batch_api: æ˜¯å¦ä½¿ç”¨æ‰¹é‡APIï¼ˆä¸€æ¬¡è°ƒç”¨åˆ†ç±»å¤šæ¡ï¼Œæ›´å¿«ï¼‰
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹åˆ—è¡¨
        """
        total = len(items)
        
        # å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œåˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„å†…å®¹
        cached_items = []
        uncached_items = []
        uncached_indices = []
        
        for i, item in enumerate(items):
            content_hash = self._get_content_hash(item)
            if self.enable_cache and content_hash in self.cache:
                self.stats['cache_hits'] += 1
                self.stats['total_calls'] += 1
                classified = item.copy()
                classified.update(self.cache[content_hash])
                classified['from_cache'] = True
                cached_items.append((i, classified))
            else:
                uncached_items.append(item)
                uncached_indices.append(i)
        
        cached_count = len(cached_items)
        uncached_count = len(uncached_items)
        
        print(f"\nğŸ¤– å¼€å§‹LLMæ‰¹é‡åˆ†ç±» ({total} æ¡å†…å®¹)")
        print(f"   æä¾›å•†: {self.provider.value} | æ¨¡å‹: {self.model}")
        print(f"   å¹¶å‘æ•°: {self.max_workers} | ç¼“å­˜å‘½ä¸­: {cached_count}/{total}")
        
        if uncached_count == 0:
            print(f"   âœ¨ å…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼Œè·³è¿‡LLMè°ƒç”¨")
            cached_items.sort(key=lambda x: x[0])
            return [item for _, item in cached_items]
        
        # æ¨¡å‹é¢„çƒ­ï¼ˆä»…Ollamaä¸”æœªé¢„çƒ­æ—¶ï¼‰
        if self.provider == LLMProvider.OLLAMA and not self.is_warmed_up:
            self.warmup_model()
        
        start_time = time.time()
        classified_uncached = []
        
        # é€‰æ‹©åˆ†ç±»ç­–ç•¥
        if use_batch_api and self.batch_size > 1 and self.provider == LLMProvider.OLLAMA:
            # æ‰¹é‡APIæ¨¡å¼ï¼šä¸€æ¬¡è°ƒç”¨åˆ†ç±»å¤šæ¡ï¼ˆæ›´å¿«ï¼‰
            print(f"   æ¨¡å¼: æ‰¹é‡åˆ†ç±» (æ¯æ‰¹ {self.batch_size} æ¡)")
            classified_uncached = self._classify_batch_mode(uncached_items, uncached_indices, show_progress)
        else:
            # å¹¶å‘å•æ¡æ¨¡å¼
            print(f"   æ¨¡å¼: å¹¶å‘å•æ¡")
            classified_uncached = self._classify_concurrent_mode(uncached_items, uncached_indices, show_progress)
        
        # åˆå¹¶ç»“æœ
        all_items = cached_items + classified_uncached
        all_items.sort(key=lambda x: x[0])
        result = [item for _, item in all_items]
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache()
        
        # ç»Ÿè®¡
        elapsed = time.time() - start_time
        self._print_stats(elapsed)
        
        return result
    
    def _classify_batch_mode(self, items: List[Dict], indices: List[int], 
                             show_progress: bool) -> List[Tuple[int, Dict]]:
        """æ‰¹é‡åˆ†ç±»æ¨¡å¼ï¼šä¸€æ¬¡LLMè°ƒç”¨å¤„ç†å¤šæ¡å†…å®¹"""
        results = []
        total = len(items)
        total_batches = (total + self.batch_size - 1) // self.batch_size
        
        # åˆ†æ‰¹å¤„ç†
        batch_num = 0
        for batch_start in range(0, total, self.batch_size):
            batch_start_time = time.time()
            batch_num += 1
            batch_end = min(batch_start + self.batch_size, total)
            batch_items = items[batch_start:batch_end]
            batch_indices = indices[batch_start:batch_end]
            
            # æ„å»ºæ‰¹é‡prompt
            prompt = self._build_batch_prompt(batch_items)
            response = self._call_llm(prompt)
            batch_results = self._parse_batch_response(response, len(batch_items))
            
            # å¤„ç†ç»“æœ
            for i, (item, idx) in enumerate(zip(batch_items, batch_indices)):
                self.stats['total_calls'] += 1
                classified = item.copy()
                
                if batch_results and i < len(batch_results) and batch_results[i]:
                    result = batch_results[i]
                    self.stats['llm_calls'] += 1
                    
                    classified['content_type'] = result.get('content_type', 'market')
                    classified['confidence'] = result.get('confidence', 0.7)
                    classified['tech_categories'] = result.get('tech_fields', ['General AI'])
                    classified['is_verified'] = result.get('is_verified', True)
                    classified['llm_reasoning'] = result.get('reasoning', '')
                    classified['classified_by'] = f"llm:batch:{self.provider.value}/{self.model}"
                    classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    classified['region'] = self.rule_classifier.classify_region(item)
                    
                    # ç¼“å­˜
                    content_hash = self._get_content_hash(item)
                    if self.enable_cache:
                        self.cache[content_hash] = {
                            'content_type': classified['content_type'],
                            'confidence': classified['confidence'],
                            'tech_categories': classified['tech_categories'],
                            'is_verified': classified.get('is_verified', True),
                            'llm_reasoning': classified.get('llm_reasoning', ''),
                            'region': classified['region']
                        }
                else:
                    # æ‰¹é‡å¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™åˆ†ç±»
                    self.stats['fallback_calls'] += 1
                    classified = self.rule_classifier.classify_item(item)
                    classified['classified_by'] = 'rule:batch_fallback'
                
                results.append((idx, classified))
            
            if show_progress:
                completed = min(batch_end, total)
                batch_time = time.time() - batch_start_time
                remaining_batches = total_batches - batch_num
                estimated_remaining = batch_time * remaining_batches
                
                if remaining_batches > 0:
                    print(f"   è¿›åº¦: {completed}/{total} ({completed/total:.0%}) | æœ¬æ‰¹è€—æ—¶: {batch_time:.1f}ç§’ | é¢„è®¡å‰©ä½™: {estimated_remaining:.0f}ç§’")
                else:
                    print(f"   è¿›åº¦: {completed}/{total} ({completed/total:.0%}) | æœ¬æ‰¹è€—æ—¶: {batch_time:.1f}ç§’")
        
        return results
    
    def _classify_concurrent_mode(self, items: List[Dict], indices: List[int],
                                   show_progress: bool) -> List[Tuple[int, Dict]]:
        """å¹¶å‘å•æ¡åˆ†ç±»æ¨¡å¼"""
        results = []
        total = len(items)
        last_progress_time = time.time()
        last_progress_count = 0
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.classify_item, item): (i, idx) 
                      for i, (item, idx) in enumerate(zip(items, indices))}
            
            completed = 0
            for future in as_completed(futures):
                try:
                    result = future.result()
                    _, idx = futures[future]
                    results.append((idx, result))
                    completed += 1
                    
                    if show_progress and completed % 5 == 0:
                        current_time = time.time()
                        interval_time = current_time - last_progress_time
                        interval_count = completed - last_progress_count
                        
                        if interval_count > 0 and interval_time > 0:
                            rate = interval_count / interval_time  # æ¡/ç§’
                            remaining = total - completed
                            estimated_remaining = remaining / rate if rate > 0 else 0
                            print(f"   è¿›åº¦: {completed}/{total} ({completed/total:.0%}) | é€Ÿåº¦: {rate:.1f}æ¡/ç§’ | é¢„è®¡å‰©ä½™: {estimated_remaining:.0f}ç§’")
                        else:
                            print(f"   è¿›åº¦: {completed}/{total} ({completed/total:.0%})")
                        
                        last_progress_time = current_time
                        last_progress_count = completed
                        
                except Exception as e:
                    print(f"âš ï¸ åˆ†ç±»ä»»åŠ¡å¤±è´¥: {e}")
                    self.stats['errors'] += 1
        
        return results
    
    def _parse_batch_response(self, response: str, expected_count: int) -> List[Optional[Dict]]:
        """è§£ææ‰¹é‡åˆ†ç±»å“åº”"""
        results = [None] * expected_count
        
        if not response:
            return results
        
        try:
            # å°è¯•æŒ‰è¡Œè§£æJSON
            lines = response.strip().split('\n')
            json_objects = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æŸ¥æ‰¾JSON
                start = line.find('{')
                end = line.rfind('}') + 1
                
                if start >= 0 and end > start:
                    try:
                        obj = json.loads(line[start:end])
                        json_objects.append(obj)
                    except json.JSONDecodeError:
                        continue
            
            # åŒ¹é…åˆ°ç»“æœ
            for obj in json_objects:
                idx = obj.get('id', len([r for r in results if r is not None]) + 1) - 1
                if 0 <= idx < expected_count:
                    results[idx] = {
                        'content_type': obj.get('content_type', 'market').lower(),
                        'confidence': float(obj.get('confidence', 0.7)),
                        'tech_fields': obj.get('tech_fields', ['General AI']),
                        'is_verified': obj.get('is_verified', True),
                        'reasoning': obj.get('reasoning', '')
                    }
            
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡å“åº”è§£æå¤±è´¥: {e}")
        
        return results
    
    def _print_stats(self, elapsed: float):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚: {self.stats['total_calls']}")
        print(f"   ç¼“å­˜å‘½ä¸­: {self.stats['cache_hits']} ({self.stats['cache_hits']/max(1,self.stats['total_calls']):.0%})")
        print(f"   LLMè°ƒç”¨: {self.stats['llm_calls']}")
        print(f"   è§„åˆ™é™çº§: {self.stats['fallback_calls']}")
        print(f"   é”™è¯¯æ•°: {self.stats['errors']}")
        print(f"   è€—æ—¶: {elapsed:.1f}ç§’")
        
        if self.stats['llm_calls'] > 0:
            avg_time = elapsed / self.stats['llm_calls']
            print(f"   å¹³å‡æ¯æ¡: {avg_time:.1f}ç§’")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache = {}
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        print("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")


def select_llm_provider() -> Tuple[str, str]:
    """
    äº¤äº’å¼é€‰æ‹©LLMæä¾›å•†å’Œæ¨¡å‹
    
    Returns:
        (provider, model)
    """
    print("\n" + "="*60)
    print("ğŸ¤– é€‰æ‹©LLMæä¾›å•†")
    print("="*60)
    
    print("\nå¯ç”¨çš„æä¾›å•†:")
    print("  1. Ollama (æœ¬åœ°å…è´¹) â­ æ¨è")
    print("  2. OpenAI (éœ€è¦APIå¯†é’¥)")
    print("  3. Anthropic (éœ€è¦APIå¯†é’¥)")
    
    provider_choice = input("\nè¯·é€‰æ‹©æä¾›å•† (1-3) [é»˜è®¤: 1]: ").strip() or '1'
    
    provider_map = {'1': 'ollama', '2': 'openai', '3': 'anthropic'}
    provider = provider_map.get(provider_choice, 'ollama')
    
    # é€‰æ‹©æ¨¡å‹
    print(f"\nå¯ç”¨çš„ {provider.upper()} æ¨¡å‹:")
    
    provider_enum = LLMProvider(provider)
    models = AVAILABLE_MODELS.get(provider_enum, {})
    
    model_list = list(models.keys())
    for i, (model_id, info) in enumerate(models.items(), 1):
        recommended = " â­" if i == 1 else ""
        print(f"  {i}. {info['name']}{recommended}")
        print(f"     {info['description']}")
    
    model_choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(model_list)}) [é»˜è®¤: 1]: ").strip() or '1'
    
    try:
        model_idx = int(model_choice) - 1
        model = model_list[model_idx] if 0 <= model_idx < len(model_list) else model_list[0]
    except:
        model = model_list[0]
    
    print(f"\nâœ… å·²é€‰æ‹©: {provider} / {model}")
    
    return provider, model


def get_available_ollama_models() -> List[str]:
    """è·å–æœ¬åœ°å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
    try:
        import requests
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
    except:
        pass
    return []


def check_ollama_status() -> Dict:
    """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
    result = {
        'running': False,
        'models': [],
        'recommended': None
    }
    
    try:
        import requests
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            result['running'] = True
            data = response.json()
            result['models'] = [model['name'] for model in data.get('models', [])]
            
            # æ¨èæ¨¡å‹ä¼˜å…ˆçº§
            preferred = ['qwen3:8b', 'qwen3:4b', 'llama3.2:3b', 'mistral:7b']
            for model in preferred:
                if model in result['models']:
                    result['recommended'] = model
                    break
            
            if not result['recommended'] and result['models']:
                result['recommended'] = result['models'][0]
                
    except Exception as e:
        result['error'] = str(e)
    
    return result


# ä¾¿æ·å‡½æ•°
def create_llm_classifier(auto_select: bool = False) -> LLMClassifier:
    """
    åˆ›å»ºLLMåˆ†ç±»å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        auto_select: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
        
    Returns:
        LLMClassifierå®ä¾‹
    """
    if auto_select:
        # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollama
        status = check_ollama_status()
        if status['running'] and status['recommended']:
            return LLMClassifier(provider='ollama', model=status['recommended'])
        
        # æ£€æŸ¥OpenAI
        if os.getenv('OPENAI_API_KEY'):
            return LLMClassifier(provider='openai', model='gpt-4o-mini')
        
        # æ£€æŸ¥Anthropic
        if os.getenv('ANTHROPIC_API_KEY'):
            return LLMClassifier(provider='anthropic', model='claude-3-haiku-20240307')
        
        print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„LLMæœåŠ¡ï¼Œå°†ä½¿ç”¨è§„åˆ™åˆ†ç±»")
        return None
    else:
        # äº¤äº’å¼é€‰æ‹©
        provider, model = select_llm_provider()
        return LLMClassifier(provider=provider, model=model)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("="*60)
    print("LLMåˆ†ç±»å™¨æµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥OllamaçŠ¶æ€
    status = check_ollama_status()
    print(f"\nOllamaçŠ¶æ€: {'è¿è¡Œä¸­ âœ…' if status['running'] else 'æœªè¿è¡Œ âŒ'}")
    if status['models']:
        print(f"å¯ç”¨æ¨¡å‹: {', '.join(status['models'])}")
        print(f"æ¨èæ¨¡å‹: {status['recommended']}")
    
    # åˆ›å»ºåˆ†ç±»å™¨
    if status['running']:
        classifier = LLMClassifier(
            provider='ollama',
            model=status['recommended'] or 'qwen3:8b'
        )
        
        # æµ‹è¯•åˆ†ç±»
        test_item = {
            'title': 'OpenAI officially launches GPT-4o with new features',
            'summary': 'OpenAI announces the general availability of GPT-4o model with improved capabilities',
            'source': 'TechCrunch'
        }
        
        print(f"\næµ‹è¯•å†…å®¹: {test_item['title']}")
        result = classifier.classify_item(test_item)
        
        print(f"\nåˆ†ç±»ç»“æœ:")
        print(f"  ç±»å‹: {result.get('content_type')}")
        print(f"  ç½®ä¿¡åº¦: {result.get('confidence', 0):.1%}")
        print(f"  æŠ€æœ¯é¢†åŸŸ: {result.get('tech_categories')}")
        print(f"  å¯ä¿¡: {result.get('is_verified')}")
        print(f"  ç†ç”±: {result.get('llm_reasoning')}")
    else:
        print("\nâš ï¸ è¯·å…ˆå¯åŠ¨OllamaæœåŠ¡: ollama serve")

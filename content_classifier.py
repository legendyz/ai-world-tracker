"""
å†…å®¹åˆ†ç±»ç³»ç»Ÿ - Content Classifier
åŸºäºå…³é”®è¯å’Œè§„åˆ™å¯¹AIå†…å®¹è¿›è¡Œå¤šç»´åº¦åˆ†ç±»

åŒ…å«:
- ImportanceEvaluator: å¤šç»´åº¦é‡è¦æ€§è¯„ä¼°å™¨
- ContentClassifier: åŸºäºè§„åˆ™çš„å†…å®¹åˆ†ç±»å™¨
"""

from typing import Dict, List, Set, Tuple, Optional
import re
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import math
from collections import Counter
from logger import get_log_helper

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('classifier')


class ImportanceEvaluator:
    """
    å¤šç»´åº¦é‡è¦æ€§è¯„ä¼°å™¨
    
    è¯„ä¼°ç»´åº¦:
    1. æ¥æºæƒå¨åº¦ (source_authority) - 25%
    2. æ—¶æ•ˆæ€§ (recency) - 25%
    3. åˆ†ç±»ç½®ä¿¡åº¦ (confidence) - 20% (å¯¹ä½ä»·å€¼å†…å®¹è®¾ä¸Šé™)
    4. å†…å®¹ç›¸å…³åº¦ (relevance) - 20%
    5. ç¤¾äº¤çƒ­åº¦ (engagement) - 10%
    """
    
    def __init__(self):
        # ç»´åº¦æƒé‡é…ç½®
        self.weights = {
            'source_authority': 0.25,
            'recency': 0.25,
            'confidence': 0.20,
            'relevance': 0.20,
            'engagement': 0.10
        }
        
        # æ¥æºæƒå¨åº¦è¯„åˆ†
        self.source_authority_scores = {
            # å®˜æ–¹ä¸€æ‰‹æ¥æº (0.9-1.0)
            'openai.com': 1.0,
            'openai': 1.0,
            'blog.google': 1.0,
            'google ai': 0.95,
            'ai.meta.com': 1.0,
            'meta ai': 0.95,
            'anthropic.com': 1.0,
            'anthropic': 0.95,
            'microsoft.com': 0.95,
            'blogs.microsoft': 0.95,
            'nvidia': 0.90,
            'arxiv.org': 0.95,
            'arxiv': 0.95,
            'github.com': 0.90,
            'github': 0.90,
            'huggingface.co': 0.90,
            'hugging face': 0.90,
            
            # ä¸­å›½AIå…¬å¸å®˜æ–¹
            'baidu': 0.90,
            'ç™¾åº¦': 0.90,
            'alibaba': 0.90,
            'é˜¿é‡Œ': 0.90,
            'tencent': 0.90,
            'è…¾è®¯': 0.90,
            'deepseek': 0.90,
            'æ™ºè°±': 0.85,
            'æœˆä¹‹æš—é¢': 0.85,
            'kimi': 0.85,
            
            # ä¸“ä¸šåª’ä½“ (0.7-0.85)
            'techcrunch': 0.85,
            'theverge': 0.80,
            'the verge': 0.80,
            'wired': 0.80,
            'technologyreview': 0.85,
            'mit technology review': 0.85,
            'ieee spectrum': 0.85,
            'artificialintelligence-news': 0.80,
            'syncedreview': 0.80,
            'æœºå™¨ä¹‹å¿ƒ': 0.85,
            'jiqizhixin': 0.85,
            'é‡å­ä½': 0.80,
            'qbitai': 0.80,
            'infoq': 0.75,
            '36kr': 0.70,
            '36æ°ª': 0.70,
            'ithome': 0.70,
            'itä¹‹å®¶': 0.70,
            
            # ç¤¾åŒº/èšåˆ (0.5-0.7)
            'reddit': 0.65,
            'producthunt': 0.70,
            'product hunt': 0.70,
            'hacker news': 0.70,
            'hnrss': 0.70,
            
            # é€šç”¨æ–°é—» (0.4-0.6)
            'news.google': 0.50,
            'bing.com/news': 0.50,
            'reuters': 0.75,
            'bloomberg': 0.75,
            
            # ä¸ªäººåšå®¢/æ’­å®¢
            'sam altman': 0.90,
            'karpathy': 0.90,
            'andrej karpathy': 0.90,
            'lex fridman': 0.80,
        }
        
        # é«˜ä»·å€¼å…³é”®è¯ (ç”¨äºç›¸å…³åº¦è®¡ç®—) - åˆ†å±‚æƒé‡ç³»ç»Ÿ
        # ç¬¬ä¸€å±‚ï¼šçªç ´æ€§/é‡Œç¨‹ç¢‘äº‹ä»¶ (0.12-0.18)
        self.breakthrough_keywords = {
            'breakthrough': 0.18, 'sota': 0.15, 'state-of-the-art': 0.15,
            'world record': 0.15, 'revolutionary': 0.14, 'game-changer': 0.14,
            'milestone': 0.12, 'paradigm shift': 0.15, 'first-ever': 0.14,
            # ä¸­æ–‡
            'çªç ´': 0.18, 'é‡Œç¨‹ç¢‘': 0.14, 'é©å‘½æ€§': 0.14, 'é¢ è¦†': 0.12,
            'å²ä¸Šé¦–æ¬¡': 0.15, 'é‡å¤§çªç ´': 0.16,
        }
        
        # ç¬¬äºŒå±‚ï¼šå‘å¸ƒ/å…¬å‘Šäº‹ä»¶ (0.08-0.12)
        self.release_keywords = {
            'release': 0.10, 'launch': 0.10, 'announce': 0.10, 'unveil': 0.12,
            'introduce': 0.08, 'available': 0.08, 'official': 0.10,
            'beta': 0.06, 'preview': 0.06, 'alpha': 0.05,
            'general availability': 0.10, 'ga': 0.08, 'v1.0': 0.08,
            # ä¸­æ–‡
            'å‘å¸ƒ': 0.10, 'æ¨å‡º': 0.10, 'ä¸Šçº¿': 0.10, 'æ­£å¼ç‰ˆ': 0.10,
            'å®˜å®£': 0.10, 'å®˜æ–¹': 0.08, 'å…¬æµ‹': 0.06, 'å†…æµ‹': 0.05,
        }
        
        # ç¬¬ä¸‰å±‚ï¼šæŠ€æœ¯/æ¨¡å‹ç›¸å…³ (0.05-0.10)
        self.tech_keywords = {
            'open source': 0.10, 'open-source': 0.10, 'opensource': 0.10,
            'benchmark': 0.08, 'evaluation': 0.06, 'paper': 0.06,
            'gpt': 0.06, 'llm': 0.06, 'transformer': 0.05, 'diffusion': 0.06,
            'multimodal': 0.08, 'reasoning': 0.08, 'chain-of-thought': 0.08,
            'agent': 0.08, 'agi': 0.10, 'agentic': 0.08,
            'fine-tune': 0.06, 'finetune': 0.06, 'rlhf': 0.07,
            'inference': 0.05, 'training': 0.05, 'dataset': 0.06,
            # ä¸­æ–‡
            'å¼€æº': 0.10, 'æ¨¡å‹': 0.05, 'å¤§æ¨¡å‹': 0.07, 'å¤šæ¨¡æ€': 0.08,
            'æ¨ç†': 0.06, 'è®­ç»ƒ': 0.05, 'å¾®è°ƒ': 0.06, 'æ•°æ®é›†': 0.06,
        }
        
        # ç¬¬å››å±‚ï¼šä¸€èˆ¬æ€§æè¿° (0.02-0.05)
        self.general_keywords = {
            'new': 0.03, 'update': 0.04, 'improve': 0.04, 'enhance': 0.04,
            'feature': 0.03, 'support': 0.03, 'capability': 0.04,
            'performance': 0.05, 'faster': 0.04, 'better': 0.03,
            # ä¸­æ–‡
            'æœ€æ–°': 0.04, 'æ›´æ–°': 0.04, 'å‡çº§': 0.05, 'ä¼˜åŒ–': 0.04,
            'æ–°å¢': 0.04, 'æ”¯æŒ': 0.03, 'åŠŸèƒ½': 0.03,
        }
        
        # è´Ÿé¢/é™æƒå…³é”®è¯
        self.negative_relevance_keywords = {
            'rumor': -0.08, 'speculation': -0.06, 'might': -0.03, 'may': -0.02,
            'could': -0.02, 'possibly': -0.04, 'unconfirmed': -0.08,
            'alleged': -0.06, 'reportedly': -0.04,
            # ä¸­æ–‡
            'ä¼ é—»': -0.08, 'æ®æ‚‰': -0.04, 'æˆ–å°†': -0.04, 'å¯èƒ½': -0.03,
            'ç–‘ä¼¼': -0.06, 'æœªç»è¯å®': -0.08,
        }
        
        # å†…å®¹ç±»å‹ç›¸å…³åº¦ç³»æ•°
        self.type_relevance_multipliers = {
            'research': 1.15,   # ç ”ç©¶ç±»é€šå¸¸ç›¸å…³åº¦é«˜
            'product': 1.12,    # äº§å“å‘å¸ƒé‡è¦
            'leader': 1.08,     # é¢†è¢–è¨€è®º
            'developer': 1.05,  # å¼€å‘è€…å†…å®¹
            'tutorial': 1.0,    # æ•™ç¨‹å†…å®¹
            'news': 0.95,       # æ–°é—»å¯èƒ½æ³›æ³›è€Œè°ˆ
            'market': 0.88,     # å¸‚åœºåˆ†æ
            'community': 0.85,  # ç¤¾åŒºè®¨è®º
            'opinion': 0.80,    # è§‚ç‚¹è¯„è®º
            'other': 0.75,      # å…¶ä»–å†…å®¹
        }
        
        # æ—¶æ•ˆæ€§è¡°å‡å‚æ•°
        self.recency_decay_rate = 0.12  # è¡°å‡ç‡ï¼Œå€¼è¶Šå¤§è¡°å‡è¶Šå¿«
        self.recency_min_score = 0.08   # æœ€ä½æ—¶æ•ˆåˆ†æ•°
        
        # ç¤¾äº¤çƒ­åº¦ç»Ÿä¸€é…ç½®
        self.engagement_config = {
            'github_stars': {'threshold_low': 100, 'threshold_high': 50000, 'weight': 1.0},
            'huggingface_downloads': {'threshold_low': 1000, 'threshold_high': 1000000, 'weight': 0.9},
            'reddit_score': {'threshold_low': 50, 'threshold_high': 5000, 'weight': 0.85},
            'hn_points': {'threshold_low': 30, 'threshold_high': 1000, 'weight': 0.85},
            'likes': {'threshold_low': 100, 'threshold_high': 10000, 'weight': 0.7},
            'comments': {'threshold_low': 20, 'threshold_high': 500, 'weight': 0.6},
        }
    
    def calculate_importance(self, item: Dict, 
                            classification_result: Optional[Dict] = None) -> Tuple[float, Dict]:
        """
        è®¡ç®—å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°
        
        Args:
            item: åŸå§‹æ•°æ®é¡¹
            classification_result: åˆ†ç±»ç»“æœï¼ŒåŒ…å« content_type, confidence ç­‰
            
        Returns:
            (importance_score, score_breakdown)
        """
        if classification_result is None:
            classification_result = {}
        
        breakdown = {}
        
        # 1. æ¥æºæƒå¨åº¦ (0-1)
        source_score = self._calculate_source_authority(item)
        breakdown['source_authority'] = round(source_score, 3)
        
        # 2. æ—¶æ•ˆæ€§ (0-1)
        recency_score = self._calculate_recency(item)
        breakdown['recency'] = round(recency_score, 3)
        
        # 3. åˆ†ç±»ç½®ä¿¡åº¦ (0-1) - å¯¹ä½ä»·å€¼å†…å®¹è®¾ç½®ä¸Šé™
        confidence = classification_result.get('confidence', 0.5)
        # ä½æ—¶æ•ˆå†…å®¹ï¼ˆ>14å¤©ï¼‰é™åˆ¶ç½®ä¿¡åº¦è´¡çŒ®
        if recency_score <= 0.50:  # 14å¤©ä»¥ä¸Šçš„å†…å®¹
            if source_score < 0.80:  # éå®˜æ–¹é«˜æƒå¨æ¥æº
                confidence = min(confidence, 0.60)  # ç½®ä¿¡åº¦ä¸Šé™60%
            else:
                confidence = min(confidence, 0.75)  # å®˜æ–¹æ¥æºä¸Šé™75%
        elif recency_score <= 0.70:  # 7-14å¤©çš„å†…å®¹
            if source_score < 0.70:
                confidence = min(confidence, 0.75)  # æ™®é€šæ¥æºä¸Šé™75%
        breakdown['confidence'] = round(confidence, 3)
        
        # 4. å†…å®¹ç›¸å…³åº¦ (0-1)
        content_type = classification_result.get('content_type', 'news')
        relevance_score = self._calculate_relevance(item, content_type)
        breakdown['relevance'] = round(relevance_score, 3)
        
        # 5. ç¤¾äº¤çƒ­åº¦ (0-1)
        engagement_score = self._calculate_engagement(item)
        breakdown['engagement'] = round(engagement_score, 3)
        
        # åŠ æƒæ±‚å’Œ
        total_score = (
            source_score * self.weights['source_authority'] +
            recency_score * self.weights['recency'] +
            confidence * self.weights['confidence'] +
            relevance_score * self.weights['relevance'] +
            engagement_score * self.weights['engagement']
        )
        
        # ç¡®ä¿åœ¨ 0-1 èŒƒå›´å†…
        importance = round(min(max(total_score, 0.0), 1.0), 3)
        
        return importance, breakdown
    
    def _calculate_source_authority(self, item: Dict) -> float:
        """
        è®¡ç®—æ¥æºæƒå¨åº¦
        
        Args:
            item: æ•°æ®é¡¹
            
        Returns:
            æƒå¨åº¦åˆ†æ•° 0-1
        """
        source = item.get('source', '').lower()
        url = item.get('url', '').lower()
        author = item.get('author', '').lower()
        
        # åˆå¹¶æ£€æŸ¥æ–‡æœ¬
        check_text = f"{source} {url} {author}"
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ¥æº
        best_score = 0.40  # é»˜è®¤å€¼
        
        for known_source, score in self.source_authority_scores.items():
            if known_source.lower() in check_text:
                best_score = max(best_score, score)
        
        return best_score
    
    def _calculate_recency(self, item: Dict) -> float:
        """
        è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•° - å¹³æ»‘æŒ‡æ•°è¡°å‡æ›²çº¿
        
        ä½¿ç”¨æŒ‡æ•°è¡°å‡å…¬å¼: score = max_score * e^(-decay_rate * days) + min_score
        
        è¡°å‡æ›²çº¿å‚è€ƒå€¼:
        - ä»Šå¤©: 1.0
        - 1å¤©å‰: ~0.89
        - 3å¤©å‰: ~0.70
        - 7å¤©å‰: ~0.44
        - 14å¤©å‰: ~0.22
        - 30å¤©å‰: ~0.10
        
        Args:
            item: æ•°æ®é¡¹
            
        Returns:
            æ—¶æ•ˆæ€§åˆ†æ•° 0-1
        """
        published = item.get('published', '')
        
        if not published:
            # æ— æ—¥æœŸä¿¡æ¯ï¼Œç»™ä¸­ç­‰åˆ†æ•°
            return 0.5
        
        try:
            # è§£ææ—¥æœŸ
            if isinstance(published, datetime):
                pub_date = published
            elif isinstance(published, str):
                # å°è¯•å¤šç§æ ¼å¼è§£æ
                try:
                    pub_date = date_parser.parse(published)
                except (ValueError, TypeError):
                    # å°è¯•ç®€å•æ ¼å¼
                    if len(published) >= 10:
                        pub_date = datetime.strptime(published[:10], '%Y-%m-%d')
                    else:
                        return 0.5
            else:
                return 0.5
            
            # è®¡ç®—å¤©æ•°å·®
            now = datetime.now()
            # å¤„ç†æ—¶åŒº
            if pub_date.tzinfo is not None and now.tzinfo is None:
                pub_date = pub_date.replace(tzinfo=None)
            
            days_ago = (now - pub_date).days
            
            # æœªæ¥æ—¥æœŸæˆ–ä»Šå¤©
            if days_ago <= 0:
                return 1.0
            
            # å¹³æ»‘æŒ‡æ•°è¡°å‡
            # score = (1 - min_score) * e^(-decay_rate * days) + min_score
            decay_rate = self.recency_decay_rate
            min_score = self.recency_min_score
            
            score = (1.0 - min_score) * math.exp(-decay_rate * days_ago) + min_score
            
            return round(max(min(score, 1.0), min_score), 3)
                
        except Exception:
            return 0.5
    
    def _calculate_relevance(self, item: Dict, content_type: str) -> float:
        """
        è®¡ç®—å†…å®¹ç›¸å…³åº¦ - åˆ†å±‚å…³é”®è¯ç³»ç»Ÿ
        
        ä½¿ç”¨åˆ†å±‚å…³é”®è¯ç³»ç»Ÿ:
        1. çªç ´æ€§/é‡Œç¨‹ç¢‘äº‹ä»¶ (æœ€é«˜æƒé‡)
        2. å‘å¸ƒ/å…¬å‘Šäº‹ä»¶ (é«˜æƒé‡)
        3. æŠ€æœ¯/æ¨¡å‹ç›¸å…³ (ä¸­æƒé‡)
        4. ä¸€èˆ¬æ€§æè¿° (ä½æƒé‡)
        å¹¶è€ƒè™‘è´Ÿé¢å…³é”®è¯é™æƒ
        
        Args:
            item: æ•°æ®é¡¹
            content_type: åˆ†ç±»ç±»å‹
            
        Returns:
            ç›¸å…³åº¦åˆ†æ•° 0-1
        """
        title = item.get('title', '').lower()
        summary = item.get('summary', '').lower()
        text = f"{title} {summary}"
        
        # åŸºç¡€åˆ†
        score = 0.25
        
        # åˆ†å±‚å…³é”®è¯åŒ¹é… - ä½¿ç”¨é›†åˆé¿å…é‡å¤è®¡åˆ†
        matched_keywords = set()
        layer_scores = {'breakthrough': 0, 'release': 0, 'tech': 0, 'general': 0}
        
        # ç¬¬ä¸€å±‚: çªç ´æ€§å…³é”®è¯ (æœ€é«˜ä»·å€¼)
        for keyword, boost in self.breakthrough_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['breakthrough'] += boost
                matched_keywords.add(keyword)
        
        # ç¬¬äºŒå±‚: å‘å¸ƒå…³é”®è¯
        for keyword, boost in self.release_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['release'] += boost
                matched_keywords.add(keyword)
        
        # ç¬¬ä¸‰å±‚: æŠ€æœ¯å…³é”®è¯
        for keyword, boost in self.tech_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['tech'] += boost
                matched_keywords.add(keyword)
        
        # ç¬¬å››å±‚: ä¸€èˆ¬å…³é”®è¯
        for keyword, boost in self.general_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['general'] += boost
                matched_keywords.add(keyword)
        
        # åˆ†å±‚åŠ åˆ†ï¼Œé«˜å±‚æ¬¡å…³é”®è¯æœ‰æ›´å¤§å½±å“
        # çªç ´å±‚å…¨é¢è®¡åˆ†ï¼Œå…¶ä»–å±‚æœ‰è¡°å‡
        score += layer_scores['breakthrough']  # 100% æƒé‡
        score += layer_scores['release'] * 0.9  # 90% æƒé‡
        score += layer_scores['tech'] * 0.8     # 80% æƒé‡
        score += layer_scores['general'] * 0.6  # 60% æƒé‡
        
        # è´Ÿé¢å…³é”®è¯é™æƒ
        for keyword, penalty in self.negative_relevance_keywords.items():
            if keyword in text:
                score += penalty  # penalty æ˜¯è´Ÿæ•°
        
        # æ ‡é¢˜ä¸­çš„å…³é”®è¯é¢å¤–åŠ åˆ† (æ ‡é¢˜é€šå¸¸æ›´é‡è¦)
        title_bonus = 0
        for keyword in matched_keywords:
            if keyword in title:
                title_bonus += 0.02  # æ¯ä¸ªæ ‡é¢˜ä¸­çš„å…³é”®è¯é¢å¤–+0.02
        score += min(title_bonus, 0.10)  # ä¸Šé™ 0.10
        
        # æ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´
        multiplier = self.type_relevance_multipliers.get(content_type, 0.9)
        score *= multiplier
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
        return round(max(min(score, 1.0), 0.1), 3)
    
    def _calculate_engagement(self, item: Dict) -> float:
        """
        è®¡ç®—ç¤¾äº¤çƒ­åº¦ - ç»Ÿä¸€å½’ä¸€åŒ–ç®—æ³•
        
        ä½¿ç”¨ç»Ÿä¸€çš„å¯¹æ•°å½’ä¸€åŒ–å…¬å¼:
        score = log(value + 1) / log(threshold_high + 1) * weight
        
        æ”¯æŒå¤šä¸ªç¤¾äº¤ä¿¡å·çš„åŠ æƒç»„åˆ
        
        Args:
            item: æ•°æ®é¡¹
            
        Returns:
            çƒ­åº¦åˆ†æ•° 0-1
        """
        signals = []
        
        # ç»Ÿä¸€çš„å½’ä¸€åŒ–å‡½æ•°
        def normalize_signal(value: int, config: dict) -> float:
            """ç»Ÿä¸€çš„å¯¹æ•°å½’ä¸€åŒ–"""
            if not value or value <= 0:
                return None
            
            threshold_low = config['threshold_low']
            threshold_high = config['threshold_high']
            weight = config['weight']
            
            # å¯¹æ•°å½’ä¸€åŒ–ï¼Œå¹¶åº”ç”¨æƒé‡
            # ä½äºé˜ˆå€¼çš„ç»™äºˆè¾ƒä½åˆ†
            if value < threshold_low:
                score = math.log(value + 1) / math.log(threshold_low + 1) * 0.4
            else:
                # åœ¨é˜ˆå€¼èŒƒå›´å†…çš„æ­£å¸¸è®¡åˆ†
                score = 0.4 + 0.6 * math.log(value / threshold_low + 1) / math.log(threshold_high / threshold_low + 1)
            
            return min(score * weight, 1.0)
        
        # GitHub stars
        stars = item.get('stars', 0)
        if stars:
            score = normalize_signal(stars, self.engagement_config['github_stars'])
            if score is not None:
                signals.append(('stars', score, self.engagement_config['github_stars']['weight']))
        
        # HuggingFace downloads
        downloads = item.get('downloads', 0)
        if downloads:
            score = normalize_signal(downloads, self.engagement_config['huggingface_downloads'])
            if score is not None:
                signals.append(('downloads', score, self.engagement_config['huggingface_downloads']['weight']))
        
        # Reddit score
        reddit_score = item.get('score', 0)
        if reddit_score and 'reddit' in item.get('source', '').lower():
            score = normalize_signal(reddit_score, self.engagement_config['reddit_score'])
            if score is not None:
                signals.append(('reddit', score, self.engagement_config['reddit_score']['weight']))
        
        # Hacker News points
        hn_points = item.get('points', 0)
        if hn_points:
            score = normalize_signal(hn_points, self.engagement_config['hn_points'])
            if score is not None:
                signals.append(('hn', score, self.engagement_config['hn_points']['weight']))
        
        # é€šç”¨likes
        likes = item.get('likes', item.get('favorites', 0))
        if likes:
            score = normalize_signal(likes, self.engagement_config['likes'])
            if score is not None:
                signals.append(('likes', score, self.engagement_config['likes']['weight']))
        
        # è¯„è®ºæ•°
        comments = item.get('comments', item.get('num_comments', 0))
        if comments:
            score = normalize_signal(comments, self.engagement_config['comments'])
            if score is not None:
                signals.append(('comments', score, self.engagement_config['comments']['weight']))
        
        # æ— ç¤¾äº¤æ•°æ®ï¼Œç»™ä¸­ç­‰åˆ†
        if not signals:
            return 0.5
        
        # åŠ æƒå¹³å‡
        total_weight = sum(s[2] for s in signals)
        weighted_sum = sum(s[1] for s in signals)
        
        # ç»„åˆå¤šä¸ªä¿¡å·æ—¶ç»™äºˆå°å¹…åŠ åˆ† (å¤šç»´åº¦éªŒè¯)
        multi_signal_bonus = min(len(signals) - 1, 3) * 0.03
        
        final_score = weighted_sum / total_weight + multi_signal_bonus
        
        return round(min(max(final_score, 0.0), 1.0), 3)
    
    def get_importance_level(self, score: float) -> Tuple[str, str]:
        """
        è·å–é‡è¦æ€§ç­‰çº§å’Œæ ‡ç­¾
        
        Args:
            score: é‡è¦æ€§åˆ†æ•°
            
        Returns:
            (ç­‰çº§, emojiæ ‡ç­¾)
        """
        if score >= 0.85:
            return 'critical', 'ğŸ”´'
        elif score >= 0.70:
            return 'high', 'ğŸŸ '
        elif score >= 0.55:
            return 'medium', 'ğŸŸ¡'
        elif score >= 0.40:
            return 'low', 'ğŸŸ¢'
        else:
            return 'minimal', 'âšª'


class ContentClassifier:
    """AIå†…å®¹æ™ºèƒ½åˆ†ç±»å™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self):
        # åˆå§‹åŒ–é‡è¦æ€§è¯„ä¼°å™¨
        self.importance_evaluator = ImportanceEvaluator()
        
        # å¦å®šè¯å’Œä¸ç¡®å®šæ€§è¯æ±‡ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.negative_words = {
            # å¼ºå¦å®š
            'not', 'no', 'never', 'fake', 'false', 'denied', 'debunk', 'refute',
            'ä¸æ˜¯', 'é', 'å¦è®¤', 'è™šå‡', 'è¾Ÿè°£',
            
            # ä¼ é—»/çŒœæµ‹
            'rumor', 'speculation', 'allegedly', 'unconfirmed', 'unverified',
            'might', 'could', 'possibly', 'potentially', 'reportedly',
            'ä¼ é—»', 'è°£è¨€', 'æœªç»è¯å®', 'æ®ç§°', 'å¯èƒ½', 'æˆ–è®¸', 'çŒœæµ‹',
            
            # å»¶æœŸ/å–æ¶ˆ
            'delayed', 'postponed', 'cancelled', 'canceled', 'suspended',
            'å»¶æœŸ', 'æ¨è¿Ÿ', 'å–æ¶ˆ', 'æš‚åœ'
        }
        
        # é«˜å¯ä¿¡åº¦æ¥æºï¼ˆç”¨äºæå‡ç½®ä¿¡åº¦ï¼‰
        self.trusted_sources = {
            'official', 'blog', 'press release', 'announcement', 'techcrunch',
            'reuters', 'bloomberg', 'the verge', 'wired',
            'å®˜æ–¹', 'å®˜ç½‘', 'æ–°é—»ç¨¿', 'å…¬å‘Š'
        }
        
        # ç ”ç©¶ç±»å…³é”®è¯ï¼ˆå¸¦æƒé‡ï¼‰- 2025å¹´æ›´æ–°ç‰ˆ
        self.research_keywords = {
            # é«˜æƒé‡ï¼ˆ3åˆ†ï¼‰- å¼ºç ”ç©¶æŒ‡æ ‡
            'arxiv': 3, 'conference': 3, 'journal': 3, 'paper': 3, 'publication': 3,
            'peer-reviewed': 3, 'proceedings': 3, 'academic': 3, 'neurips': 3, 'icml': 3,
            'iclr': 3, 'cvpr': 3, 'acl': 3, 'emnlp': 3, 'aaai': 3,
            'è®ºæ–‡': 3, 'å­¦æœ¯': 3, 'æœŸåˆŠ': 3, 'ä¼šè®®': 3,
            
            # ä¸­æƒé‡ï¼ˆ2åˆ†ï¼‰- ç ”ç©¶ç›¸å…³ + 2024-2025æ–°ç ”ç©¶æ–¹å‘
            'research': 2, 'study': 2, 'experiment': 2, 'methodology': 2,
            'findings': 2, 'analysis': 2, 'survey': 2, 'benchmark': 2,
            'state-of-the-art': 2, 'sota': 2, 'baseline': 2, 'ablation': 2,
            'reasoning': 2, 'chain-of-thought': 2, 'cot': 2, 'in-context learning': 2,
            'multimodal': 2, 'moe': 2, 'mixture of experts': 2, 'sparse': 2,
            'distillation': 2, 'quantization': 2, 'pruning': 2, 'efficient': 2,
            'scaling law': 2, 'emergent': 2, 'alignment': 2, 'rlhf': 2, 'dpo': 2,
            'ç ”ç©¶': 2, 'å®éªŒ': 2, 'åˆ†æ': 2, 'åŸºå‡†': 2, 'æ¶ˆè': 2,
            
            # ä½æƒé‡ï¼ˆ1åˆ†ï¼‰- æŠ€æœ¯æœ¯è¯­
            'algorithm': 1, 'model': 1, 'neural network': 1, 'deep learning': 1, 
            'machine learning': 1, 'architecture': 1, 'attention': 1, 'transformer': 1,
            'ç®—æ³•': 1, 'æ¨¡å‹': 1, 'ç¥ç»ç½‘ç»œ': 1, 'å­¦ä¹ ': 1
        }
        
        # å¼€å‘è€…ç±»å…³é”®è¯ï¼ˆå¸¦æƒé‡ï¼‰
        self.developer_keywords = {
            # é«˜æƒé‡ï¼ˆ3åˆ†ï¼‰- å¼ºå¼€å‘æŒ‡æ ‡
            'github': 3, 'repository': 3, 'open source': 3, 'commit': 3, 
            'pull request': 3, 'sdk': 3, 'api documentation': 3,
            'å¼€æº': 3, 'ä»“åº“': 3, 'ä»£ç åº“': 3,
            
            # ä¸­æƒé‡ï¼ˆ2åˆ†ï¼‰- å¼€å‘ç›¸å…³
            'library': 2, 'framework': 2, 'implementation': 2, 'tutorial': 2,
            'guide': 2, 'documentation': 2, 'developer': 2, 'programming': 2,
            'å¼€å‘': 2, 'åº“': 2, 'æ¡†æ¶': 2, 'æ•™ç¨‹': 2, 'æ–‡æ¡£': 2, 'æŒ‡å—': 2,
            
            # ä½æƒé‡ï¼ˆ1åˆ†ï¼‰- æŠ€æœ¯è¯æ±‡
            'code': 1, 'api': 1, 'package': 1, 'tool': 1,
            'ä»£ç ': 1, 'å·¥å…·': 1
        }
        
        # äº§å“ç±»å…³é”®è¯ï¼ˆå¸¦æƒé‡ï¼‰- 2025å¹´æ›´æ–°ç‰ˆ
        self.product_keywords = {
            # é«˜æƒé‡ï¼ˆ3åˆ†ï¼‰- å¼ºå‘å¸ƒæŒ‡æ ‡ + 2024-2025æ–°äº§å“å
            'official release': 3, 'officially launched': 3, 'announces launch': 3,
            'unveil': 3, 'debut': 3, 'available now': 3, 'now available': 3,
            'rolls out': 3, 'ships': 3, 'goes live': 3, 'general availability': 3,
            'gpt-4o': 3, 'gpt-4-turbo': 3, 'o1': 3, 'o1-preview': 3, 'o1-mini': 3, 'o3': 3,
            'claude-3': 3, 'claude-3.5': 3, 'claude-3-opus': 3, 'claude-3-sonnet': 3,
            'gemini': 3, 'gemini-pro': 3, 'gemini-ultra': 3, 'gemini 2.0': 3,
            'sora': 3, 'veo': 3, 'imagen 3': 3, 'firefly': 3,
            'llama-3': 3, 'llama-3.1': 3, 'llama-3.2': 3,
            'copilot': 3, 'github copilot': 3, 'cursor': 3,
            'æ­£å¼å‘å¸ƒ': 3, 'æ­£å¼æ¨å‡º': 3, 'æ­£å¼ä¸Šçº¿': 3, 'å®˜æ–¹å‘å¸ƒ': 3, 'å…¨é¢å¼€æ”¾': 3,
            'è±†åŒ…': 3, 'doubao': 3, 'kimi': 3, 'é€šä¹‰åƒé—®': 3, 'qwen': 3,
            'æ–‡å¿ƒä¸€è¨€': 3, 'ernie': 3, 'æ˜Ÿç«': 3, 'spark': 3,
            
            # ä¸­æƒé‡ï¼ˆ2åˆ†ï¼‰- å‘å¸ƒç›¸å…³
            'release': 2, 'launch': 2, 'announce': 2, 'introduce': 2,
            'version': 2, 'update': 2, 'available': 2, 'upgrade': 2,
            'new feature': 2, 'new model': 2, 'latest version': 2,
            'å‘å¸ƒ': 2, 'æ¨å‡º': 2, 'å®£å¸ƒ': 2, 'ä¸Šçº¿': 2, 'ç‰ˆæœ¬': 2, 'å‡çº§': 2, 'æ›´æ–°': 2,
            
            # ä½æƒé‡ï¼ˆ1åˆ†ï¼‰- äº§å“æœ¯è¯­
            'official': 1, 'commercial': 1, 'enterprise': 1, 'product': 1,
            'platform': 1, 'service': 1, 'solution': 1, 'beta': 1, 'preview': 1,
            'pro': 1, 'plus': 1, 'premium': 1, 'subscription': 1,
            'å®˜æ–¹': 1, 'å•†ä¸š': 1, 'ä¼ä¸š': 1, 'äº§å“': 1, 'å¹³å°': 1, 'æœåŠ¡': 1, 'å…¬æµ‹': 1, 'è®¢é˜…': 1
        }
        
        # å¸‚åœºç±»å…³é”®è¯ï¼ˆå¸¦æƒé‡ï¼‰- 2025å¹´æ›´æ–°ç‰ˆ
        self.market_keywords = {
            # é«˜æƒé‡ï¼ˆ3åˆ†ï¼‰- å¼ºå¸‚åœºæŒ‡æ ‡
            'funding round': 3, 'investment': 3, 'acquisition': 3, 'ipo': 3,
            'valuation': 3, 'revenue': 3, 'raises': 3, 'secures funding': 3,
            'series a': 3, 'series b': 3, 'series c': 3, 'series d': 3,
            'unicorn': 3, 'billion': 3, 'million': 3,
            'èèµ„': 3, 'æŠ•èµ„': 3, 'æ”¶è´­': 3, 'ä¸Šå¸‚': 3, 'ä¼°å€¼': 3,
            'è½®èèµ„': 3, 'ç‹¬è§’å…½': 3, 'äº¿ç¾å…ƒ': 3, 'äº¿å…ƒ': 3,
            
            # ä¸­æƒé‡ï¼ˆ2åˆ†ï¼‰- å¸‚åœºç›¸å…³ + æ”¿ç­–æ³•è§„
            'market': 2, 'business': 2, 'startup': 2, 'company': 2,
            'policy': 2, 'regulation': 2, 'industry': 2, 'layoff': 2, 'layoffs': 2,
            'antitrust': 2, 'lawsuit': 2, 'copyright': 2, 'license': 2,
            'ai act': 2, 'executive order': 2, 'ban': 2, 'restrict': 2,
            'å¸‚åœº': 2, 'ä¼ä¸š': 2, 'å…¬å¸': 2, 'æ”¿ç­–': 2, 'ç›‘ç®¡': 2, 'è¡Œä¸š': 2,
            'è£å‘˜': 2, 'åå„æ–­': 2, 'ç‰ˆæƒ': 2, 'åˆè§„': 2, 'æ³•æ¡ˆ': 2,
            
            # ä½æƒé‡ï¼ˆ1åˆ†ï¼‰- å•†ä¸šæœ¯è¯­
            'funding': 1, 'partnership': 1, 'collaboration': 1, 'deal': 1,
            'contract': 1, 'profit': 1, 'loss': 1, 'growth': 1,
            'åˆä½œ': 1, 'ä¼™ä¼´': 1, 'äº¤æ˜“': 1, 'åˆåŒ': 1, 'è¥æ”¶': 1, 'å¢é•¿': 1
        }
        
        # é¢†è¢–è¨€è®ºå…³é”®è¯ï¼ˆå¸¦æƒé‡ï¼‰
        self.leader_keywords = {
            # é«˜æƒé‡ï¼ˆ3åˆ†ï¼‰- å¼ºè¨€è®ºæŒ‡æ ‡
            'interview': 3, 'speech': 3, 'keynote': 3, 'statement': 3,
            'exclusive interview': 3, 'in an interview': 3,
            'é‡‡è®¿': 3, 'æ¼”è®²': 3, 'ä¸»é¢˜æ¼”è®²': 3, 'å£°æ˜': 3,
            
            # ä¸­æƒé‡ï¼ˆ2åˆ†ï¼‰- è¨€è®ºç›¸å…³
            'said': 2, 'stated': 2, 'believes': 2, 'warns': 2, 'predicts': 2,
            'opinion': 2, 'commented': 2,
            'è¡¨ç¤º': 2, 'è®¤ä¸º': 2, 'è­¦å‘Š': 2, 'é¢„æµ‹': 2, 'è¯„è®º': 2, 'è§‚ç‚¹': 2,
            
            # ä½æƒé‡ï¼ˆ1åˆ†ï¼‰- ç¤¾äº¤åª’ä½“
            'tweeted': 1, 'posted': 1, 'quote': 1,
            'è¯´': 1, 'å‘æ–‡': 1
        }
        
        # æŠ€æœ¯é¢†åŸŸå…³é”®è¯
        self.tech_categories = {
            'NLP': [
                'nlp', 'natural language', 'text mining', 'embedding', 'bert', 'transformer', 
                'sentiment analysis', 'translation', 'linguistics', 'tokenization',
                'è‡ªç„¶è¯­è¨€', 'æ–‡æœ¬æŒ–æ˜', 'è¯­ä¹‰', 'ç¿»è¯‘', 'è¯å‘é‡'
            ],
            'Computer Vision': [
                'vision', 'image', 'video', 'detection', 'recognition', 'segmentation', 'ocr',
                'yolo', 'resnet', 'vit', 'è§†è§‰', 'å›¾åƒ', 'è§†é¢‘', 'è¯†åˆ«', 'æ£€æµ‹'
            ],
            'Reinforcement Learning': [
                'reinforcement', 'rl', 'agent', 'policy', 'reward', 'q-learning', 'ppo',
                'å¼ºåŒ–å­¦ä¹ ', 'æ™ºèƒ½ä½“', 'å¥–åŠ±'
            ],
            'Generative AI': [
                'generative', 'generation', 'aigc', 'llm', 'large language model', 'foundation model',
                'gpt', 'chatgpt', 'claude', 'llama', 'mistral', 'gemini', 'copilot', 'grok',
                'sora', 'midjourney', 'dalle', 'stable diffusion', 'runway', 'pika', 'flux',
                'text-to-image', 'text-to-video', 'ç”Ÿæˆå¼', 'å¤§æ¨¡å‹', 'è¯­è¨€æ¨¡å‹', 'æ–‡ç”Ÿå›¾', 'æ–‡ç”Ÿè§†é¢‘'
            ],
            'MLOps': ['mlops', 'deployment', 'production', 'monitoring', 'pipeline', 'éƒ¨ç½²', 'è¿ç»´'],
            'AI Ethics': ['ethics', 'bias', 'fairness', 'privacy', 'safety', 'alignment', 'ä¼¦ç†', 'åè§', 'éšç§', 'å®‰å…¨', 'å¯¹é½']
        }
        
        # åŒºåŸŸå…³é”®è¯
        self.region_keywords = {
            'China': ['china', 'chinese', 'beijing', 'shanghai', 'baidu', 'alibaba', 'tencent', 'ä¸­å›½', 'ç™¾åº¦', 'é˜¿é‡Œ', 'è…¾è®¯'],
            'USA': ['usa', 'us', 'silicon valley', 'openai', 'google', 'microsoft', 'meta', 'ç¾å›½'],
            'Europe': ['europe', 'eu', 'european', 'mistral', 'deepmind', 'æ¬§æ´²'],
            'Global': ['global', 'international', 'worldwide', 'å…¨çƒ', 'å›½é™…']
        }
        
        # ============ æ–°å¢ï¼šä¸Šä¸‹æ–‡çŸ­è¯­åŒ¹é…æ¨¡å¼ ============
        self.phrase_patterns = {
            'product': [
                r'officially\s+(launched|released|announced)',
                r'now\s+available\s+(for|to|in)',
                r'rolling\s+out\s+to',
                r'is\s+now\s+(live|available|open)',
                r'has\s+(launched|released|unveiled)',
                r'introduces?\s+new',
                r'æ­£å¼(å‘å¸ƒ|ä¸Šçº¿|æ¨å‡º|å¼€æ”¾)',
                r'å…¨é¢(å¼€æ”¾|ä¸Šçº¿|æ¨å‡º)',
                r'(å¼€å§‹|å¼€å¯)(å†…æµ‹|å…¬æµ‹|å•†ç”¨)',
            ],
            'research': [
                r'we\s+propose',
                r'we\s+present',
                r'we\s+introduce\s+a\s+(new|novel)',
                r'our\s+(method|approach|model)\s+(achieves?|outperforms?)',
                r'state-of-the-art\s+(results?|performance)',
                r'benchmark\s+results?',
                r'experiments?\s+(show|demonstrate)',
                r'(æœ¬æ–‡|æˆ‘ä»¬)(æå‡º|ä»‹ç»|ç ”ç©¶)',
                r'(å®éªŒ|ç»“æœ)(è¡¨æ˜|æ˜¾ç¤º|è¯æ˜)',
            ],
            'market': [
                r'raises?\s+\$?\d+\s*(m|million|b|billion)',
                r'valued\s+at\s+\$',
                r'acquisition\s+of',
                r'acquires?\s+',
                r'ipo\s+(filing|plans?)',
                r'layoffs?\s+at',
                r'(è·å¾—|å®Œæˆ).{0,10}(èèµ„|æŠ•èµ„)',
                r'ä¼°å€¼.{0,5}(äº¿|ä¸‡)',
                r'(æ”¶è´­|å¹¶è´­)',
            ],
            'leader': [
                r'(ceo|cto|founder|chief).{0,20}(said|says|stated|believes)',
                r'in\s+(an\s+)?interview',
                r'(sam altman|elon musk|jensen huang|sundar pichai).{0,30}(said|says|warns|predicts)',
                r'(è¡¨ç¤º|è®¤ä¸º|æŒ‡å‡º|è­¦å‘Š|é¢„æµ‹).{0,10}(è¯´|ç§°)',
            ]
        }
        
        # ============ æ–°å¢ï¼šæ¥æºå…ˆéªŒæ¦‚ç‡ ============
        self.source_priors = {
            # ç ”ç©¶æº
            'arxiv': {'research': 0.95, 'developer': 0.02, 'product': 0.01, 'market': 0.01, 'leader': 0.01},
            'arxiv.org': {'research': 0.95, 'developer': 0.02, 'product': 0.01, 'market': 0.01, 'leader': 0.01},
            
            # å¼€å‘è€…æº
            'github': {'developer': 0.90, 'research': 0.05, 'product': 0.03, 'market': 0.01, 'leader': 0.01},
            'github.com': {'developer': 0.90, 'research': 0.05, 'product': 0.03, 'market': 0.01, 'leader': 0.01},
            'huggingface': {'developer': 0.70, 'research': 0.20, 'product': 0.05, 'market': 0.03, 'leader': 0.02},
            'hugging face': {'developer': 0.70, 'research': 0.20, 'product': 0.05, 'market': 0.03, 'leader': 0.02},
            
            # ç§‘æŠ€æ–°é—»æº
            'techcrunch': {'product': 0.40, 'market': 0.35, 'developer': 0.10, 'research': 0.05, 'leader': 0.10},
            'the verge': {'product': 0.45, 'market': 0.25, 'developer': 0.10, 'research': 0.05, 'leader': 0.15},
            'wired': {'product': 0.35, 'market': 0.25, 'research': 0.15, 'developer': 0.10, 'leader': 0.15},
            'mit technology review': {'research': 0.40, 'product': 0.25, 'market': 0.15, 'developer': 0.10, 'leader': 0.10},
            
            # ç¤¾åŒºæº
            'product hunt': {'product': 0.70, 'developer': 0.20, 'market': 0.05, 'research': 0.03, 'leader': 0.02},
            'hacker news': {'developer': 0.40, 'product': 0.25, 'research': 0.15, 'market': 0.10, 'leader': 0.10},
            
            # å®˜æ–¹åšå®¢
            'openai': {'product': 0.50, 'research': 0.30, 'developer': 0.10, 'leader': 0.08, 'market': 0.02},
            'google ai': {'product': 0.45, 'research': 0.35, 'developer': 0.10, 'leader': 0.05, 'market': 0.05},
            'microsoft': {'product': 0.50, 'developer': 0.25, 'market': 0.15, 'research': 0.05, 'leader': 0.05},
            'anthropic': {'product': 0.45, 'research': 0.35, 'developer': 0.10, 'leader': 0.05, 'market': 0.05},
            
            # ä¸­æ–‡æº
            '36æ°ª': {'market': 0.50, 'product': 0.35, 'leader': 0.08, 'developer': 0.05, 'research': 0.02},
            '36kr': {'market': 0.50, 'product': 0.35, 'leader': 0.08, 'developer': 0.05, 'research': 0.02},
            'æœºå™¨ä¹‹å¿ƒ': {'research': 0.35, 'product': 0.30, 'developer': 0.15, 'market': 0.10, 'leader': 0.10},
            'é‡å­ä½': {'product': 0.35, 'research': 0.30, 'market': 0.15, 'developer': 0.10, 'leader': 0.10},
            'itä¹‹å®¶': {'product': 0.50, 'market': 0.25, 'developer': 0.10, 'research': 0.05, 'leader': 0.10},
        }
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæé«˜æ€§èƒ½ï¼‰
        self._compiled_patterns = {}
        for category, patterns in self.phrase_patterns.items():
            self._compiled_patterns[category] = [re.compile(p, re.IGNORECASE) for p in patterns]
    
    def classify_content_type(self, item: Dict) -> Tuple[str, float, List[str]]:
        """
        åˆ†ç±»å†…å®¹ç±»å‹ï¼šç ”ç©¶/å¼€å‘è€…/äº§å“/å¸‚åœº/é¢†è¢–/ç¤¾åŒº
        
        å¢å¼ºç‰ˆç‰¹æ€§ï¼š
        - æ ‡é¢˜/å†…å®¹æƒé‡åˆ†ç¦»ï¼ˆæ ‡é¢˜æƒé‡ x1.5ï¼‰
        - ä¸Šä¸‹æ–‡çŸ­è¯­åŒ¹é…
        - æ¥æºå…ˆéªŒæ¦‚ç‡
        
        Args:
            item: å†…å®¹é¡¹ï¼ˆåŒ…å«title, summaryç­‰å­—æ®µï¼‰
            
        Returns:
            (ä¸»åˆ†ç±», ç½®ä¿¡åº¦åˆ†æ•° 0-1, æ¬¡è¦æ ‡ç­¾åˆ—è¡¨)
        """
        # å¦‚æœé‡‡é›†æ—¶å·²ç»æŒ‡å®šäº†ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
        category = item.get('category')
        if category in ['research', 'developer', 'product', 'market', 'leader', 'community']:
            return str(category), 1.0, []

        # ============ åˆ†ç¦»æ ‡é¢˜å’Œå†…å®¹ ============
        title = item.get('title', '').lower()
        summary = f"{item.get('summary', '')} {item.get('description', '')}".lower()
        full_text = f"{title} {summary}"
        source = item.get('source', '').lower()
        
        # æ£€æµ‹å¦å®šè¯å’Œå¯ä¿¡åº¦
        negative_score = self._detect_negative_context(full_text)
        source_trust = self._calculate_source_trust(source, full_text)
        
        # ç»å¯¹ä¼˜å…ˆè§„åˆ™ï¼šGitHubæ¥æºå¿…é¡»å½’ç±»ä¸ºå¼€å‘è€…ï¼ˆç»´æŒä¸å˜ï¼‰
        if 'github' in source or 'github.com' in full_text:
            secondary = self._get_secondary_labels(full_text, exclude='developer')
            return 'developer', 0.95, secondary
        
        # arXivæ¥æºå¿…é¡»å½’ç±»ä¸ºç ”ç©¶ï¼ˆç»´æŒä¸å˜ï¼‰
        if 'arxiv' in source or 'arxiv.org' in full_text:
            secondary = self._get_secondary_labels(full_text, exclude='research')
            return 'research', 0.95, secondary
        
        # äº§å“ç±»ä¸¥æ ¼è§„åˆ™ï¼šå¿…é¡»åŒæ—¶åŒ…å«å…¬å¸åç§°å’Œäº§å“å‘å¸ƒå…³é”®è¯
        company_indicators = ['google', 'microsoft', 'openai', 'anthropic', 'meta', 'apple', 'amazon', 
                             'baidu', 'alibaba', 'tencent', 'bytedance', 'huawei', 'xiaomi',
                             'ç™¾åº¦', 'é˜¿é‡Œ', 'è…¾è®¯', 'å­—èŠ‚', 'åä¸º', 'å°ç±³',
                             'deepseek', 'mistral', 'cohere', 'stability', 'midjourney', 'runway',
                             'æ™ºè°±', 'æœˆä¹‹æš—é¢', 'é›¶ä¸€ä¸‡ç‰©', 'ç™¾å·', 'ç§‘å¤§è®¯é£']
        
        has_company = any(company in full_text or company in source for company in company_indicators)
        
        # ============ æ–°å¢ï¼šæ ‡é¢˜/å†…å®¹åˆ†ç¦»åŠ æƒè¯„åˆ† ============
        all_keywords = {
            'research': self.research_keywords,
            'developer': self.developer_keywords,
            'product': self.product_keywords,
            'market': self.market_keywords,
            'leader': self.leader_keywords
        }
        
        scores = {}
        for cat, kw in all_keywords.items():
            # æ ‡é¢˜æƒé‡ x1.5ï¼Œå†…å®¹æƒé‡ x1.0
            title_score = self._calculate_weighted_score(title, kw) * 1.5
            summary_score = self._calculate_weighted_score(summary, kw)
            scores[cat] = title_score + summary_score
        
        # ============ æ–°å¢ï¼šä¸Šä¸‹æ–‡çŸ­è¯­åŒ¹é…åŠ åˆ† ============
        phrase_scores = self._calculate_phrase_scores(full_text)
        for cat, phrase_score in phrase_scores.items():
            if cat in scores:
                # çŸ­è¯­åŒ¹é…ç»™äºˆé¢å¤–åŠ åˆ†ï¼ˆæ¯ä¸ªåŒ¹é… +3 åˆ†ï¼‰
                scores[cat] += phrase_score * 3.0
        
        # ============ æ–°å¢ï¼šæ¥æºå…ˆéªŒæ¦‚ç‡åŠ æˆ ============
        scores = self._apply_source_prior(scores, source)
        
        # äº§å“ç±»åŠ æˆè§„åˆ™ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if has_company and scores['product'] > 0:
            scores['product'] *= 2.5
        elif scores['product'] > 0:
            scores['product'] *= 1.3
        
        # å¦å®šè¯å½±å“ï¼ˆæ”¹è¿›ç‰ˆï¼šæ ¹æ®å¦å®šå¼ºåº¦è°ƒæ•´ï¼‰
        if negative_score > 0:
            # å¼ºå¦å®šï¼ˆåˆ†æ•°é«˜ï¼‰= æ›´å¤§å¹…åº¦é™ä½
            negative_factor = max(0.2, 1 - (negative_score * 0.15))
            scores['product'] *= negative_factor
            scores['market'] *= (negative_factor + 0.2)  # å¸‚åœºç±»å—å½±å“è¾ƒå°
        
        # æ¥æºå¯ä¿¡åº¦åŠ æˆ
        if source_trust > 0:
            # å¯ä¿¡æ¥æºæå‡äº§å“å’Œç ”ç©¶ç±»åˆ†æ•°
            scores['product'] *= (1 + source_trust * 0.3)
            scores['research'] *= (1 + source_trust * 0.2)
        
        # è·å–ä¸»åˆ†ç±»å’Œæ¬¡è¦æ ‡ç­¾
        max_category = max(scores.items(), key=lambda x: x[1])
        confidence = self._calculate_confidence(scores, max_category[0])
        secondary_labels = self._get_secondary_labels_from_scores(scores, max_category[0])
        
        return max_category[0], confidence, secondary_labels
    
    def _calculate_phrase_scores(self, text: str) -> Dict[str, int]:
        """
        è®¡ç®—çŸ­è¯­åŒ¹é…åˆ†æ•°
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            å„åˆ†ç±»çš„çŸ­è¯­åŒ¹é…æ•°é‡
        """
        scores = {}
        for category, patterns in self._compiled_patterns.items():
            match_count = 0
            for pattern in patterns:
                if pattern.search(text):
                    match_count += 1
            scores[category] = match_count
        return scores
    
    def _apply_source_prior(self, scores: Dict[str, float], source: str) -> Dict[str, float]:
        """
        åº”ç”¨æ¥æºå…ˆéªŒæ¦‚ç‡
        
        Args:
            scores: å½“å‰å„åˆ†ç±»åˆ†æ•°
            source: æ¥æºå­—ç¬¦ä¸²
            
        Returns:
            è°ƒæ•´åçš„åˆ†æ•°
        """
        # æŸ¥æ‰¾åŒ¹é…çš„æ¥æº
        matched_prior = None
        for source_key, priors in self.source_priors.items():
            if source_key in source:
                matched_prior = priors
                break
        
        if matched_prior:
            # åº”ç”¨å…ˆéªŒæ¦‚ç‡åŠ æˆï¼ˆå…ˆéªŒæ¦‚ç‡ * æƒé‡ç³»æ•°ï¼‰
            for cat, prior in matched_prior.items():
                if cat in scores:
                    # é«˜å…ˆéªŒæ¦‚ç‡çš„åˆ†ç±»è·å¾—æ›´å¤šåŠ æˆ
                    boost = 1 + (prior * 0.5)  # æœ€é«˜åŠ æˆ 50%
                    scores[cat] *= boost
        
        return scores
    
    def classify_tech_category(self, item: Dict) -> List[str]:
        """
        åˆ†ç±»æŠ€æœ¯é¢†åŸŸï¼ˆå¯å¤šæ ‡ç­¾ï¼‰
        
        Args:
            item: å†…å®¹é¡¹
            
        Returns:
            æŠ€æœ¯é¢†åŸŸåˆ—è¡¨
        """
        text = f"{item.get('title', '')} {item.get('summary', '')} {item.get('description', '')}".lower()
        categories = []
        
        for category, keywords in self.tech_categories.items():
            score = self._calculate_keyword_score(text, keywords)
            if score > 0:
                categories.append(category)
        
        return categories if categories else ['General AI']
    
    def classify_region(self, item: Dict) -> str:
        """
        åˆ†ç±»åœ°åŒº
        
        Args:
            item: å†…å®¹é¡¹
            
        Returns:
            åœ°åŒºåˆ†ç±»
        """
        # å¦‚æœå·²æœ‰regionå­—æ®µ
        if 'region' in item and item['region']:
            return item['region']
        
        text = f"{item.get('title', '')} {item.get('summary', '')} {item.get('description', '')} {item.get('source', '')}".lower()
        
        scores = {}
        for region, keywords in self.region_keywords.items():
            scores[region] = self._calculate_keyword_score(text, keywords)
        
        max_region = max(scores.items(), key=lambda x: x[1])[0]
        return max_region if scores[max_region] > 0 else 'Global'
    
    def classify_item(self, item: Dict) -> Dict:
        """
        å¯¹å•ä¸ªå†…å®¹é¡¹è¿›è¡Œå®Œæ•´åˆ†ç±»
        
        Args:
            item: åŸå§‹å†…å®¹é¡¹
            
        Returns:
            æ·»åŠ äº†åˆ†ç±»ä¿¡æ¯çš„å†…å®¹é¡¹ï¼ŒåŒ…å«:
            - content_type: å†…å®¹ç±»å‹
            - confidence: åˆ†ç±»ç½®ä¿¡åº¦
            - importance: å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°
            - importance_breakdown: é‡è¦æ€§åˆ†æ•°æ˜ç»†
            - importance_level: é‡è¦æ€§ç­‰çº§
        """
        classified = item.copy()
        
        content_type, confidence, secondary_labels = self.classify_content_type(item)
        classified['content_type'] = content_type
        classified['confidence'] = round(confidence, 3)
        
        # æ·»åŠ æ¬¡è¦æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if secondary_labels:
            classified['secondary_labels'] = secondary_labels
        
        classified['tech_categories'] = self.classify_tech_category(item)
        classified['region'] = self.classify_region(item)
        classified['classified_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        classified['classified_by'] = 'rule'
        
        # è®¡ç®—å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°
        importance, importance_breakdown = self.importance_evaluator.calculate_importance(
            item,
            {'content_type': content_type, 'confidence': confidence}
        )
        classified['importance'] = importance
        classified['importance_breakdown'] = importance_breakdown
        
        # æ·»åŠ é‡è¦æ€§ç­‰çº§
        level, emoji = self.importance_evaluator.get_importance_level(importance)
        classified['importance_level'] = level
        
        # å¦‚æœç½®ä¿¡åº¦ä½äº0.6ï¼Œæ ‡è®°ä¸ºéœ€è¦äººå·¥å®¡æ ¸
        if confidence < 0.6:
            classified['needs_review'] = True
        
        return classified
    
    def classify_batch(self, items: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡åˆ†ç±»
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            
        Returns:
            åˆ†ç±»åçš„å†…å®¹é¡¹åˆ—è¡¨
        """
        log.dual_rule(f"æ­£åœ¨å¯¹ {len(items)} æ¡å†…å®¹è¿›è¡Œè§„åˆ™åˆ†ç±»...")
        
        classified_items = []
        for item in items:
            classified_items.append(self.classify_item(item))
        
        # ç»Ÿè®¡
        stats = self._calculate_statistics(classified_items)
        low_confidence = sum(1 for item in classified_items if item.get('confidence', 1) < 0.6)
        avg_confidence = sum(item.get('confidence', 0) for item in classified_items) / len(classified_items) if classified_items else 0
        
        # é‡è¦æ€§ç»Ÿè®¡
        avg_importance = sum(item.get('importance', 0) for item in classified_items) / len(classified_items) if classified_items else 0
        high_importance = sum(1 for item in classified_items if item.get('importance', 0) >= 0.70)
        
        log.dual_success("è§„åˆ™åˆ†ç±»å®Œæˆï¼")
        log.dual_data(f"ç ”ç©¶: {stats['research']} | å¼€å‘è€…: {stats['developer']} | äº§å“: {stats['product']} | å¸‚åœº: {stats['market']} | é¢†è¢–: {stats['leader']}")
        log.dual_data(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2%} | ä½ç½®ä¿¡åº¦(<60%): {low_confidence} æ¡")
        log.dual_data(f"å¹³å‡é‡è¦æ€§: {avg_importance:.2%} | é«˜é‡è¦æ€§(â‰¥70%): {high_importance} æ¡")
        
        return classified_items
    
    def _calculate_keyword_score(self, text: str, keywords) -> int:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°ï¼ˆæ—§ç‰ˆæœ¬ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        score = 0
        # æ”¯æŒSetå’ŒList
        keyword_list = list(keywords) if not isinstance(keywords, list) else keywords
        for keyword in keyword_list:
            if keyword in text:
                score += 1
        return score
    
    def _calculate_weighted_score(self, text: str, keywords: Dict[str, int]) -> float:
        """
        è®¡ç®—åŠ æƒå…³é”®è¯åˆ†æ•°
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            keywords: å…³é”®è¯åŠå…¶æƒé‡å­—å…¸
            
        Returns:
            åŠ æƒåˆ†æ•°
        """
        score = 0.0
        matched_keywords = []
        
        for keyword, weight in keywords.items():
            if keyword in text:
                # è®¡ç®—è¯é¢‘
                count = text.count(keyword)
                # TF-IDF ç®€åŒ–ç‰ˆï¼šè¯é¢‘ * æƒé‡ * logè¡°å‡
                keyword_score = weight * (1 + math.log(count)) if count > 0 else 0
                score += keyword_score
                matched_keywords.append(keyword)
        
        # è€ƒè™‘å…³é”®è¯å¤šæ ·æ€§ï¼šåŒ¹é…ä¸åŒå…³é”®è¯çš„æ•°é‡ä¹Ÿå¾ˆé‡è¦
        diversity_bonus = len(matched_keywords) * 0.5
        
        return score + diversity_bonus
    
    def _detect_negative_context(self, text: str) -> float:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­å¦å®šæˆ–ä¸ç¡®å®šæ€§è¡¨è¾¾çš„å¼ºåº¦
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            å¦å®šå¼ºåº¦åˆ†æ•° (0-5)ï¼Œ0è¡¨ç¤ºæ— å¦å®š
        """
        negative_score = 0.0
        
        # å¼ºå¦å®šè¯æƒé‡å­—å…¸
        negative_weights = {
            'fake': 3, 'false': 3, 'denied': 3, 'debunk': 2.5, 'not': 2,
            'è™šå‡': 3, 'å¦è®¤': 3, 'è¾Ÿè°£': 2.5, 'ä¸æ˜¯': 2,
            'rumor': 2, 'speculation': 2, 'unconfirmed': 2, 'allegedly': 1.5,
            'ä¼ é—»': 2, 'è°£è¨€': 2, 'æœªç»è¯å®': 2, 'æ®ç§°': 1.5,
            'delayed': 1.5, 'cancelled': 2, 'suspended': 1.5,
            'å»¶æœŸ': 1.5, 'å–æ¶ˆ': 2, 'æš‚åœ': 1.5,
            'might': 1, 'could': 1, 'possibly': 1,
            'å¯èƒ½': 1, 'æˆ–è®¸': 1
        }
        
        # å…³é”®åŠ¨ä½œè¯ï¼ˆç”¨äºåˆ¤æ–­å¦å®šè¯æ˜¯å¦ä¸æ ¸å¿ƒåŠ¨ä½œç›¸å…³ï¼‰
        action_words = ['release', 'launch', 'announce', 'unveil', 'publish',
                       'å‘å¸ƒ', 'æ¨å‡º', 'å®£å¸ƒ', 'å…¬å¸ƒ', 'ä¸Šçº¿']
        
        # æ£€æŸ¥å¦å®šè¯åŠå…¶ä¸Šä¸‹æ–‡
        for neg_word, weight in negative_weights.items():
            if neg_word in text:
                # æŸ¥æ‰¾æ‰€æœ‰å‡ºç°ä½ç½®
                pos = 0
                while pos < len(text):
                    pos = text.find(neg_word, pos)
                    if pos == -1:
                        break
                    
                    # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰å40å­—ç¬¦ï¼‰
                    context_start = max(0, pos - 40)
                    context_end = min(len(text), pos + 40)
                    context = text[context_start:context_end]
                    
                    # å¦‚æœå¦å®šè¯é™„è¿‘æœ‰æ ¸å¿ƒåŠ¨ä½œè¯ï¼Œå¢åŠ æƒé‡
                    if any(action in context for action in action_words):
                        negative_score += weight
                    else:
                        # å¦å®šè¯å­˜åœ¨ä½†ä¸ç›´æ¥å½±å“æ ¸å¿ƒåŠ¨ä½œï¼Œæƒé‡å‡åŠ
                        negative_score += weight * 0.5
                    
                    pos += len(neg_word)
        
        return min(5.0, negative_score)  # æœ€å¤§å€¼é™åˆ¶ä¸º5
    
    def _calculate_source_trust(self, source: str, text: str) -> float:
        """
        è®¡ç®—æ¥æºå¯ä¿¡åº¦
        
        Args:
            source: æ¥æºå­—ç¬¦ä¸²
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            å¯ä¿¡åº¦åˆ†æ•° (0-1)
        """
        trust_score = 0.0
        
        # æ£€æŸ¥å¯ä¿¡æ¥æºæ ‡è¯†
        for trusted in self.trusted_sources:
            if trusted in source or trusted in text:
                trust_score += 0.2
        
        return min(1.0, trust_score)
    
    def _get_secondary_labels(self, text: str, exclude: Optional[str] = None) -> List[str]:
        """
        è·å–æ¬¡è¦åˆ†ç±»æ ‡ç­¾ï¼ˆç”¨äºå¼ºåˆ¶è§„åˆ™åçš„è¡¥å……ï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            exclude: è¦æ’é™¤çš„ä¸»åˆ†ç±»
            
        Returns:
            æ¬¡è¦æ ‡ç­¾åˆ—è¡¨
        """
        scores = {
            'research': self._calculate_weighted_score(text, self.research_keywords),
            'developer': self._calculate_weighted_score(text, self.developer_keywords),
            'product': self._calculate_weighted_score(text, self.product_keywords),
            'market': self._calculate_weighted_score(text, self.market_keywords),
            'leader': self._calculate_weighted_score(text, self.leader_keywords)
        }
        
        if exclude:
            scores.pop(exclude, None)
        
        # åªè¿”å›åˆ†æ•° > 5 çš„æ¬¡è¦æ ‡ç­¾
        secondary = [cat for cat, score in scores.items() if score > 5]
        return secondary[:2]  # æœ€å¤šè¿”å›2ä¸ªæ¬¡è¦æ ‡ç­¾
    
    def _get_secondary_labels_from_scores(self, scores: Dict[str, float], primary: str) -> List[str]:
        """
        ä»åˆ†æ•°å­—å…¸ä¸­æå–æ¬¡è¦æ ‡ç­¾
        
        Args:
            scores: åˆ†æ•°å­—å…¸
            primary: ä¸»åˆ†ç±»
            
        Returns:
            æ¬¡è¦æ ‡ç­¾åˆ—è¡¨
        """
        # æ’é™¤ä¸»åˆ†ç±»
        secondary_scores = {k: v for k, v in scores.items() if k != primary}
        
        # è·å–æœ€é«˜åˆ†å’Œæ¬¡é«˜åˆ†
        sorted_scores = sorted(secondary_scores.items(), key=lambda x: x[1], reverse=True)
        
        secondary = []
        primary_score = scores[primary]
        
        # åªæœ‰å½“æ¬¡è¦åˆ†ç±»çš„åˆ†æ•° >= ä¸»åˆ†ç±»åˆ†æ•°çš„50%æ—¶æ‰æ·»åŠ 
        for cat, score in sorted_scores:
            if score >= primary_score * 0.5 and score > 3:
                secondary.append(cat)
                if len(secondary) >= 2:  # æœ€å¤š2ä¸ªæ¬¡è¦æ ‡ç­¾
                    break
        
        return secondary
    
    def _calculate_confidence(self, scores: Dict[str, float], winner: str) -> float:
        """
        è®¡ç®—åˆ†ç±»ç½®ä¿¡åº¦
        
        Args:
            scores: å„ç±»åˆ«åˆ†æ•°å­—å…¸
            winner: æœ€é«˜åˆ†ç±»åˆ«
            
        Returns:
            ç½®ä¿¡åº¦ (0-1)
        """
        if not scores or winner not in scores:
            return 0.0
        
        winner_score = scores[winner]
        
        # å¦‚æœåˆ†æ•°ä¸º0ï¼Œç½®ä¿¡åº¦æä½
        if winner_score == 0:
            return 0.1
        
        # è®¡ç®—ä¸ç¬¬äºŒåçš„å·®è·
        sorted_scores = sorted(scores.values(), reverse=True)
        if len(sorted_scores) < 2:
            return 0.8
        
        first_score = sorted_scores[0]
        second_score = sorted_scores[1]
        
        # é¿å…é™¤é›¶é”™è¯¯
        if first_score == 0:
            return 0.1
        
        # ç½®ä¿¡åº¦ = ç¬¬ä¸€ååˆ†æ•° / (ç¬¬ä¸€å + ç¬¬äºŒå) * ä¸ç¬¬äºŒåçš„å·®è·æ¯”ä¾‹
        score_ratio = first_score / (first_score + second_score)
        gap_ratio = (first_score - second_score) / first_score if first_score > 0 else 0
        
        # ç»¼åˆç½®ä¿¡åº¦ï¼šç»“åˆåˆ†æ•°æ¯”ä¾‹å’Œå·®è·
        confidence = (score_ratio * 0.6 + gap_ratio * 0.4)
        
        # å¦‚æœç¬¬ä¸€ååˆ†æ•°å¾ˆé«˜ï¼ˆ>15ï¼‰ï¼Œé€‚å½“æå‡ç½®ä¿¡åº¦
        if first_score > 15:
            confidence = min(0.95, confidence * 1.1)
        
        # å¦‚æœç¬¬ä¸€åå’Œç¬¬äºŒåéå¸¸æ¥è¿‘ï¼Œé™ä½ç½®ä¿¡åº¦
        if second_score > 0 and first_score / second_score < 1.5:
            confidence *= 0.8
        
        return min(0.99, max(0.1, confidence))
    
    def _calculate_statistics(self, items: List[Dict]) -> Dict:
        """è®¡ç®—åˆ†ç±»ç»Ÿè®¡"""
        stats = {'research': 0, 'developer': 0, 'product': 0, 'market': 0, 'leader': 0}
        
        for item in items:
            content_type = item.get('content_type', 'market')
            if content_type in stats:
                stats[content_type] += 1
        
        return stats
    
    def get_filtered_items(self, items: List[Dict], 
                          content_type: Optional[str] = None,
                          tech_category: Optional[str] = None,
                          region: Optional[str] = None) -> List[Dict]:
        """
        æ ¹æ®æ¡ä»¶è¿‡æ»¤å†…å®¹
        
        Args:
            items: åˆ†ç±»åçš„å†…å®¹åˆ—è¡¨
            content_type: å†…å®¹ç±»å‹è¿‡æ»¤
            tech_category: æŠ€æœ¯é¢†åŸŸè¿‡æ»¤
            region: åœ°åŒºè¿‡æ»¤
            
        Returns:
            è¿‡æ»¤åçš„å†…å®¹åˆ—è¡¨
        """
        filtered = items
        
        if content_type:
            filtered = [item for item in filtered if item.get('content_type') == content_type]
        
        if tech_category:
            filtered = [item for item in filtered if tech_category in item.get('tech_categories', [])]
        
        if region:
            filtered = [item for item in filtered if item.get('region') == region]
        
        return filtered


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    classifier = ContentClassifier()
    
    test_items = [
        {
            'title': 'GPT-5 Released by OpenAI',
            'summary': 'OpenAI announces the release of GPT-5 with improved capabilities',
            'source': 'TechNews'
        },
        {
            'title': 'New Research on Transformer Architecture',
            'summary': 'A breakthrough paper on attention mechanisms in neural networks',
            'source': 'arXiv'
        },
        {
            'title': 'ç™¾åº¦è·å¾—10äº¿ç¾å…ƒAIæŠ•èµ„',
            'summary': 'ä¸­å›½ç§‘æŠ€å·¨å¤´ç™¾åº¦å®£å¸ƒå®Œæˆæ–°ä¸€è½®èèµ„ï¼Œç”¨äºAIç ”å‘',
            'source': 'ä¸­å›½ç§‘æŠ€'
        }
    ]
    
    results = classifier.classify_batch(test_items)
    
    log.info("ğŸ“‹ åˆ†ç±»ç»“æœ:")
    for item in results:
        log.menu(f"\n  æ ‡é¢˜: {item['title']}")
        log.menu(f"  ç±»å‹: {item['content_type']} (ç½®ä¿¡åº¦: {item['confidence']:.1%})")
        if item.get('secondary_labels'):
            secondary_str = ', '.join(item['secondary_labels'])
            log.menu(f"  æ¬¡è¦: {secondary_str}")
        tech_str = ', '.join(item['tech_categories'])
        log.menu(f"  é¢†åŸŸ: {tech_str}")
        log.menu(f"  åœ°åŒº: {item['region']}")
        if item.get('needs_review'):
            log.warning("éœ€è¦äººå·¥å®¡æ ¸")

"""
é“¾æ¥éªŒè¯æµ‹è¯•è„šæœ¬
éªŒè¯é‡‡é›†çš„æ‰€æœ‰é“¾æ¥æ˜¯å¦å¯è®¿é—®
"""

import json
import requests
from typing import List, Dict
import time
from urllib.parse import urlparse


def validate_link_access(url: str, timeout: int = 10) -> bool:
    """
    éªŒè¯é“¾æ¥æ˜¯å¦å¯è®¿é—®
    
    Args:
        url: è¦éªŒè¯çš„URL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        æ˜¯å¦å¯è®¿é—®
    """
    if not url or not url.startswith(('http://', 'https://')):
        return False
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)
        return response.status_code < 400
    except Exception:
        return False


def test_data_links(json_file: str):
    """
    æµ‹è¯•JSONæ•°æ®æ–‡ä»¶ä¸­çš„æ‰€æœ‰é“¾æ¥
    
    Args:
        json_file: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
    """
    print("ğŸ” å¼€å§‹éªŒè¯æ•°æ®é“¾æ¥...")
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        all_items = data.get('data', [])
        
        if not all_items:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ•°æ®é¡¹")
            return
        
        total_items = len(all_items)
        links_checked = 0
        valid_links = 0
        
        print(f"ğŸ“Š å…±æœ‰ {total_items} æ¡æ•°æ®ï¼Œå¼€å§‹éªŒè¯é“¾æ¥...")
        print("-" * 60)
        
        for i, item in enumerate(all_items, 1):
            title = item.get('title', 'No title')[:50]
            url = item.get('url', '')
            pdf_url = item.get('pdf_url', '')
            clone_url = item.get('clone_url', '')
            source = item.get('source', 'Unknown')
            
            print(f"{i:2d}. {title}...")
            
            # æ£€æŸ¥ä¸»é“¾æ¥
            if url:
                links_checked += 1
                if validate_link_access(url):
                    valid_links += 1
                    print(f"    âœ… ä¸»é“¾æ¥å¯è®¿é—®: {urlparse(url).netloc}")
                else:
                    print(f"    âŒ ä¸»é“¾æ¥æ— æ³•è®¿é—®: {url}")
            
            # æ£€æŸ¥PDFé“¾æ¥
            if pdf_url and pdf_url != url:
                links_checked += 1
                if validate_link_access(pdf_url):
                    valid_links += 1
                    print(f"    âœ… PDFé“¾æ¥å¯è®¿é—®: {urlparse(pdf_url).netloc}")
                else:
                    print(f"    âŒ PDFé“¾æ¥æ— æ³•è®¿é—®: {pdf_url}")
            
            # æ£€æŸ¥å…‹éš†é“¾æ¥
            if clone_url and clone_url != url:
                links_checked += 1
                if validate_link_access(clone_url):
                    valid_links += 1
                    print(f"    âœ… å…‹éš†é“¾æ¥å¯è®¿é—®: {urlparse(clone_url).netloc}")
                else:
                    print(f"    âŒ å…‹éš†é“¾æ¥æ— æ³•è®¿é—®: {clone_url}")
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i % 10 == 0:
                print(f"    ... å·²æ£€æŸ¥ {i}/{total_items} æ¡æ•°æ®")
                time.sleep(1)
        
        print("-" * 60)
        print(f"ğŸ¯ éªŒè¯å®Œæˆï¼")
        print(f"ğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
        print(f"   - æ€»æ•°æ®é¡¹: {total_items}")
        print(f"   - æ€»é“¾æ¥æ•°: {links_checked}")
        print(f"   - æœ‰æ•ˆé“¾æ¥: {valid_links}")
        print(f"   - æˆåŠŸç‡: {valid_links/links_checked*100:.1f}%" if links_checked > 0 else "   - æˆåŠŸç‡: 0%")
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_stats = {}
        for item in all_items:
            source = item.get('source', 'Unknown')
            if source not in source_stats:
                source_stats[source] = {'total': 0, 'with_links': 0}
            source_stats[source]['total'] += 1
            if item.get('url'):
                source_stats[source]['with_links'] += 1
        
        print(f"\nğŸ“Š æŒ‰æ¥æºç»Ÿè®¡:")
        for source, stats in source_stats.items():
            coverage = stats['with_links']/stats['total']*100 if stats['total'] > 0 else 0
            print(f"   - {source}: {stats['with_links']}/{stats['total']} ({coverage:.1f}%)")
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
    except json.JSONDecodeError:
        print(f"âŒ JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {json_file}")
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")


def sample_link_test():
    """å¿«é€Ÿæµ‹è¯•å‡ ä¸ªç¤ºä¾‹é“¾æ¥"""
    print("ğŸ”¬ å¿«é€Ÿé“¾æ¥æµ‹è¯•...")
    
    test_links = [
        ("arXiv", "http://arxiv.org/abs/2511.23478v1"),
        ("GitHub", "https://github.com/openai/openai-python"),
        ("ç™¾åº¦å®˜æ–¹", "https://baijiahao.baidu.com/s?id=1783456789"),
        ("é˜¿é‡Œäº‘", "https://www.alibabacloud.com/zh/product/dashscope")
    ]
    
    for name, url in test_links:
        status = "âœ… å¯è®¿é—®" if validate_link_access(url) else "âŒ æ— æ³•è®¿é—®"
        print(f"   {name}: {status}")
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«


if __name__ == "__main__":
    print("ğŸ”— AI World Tracker - é“¾æ¥éªŒè¯å·¥å…·")
    print("=" * 60 + "\n")
    
    # å¿«é€Ÿæµ‹è¯•
    sample_link_test()
    
    print("\n" + "=" * 60 + "\n")
    
    # æŸ¥æ‰¾æœ€æ–°çš„JSONæ–‡ä»¶
    import glob
    json_files = glob.glob("ai_tracker_data_*.json")
    
    if json_files:
        latest_file = sorted(json_files)[-1]
        print(f"ğŸ“ å‘ç°æ•°æ®æ–‡ä»¶: {latest_file}")
        
        choice = input("æ˜¯å¦éªŒè¯æ‰€æœ‰é“¾æ¥ï¼Ÿè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ (Y/N): ").strip().lower()
        if choice in ['y', 'yes', 'æ˜¯']:
            test_data_links(latest_file)
        else:
            print("â­ï¸ è·³è¿‡å®Œæ•´éªŒè¯")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é‡‡é›†")
    
    print("\nâœ¨ éªŒè¯å®Œæˆï¼")
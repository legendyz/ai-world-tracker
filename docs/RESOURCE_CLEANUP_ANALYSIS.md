# èµ„æºé‡Šæ”¾æœºåˆ¶åˆ†ææŠ¥å‘Š (Resource Cleanup Analysis)

## ğŸ“‹ æ‰§è¡Œæ¦‚è¦ (Executive Summary)

æœ¬æŠ¥å‘Šå…¨é¢åˆ†æäº† AI World Tracker åº”ç”¨ç¨‹åºé€€å‡ºæ—¶çš„èµ„æºé‡Šæ”¾æœºåˆ¶ï¼Œè¯†åˆ«äº†ç°æœ‰çš„æ¸…ç†æµç¨‹ã€æ½œåœ¨çš„èµ„æºæ³„éœ²é£é™©ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„ä¼˜åŒ–å»ºè®®ã€‚

**æ ¸å¿ƒå‘ç°**:
- âœ… **ä¸»å…¥å£æ¸…ç†æœºåˆ¶**: å­˜åœ¨ try-finally ç»“æ„ç¡®ä¿åŸºæœ¬æ¸…ç†
- âš ï¸ **éƒ¨åˆ†æ¸…ç†ä¸å®Œæ•´**: LLM ç¼“å­˜ã€ImportanceEvaluator å­¦ä¹ æ•°æ®ã€HTTP å®¢æˆ·ç«¯ç­‰éœ€è¦æ”¹è¿›
- âŒ **ç¼ºå¤±æ¸…ç†**: matplotlib å›¾è¡¨å¥æŸ„ã€äº‹ä»¶å¾ªç¯ã€çº¿ç¨‹æ± ç­‰æ— æ˜¾å¼æ¸…ç†
- ğŸ”§ **ä¼˜åŒ–ç©ºé—´**: éœ€è¦æ·»åŠ ç»Ÿä¸€çš„èµ„æºç®¡ç†å™¨å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨

---

## ğŸ” èµ„æºç±»å‹æ¸…å• (Resource Inventory)

### 1. ç½‘ç»œèµ„æº (Network Resources)

#### 1.1 HTTP å®¢æˆ·ç«¯
**ä½ç½®**: `llm_classifier.py`

```python
# é—®é¢˜ä»£ç  - æ— èµ„æºæ¸…ç†
response = requests.post(
    'http://localhost:11434/api/generate',
    json={...},
    timeout=self.timeout
)
```

**é—®é¢˜**: 
- ä½¿ç”¨ `requests` åº“ç›´æ¥å‘é€è¯·æ±‚ï¼Œæœªä½¿ç”¨ä¼šè¯ç®¡ç†
- æ¯æ¬¡è¯·æ±‚åˆ›å»ºæ–°çš„è¿æ¥ï¼Œæ— è¿æ¥æ± å¤ç”¨
- æœªæ˜¾å¼å…³é—­è¿æ¥ï¼Œä¾èµ– Python GC

**é£é™©ç­‰çº§**: ğŸŸ¡ **ä¸­ç­‰**
- çŸ­æœŸå½±å“: TCP è¿æ¥å¯èƒ½å»¶è¿Ÿå…³é—­ï¼Œå ç”¨ç³»ç»Ÿèµ„æº
- é•¿æœŸå½±å“: åœ¨é¢‘ç¹è°ƒç”¨åœºæ™¯ä¸‹å¯èƒ½å¯¼è‡´ç«¯å£è€—å°½

#### 1.2 å¼‚æ­¥ HTTP ä¼šè¯
**ä½ç½®**: `data_collector.py` (lines 2298-2330)

```python
async def _collect_all_async(self) -> Dict[str, List[Dict]]:
    connector = aiohttp.TCPConnector(limit=100, limit_per_host=10)
    timeout = aiohttp.ClientTimeout(total=60, connect=10)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # ... å¼‚æ­¥é‡‡é›†é€»è¾‘
```

**çŠ¶æ€**: âœ… **æ­£ç¡®ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨**
- ä½¿ç”¨ `async with` ç¡®ä¿ä¼šè¯è‡ªåŠ¨å…³é—­
- è¿æ¥å™¨ä¼šåœ¨ä¼šè¯å…³é—­æ—¶æ­£ç¡®æ¸…ç†

**é£é™©ç­‰çº§**: ğŸŸ¢ **ä½**

#### 1.3 äº‹ä»¶å¾ªç¯æ¸…ç†
**ä½ç½®**: `data_collector.py` (lines 795-810)

```python
def _collect_all(self) -> Dict[str, List[Dict]]:
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self._collect_all_async())
        finally:
            loop.close()  # âœ… æ­£ç¡®æ¸…ç†
    except Exception as e:
        log.error(f"Async collection failed: {e}")
        return self._collect_all_sync(True, 6)
```

**çŠ¶æ€**: âœ… **æ­£ç¡®æ¸…ç†**
- ä½¿ç”¨ finally å—ç¡®ä¿ loop.close() è¢«è°ƒç”¨

**é£é™©ç­‰çº§**: ğŸŸ¢ **ä½**

### 2. æ–‡ä»¶èµ„æº (File Resources)

#### 2.1 ç¼“å­˜æ–‡ä»¶ - LLM åˆ†ç±»ç¼“å­˜
**ä½ç½®**: `llm_classifier.py` (lines 644-653)

```python
def _save_cache(self):
    """ä¿å­˜ç¼“å­˜"""
    if not self.enable_cache:
        return
    try:
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.error(t('llm_cache_save_failed', error=str(e)))
```

**å½“å‰è°ƒç”¨æ—¶æœº**: âŒ **ä»…åœ¨ç‰¹å®šåœºæ™¯è°ƒç”¨**
- åˆ†ç±»ä»»åŠ¡å®Œæˆåå¯èƒ½ä¿å­˜
- åº”ç”¨é€€å‡ºæ—¶ **æœªè°ƒç”¨**

**é—®é¢˜**:
- `cleanup()` æ–¹æ³•ä¸­æœªè°ƒç”¨ `_save_cache()`
- æœªä¿å­˜çš„ç¼“å­˜åœ¨å¼‚å¸¸é€€å‡ºæ—¶ä¼šä¸¢å¤±

**é£é™©ç­‰çº§**: ğŸ”´ **é«˜**

#### 2.2 å­¦ä¹ æ•°æ® - Importance Evaluator
**ä½ç½®**: `importance_evaluator.py` (lines 668-690)

```python
def _save_learning_data(self):
    """ä¿å­˜å­¦ä¹ æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        os.makedirs(os.path.dirname(LEARNING_CONFIG_FILE), exist_ok=True)
        
        data = {
            'source_performance': dict(self.source_performance),
            'last_updated': datetime.now().isoformat()
        }
        
        with open(LEARNING_CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        log.info(f"ğŸ’¾ Saved learning data: {len(self.source_performance)} sources")
    except Exception as e:
        log.warning(f"Failed to save learning data: {e}")
```

**å½“å‰è°ƒç”¨æ—¶æœº**: âš ï¸ **å®šæœŸè‡ªåŠ¨ä¿å­˜**
- æ¯ 10 æ¬¡æ›´æ–°è‡ªåŠ¨ä¿å­˜ (line 648)
- åº”ç”¨é€€å‡ºæ—¶ **æœªæ˜¾å¼è°ƒç”¨**

**é—®é¢˜**:
- å¦‚æœæ›´æ–°æ¬¡æ•°ä¸æ˜¯ 10 çš„å€æ•°ï¼Œæœ€åå‡ æ¬¡æ›´æ–°ä¼šä¸¢å¤±
- cleanup() æœªè°ƒç”¨è¯¥æ–¹æ³•ç¡®ä¿æœ€ç»ˆä¿å­˜

**é£é™©ç­‰çº§**: ğŸŸ¡ **ä¸­ç­‰**

#### 2.3 é‡‡é›†å†å²ç¼“å­˜
**ä½ç½®**: `data_collector.py` (lines 283-296)

```python
def _save_history_cache(self):
    """ä¿å­˜é‡‡é›†å†å²ç¼“å­˜"""
    try:
        cache_to_save = {
            'urls': list(self.history_cache['urls']),
            'titles': list(self.history_cache['titles']),
            'last_updated': datetime.now().isoformat()
        }
        with open(self.history_cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_to_save, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.error(t('dc_cache_save_failed', error=str(e)))
```

**å½“å‰è°ƒç”¨**: âœ… **åœ¨ cleanup() ä¸­è°ƒç”¨**
```python
# TheWorldOfAI.py, line 186-189
try:
    self.collector._save_history_cache()
except Exception:
    pass
```

**é£é™©ç­‰çº§**: ğŸŸ¢ **ä½**

#### 2.4 æ–‡ä»¶å¥æŸ„ä½¿ç”¨
**é€šç”¨æ¨¡å¼**: âœ… **æ­£ç¡®ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨**
```python
with open(file_path, 'w', encoding='utf-8') as f:
    f.write(content)
```

æ‰€æœ‰æ–‡ä»¶æ“ä½œå‡ä½¿ç”¨ `with` è¯­å¥ï¼Œç¡®ä¿è‡ªåŠ¨å…³é—­ã€‚

**é£é™©ç­‰çº§**: ğŸŸ¢ **ä½**

### 3. å›¾å½¢èµ„æº (Graphics Resources)

#### 3.1 Matplotlib å›¾è¡¨
**ä½ç½®**: `visualizer.py`

```python
def plot_tech_hotspots(self, tech_data: Dict, save: bool = True) -> str:
    # ... åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=(12, 6))
    # ... ç»˜åˆ¶é€»è¾‘
    
    if save:
        filepath = os.path.join(self.output_dir, 'tech_hotspots.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
    
    plt.close()  # âœ… æ­£ç¡®æ¸…ç†
    return filepath
```

**çŠ¶æ€**: âœ… **æ¯ä¸ªç»˜å›¾æ–¹æ³•æœ«å°¾è°ƒç”¨ `plt.close()`**
- å‘ç° 5 å¤„æ­£ç¡®ä½¿ç”¨ (lines 197, 259, 312, 368, 470)

**é£é™©ç­‰çº§**: ğŸŸ¢ **ä½**

### 4. å†…å­˜èµ„æº (Memory Resources)

#### 4.1 LLM æ¨¡å‹æ˜¾å­˜/å†…å­˜
**ä½ç½®**: `llm_classifier.py` (lines 555-575)

```python
def unload_model(self):
    """ç«‹å³å¸è½½æ¨¡å‹ï¼ˆé‡Šæ”¾æ˜¾å­˜/å†…å­˜ï¼‰"""
    if self.provider != LLMProvider.OLLAMA:
        return
    
    try:
        import requests
        
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                'model': self.model,
                'prompt': '',
                'stream': False,
                'keep_alive': '0s'  # ç«‹å³å¸è½½
            },
            timeout=10
        )
        
        if response.status_code == 200:
            self.is_warmed_up = False
```

**å½“å‰è°ƒç”¨**: âœ… **åœ¨ cleanup() ä¸­è°ƒç”¨**
```python
# TheWorldOfAI.py, lines 179-185
def cleanup(self):
    if self.llm_classifier is not None:
        try:
            self.llm_classifier.unload_model()
        except Exception as e:
            log.warning(t('cleanup_error', error=str(e)))
```

**é£é™©ç­‰çº§**: ğŸŸ¢ **ä½**

#### 4.2 ç¼“å­˜å­—å…¸
**ä½ç½®**: å¤šå¤„
- `llm_classifier.py`: `self.cache: Dict[str, Dict] = {}` (line 410)
- `data_collector.py`: `self.history_cache` (sets)
- `importance_evaluator.py`: `self.source_performance` (defaultdict)

**é—®é¢˜**:
- å†…å­˜ä¸­çš„å­—å…¸åœ¨åº”ç”¨é€€å‡ºæ—¶ä¼šè‡ªåŠ¨é‡Šæ”¾
- ä½†æœªä¿å­˜çš„æ•°æ®ä¼šä¸¢å¤±ï¼ˆè§æ–‡ä»¶èµ„æºç« èŠ‚ï¼‰

**é£é™©ç­‰çº§**: ğŸŸ¡ **ä¸­ç­‰** (æ•°æ®ä¸¢å¤±é£é™©)

### 5. çº¿ç¨‹/è¿›ç¨‹èµ„æº (Threading Resources)

#### 5.1 ThreadPoolExecutor
**ä½ç½®**: `data_collector.py` å¤šå¤„ä½¿ç”¨

```python
# ç¤ºä¾‹ä½¿ç”¨ (æœªæ‰¾åˆ°æ˜¾å¼æ¸…ç†)
with ThreadPoolExecutor(max_workers=max_workers) as executor:
    futures = {executor.submit(func, *args): name for ...}
    # ...
```

**çŠ¶æ€**: âœ… **ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨**
- `with` è¯­å¥ç¡®ä¿çº¿ç¨‹æ± åœ¨é€€å‡ºæ—¶å…³é—­

**é£é™©ç­‰çº§**: ğŸŸ¢ **ä½**

---

## ğŸ—ï¸ å½“å‰æ¸…ç†æµç¨‹ (Current Cleanup Flow)

### ä¸»å…¥å£ç‚¹ (Main Entry Point)
**æ–‡ä»¶**: `TheWorldOfAI.py` (lines 1522-1571)

```python
def main():
    tracker = None
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        auto_mode = '--auto' in sys.argv
        
        # åˆå§‹åŒ–è¿½è¸ªå™¨
        tracker = AIWorldTracker(auto_mode=auto_mode)
        
        # ... åº”ç”¨é€»è¾‘
        
    except KeyboardInterrupt:
        log.warning(t('user_interrupted'))
        
    finally:
        # æ¸…ç†èµ„æº
        if tracker is not None:
            tracker.cleanup()  # âœ… ç¡®ä¿æ¸…ç†è¢«è°ƒç”¨
        
        log.dual_section(t('exit_message'))
```

**ä¼˜ç‚¹**:
- âœ… ä½¿ç”¨ try-finally ç¡®ä¿æ¸…ç†
- âœ… å¤„ç† KeyboardInterrupt
- âœ… ç©ºæŒ‡é’ˆæ£€æŸ¥

**ç¼ºç‚¹**:
- âš ï¸ åªå¤„ç†äº† KeyboardInterruptï¼Œå…¶ä»–ä¿¡å· (SIGTERM, SIGINT) æœªå¤„ç†
- âš ï¸ cleanup() æ–¹æ³•åŠŸèƒ½ä¸å®Œæ•´

### æ¸…ç†æ–¹æ³• (Cleanup Method)
**æ–‡ä»¶**: `TheWorldOfAI.py` (lines 178-191)

```python
def cleanup(self):
    """æ¸…ç†èµ„æºï¼Œé‡Šæ”¾å†…å­˜/æ˜¾å­˜"""
    # 1. å¸è½½LLMæ¨¡å‹ï¼ˆå¦‚æœå·²åŠ è½½ï¼‰
    if self.llm_classifier is not None:
        try:
            self.llm_classifier.unload_model()
        except Exception as e:
            log.warning(t('cleanup_error', error=str(e)))
    
    # 2. ä¿å­˜é‡‡é›†å†å²ç¼“å­˜
    try:
        self.collector._save_history_cache()
    except Exception:
        pass
```

**å½“å‰è¦†ç›–**:
- âœ… LLM æ¨¡å‹å¸è½½
- âœ… é‡‡é›†å†å²ç¼“å­˜ä¿å­˜

**ç¼ºå¤±æ¸…ç†**:
- âŒ LLM åˆ†ç±»ç¼“å­˜ (`llm_classifier._save_cache()`)
- âŒ Importance Evaluator å­¦ä¹ æ•°æ® (`importance_evaluator._save_learning_data()`)
- âŒ å…¶ä»–æ¨¡å—çš„æ¸…ç†æ–¹æ³• (visualizer, web_publisher, etc.)

---

## âš ï¸ å·²è¯†åˆ«é—®é¢˜ (Identified Issues)

### ğŸ”´ é«˜ä¼˜å…ˆçº§

#### é—®é¢˜ 1: LLM åˆ†ç±»ç¼“å­˜æœªä¿å­˜
**å½±å“**: æœªä¿å­˜çš„åˆ†ç±»ç»“æœä¼šä¸¢å¤±ï¼Œä¸‹æ¬¡å¯åŠ¨éœ€è¦é‡æ–°åˆ†ç±»

**å½“å‰è¡Œä¸º**:
```python
# llm_classifier.py ä¸­å­˜åœ¨ _save_cache() æ–¹æ³•
# ä½† cleanup() æœªè°ƒç”¨
```

**å»ºè®®ä¿®å¤**:
```python
def cleanup(self):
    # ... ç°æœ‰ä»£ç 
    
    # æ–°å¢: ä¿å­˜ LLM åˆ†ç±»ç¼“å­˜
    if self.llm_classifier is not None:
        try:
            self.llm_classifier._save_cache()
            log.info("ğŸ’¾ LLM classification cache saved")
        except Exception as e:
            log.warning(f"Failed to save LLM cache: {e}")
```

#### é—®é¢˜ 2: ImportanceEvaluator å­¦ä¹ æ•°æ®å¯èƒ½ä¸¢å¤±
**å½±å“**: å¦‚æœæ›´æ–°æ¬¡æ•°ä¸æ˜¯ 10 çš„å€æ•°ï¼Œæœ€åå‡ æ¬¡æ›´æ–°ä¼šä¸¢å¤±

**å½“å‰è¡Œä¸º**:
```python
# importance_evaluator.py
# æ¯ 10 æ¬¡æ›´æ–°è‡ªåŠ¨ä¿å­˜ (line 648)
if self.user_feedback_count % 10 == 0:
    self._save_learning_data()

# cleanup() æœªè°ƒç”¨
```

**å»ºè®®ä¿®å¤**:
```python
def cleanup(self):
    # ... ç°æœ‰ä»£ç 
    
    # æ–°å¢: ä¿å­˜ ImportanceEvaluator å­¦ä¹ æ•°æ®
    if self.llm_classifier is not None and hasattr(self.llm_classifier, 'evaluator'):
        try:
            self.llm_classifier.evaluator._save_learning_data()
            log.info("ğŸ’¾ Importance learning data saved")
        except Exception as e:
            log.warning(f"Failed to save learning data: {e}")
```

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§

#### é—®é¢˜ 3: HTTP å®¢æˆ·ç«¯è¿æ¥æœªå¤ç”¨
**å½±å“**: æ€§èƒ½æŸå¤±ï¼Œé¢‘ç¹åˆ›å»º/é”€æ¯ TCP è¿æ¥

**å½“å‰è¡Œä¸º**:
```python
# llm_classifier.py - æ¯æ¬¡è¯·æ±‚åˆ›å»ºæ–°è¿æ¥
response = requests.post(url, json=data, timeout=timeout)
```

**å»ºè®®ä¿®å¤**:
```python
class LLMClassifier:
    def __init__(self, ...):
        # åˆ›å»ºä¼šè¯ä»¥å¤ç”¨è¿æ¥
        self.session = requests.Session()
        self.session.headers.update({'Content-Type': 'application/json'})
    
    def _call_ollama(self, ...):
        # ä½¿ç”¨ä¼šè¯
        response = self.session.post(url, json=data, timeout=timeout)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()
```

#### é—®é¢˜ 4: ä¿¡å·å¤„ç†ä¸å®Œæ•´
**å½±å“**: Docker å®¹å™¨ã€systemd æœåŠ¡ç­‰å¯èƒ½æ— æ³•æ­£å¸¸é€€å‡º

**å½“å‰è¡Œä¸º**: åªå¤„ç† KeyboardInterrupt (Ctrl+C)

**å»ºè®®ä¿®å¤**:
```python
import signal

def signal_handler(signum, frame):
    """å¤„ç†ç³»ç»Ÿä¿¡å·"""
    log.warning(f"Received signal {signum}, cleaning up...")
    if tracker is not None:
        tracker.cleanup()
    sys.exit(0)

def main():
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # kill
    
    # ... åº”ç”¨é€»è¾‘
```

### ğŸŸ¢ ä½ä¼˜å…ˆçº§

#### é—®é¢˜ 5: ç¼ºå°‘ç»Ÿä¸€çš„èµ„æºç®¡ç†å™¨
**å½±å“**: æ¸…ç†é€»è¾‘åˆ†æ•£ï¼Œéš¾ä»¥ç»´æŠ¤

**å»ºè®®**: å®ç°ä¸Šä¸‹æ–‡ç®¡ç†å™¨åè®®
```python
class AIWorldTracker:
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
        return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸

# ä½¿ç”¨ç¤ºä¾‹
with AIWorldTracker(auto_mode=False) as tracker:
    tracker.run()
# è‡ªåŠ¨è°ƒç”¨ cleanup()
```

---

## ğŸ¯ ä¼˜åŒ–å»ºè®® (Optimization Recommendations)

### å»ºè®® 1: å®Œå–„ cleanup() æ–¹æ³•

**ä¼˜å…ˆçº§**: ğŸ”´ **é«˜**

**å®ç°ä»£ç **:
```python
def cleanup(self):
    """æ¸…ç†èµ„æºï¼Œé‡Šæ”¾å†…å­˜/æ˜¾å­˜"""
    log.dual_info("ğŸ§¹ Starting resource cleanup...")
    
    # 1. å¸è½½ LLM æ¨¡å‹
    if self.llm_classifier is not None:
        try:
            log.dual_info("  â†³ Unloading LLM model...")
            self.llm_classifier.unload_model()
        except Exception as e:
            log.warning(f"Failed to unload model: {e}")
    
    # 2. ä¿å­˜ LLM åˆ†ç±»ç¼“å­˜ (æ–°å¢)
    if self.llm_classifier is not None:
        try:
            log.dual_info("  â†³ Saving LLM classification cache...")
            self.llm_classifier._save_cache()
        except Exception as e:
            log.warning(f"Failed to save LLM cache: {e}")
    
    # 3. ä¿å­˜ ImportanceEvaluator å­¦ä¹ æ•°æ® (æ–°å¢)
    if self.llm_classifier is not None and hasattr(self.llm_classifier, 'evaluator'):
        try:
            log.dual_info("  â†³ Saving importance learning data...")
            self.llm_classifier.evaluator._save_learning_data()
        except Exception as e:
            log.warning(f"Failed to save learning data: {e}")
    
    # 4. ä¿å­˜é‡‡é›†å†å²ç¼“å­˜
    try:
        log.dual_info("  â†³ Saving collection history cache...")
        self.collector._save_history_cache()
    except Exception as e:
        log.warning(f"Failed to save history cache: {e}")
    
    # 5. æ¸…ç† HTTP ä¼šè¯ (å¦‚æœä½¿ç”¨)
    if hasattr(self, 'llm_classifier') and self.llm_classifier is not None:
        if hasattr(self.llm_classifier, 'session'):
            try:
                log.dual_info("  â†³ Closing HTTP sessions...")
                self.llm_classifier.session.close()
            except Exception as e:
                log.warning(f"Failed to close session: {e}")
    
    log.dual_success("âœ… Resource cleanup completed")
```

### å»ºè®® 2: ä¸º LLMClassifier æ·»åŠ  cleanup() æ–¹æ³•

**ä¼˜å…ˆçº§**: ğŸŸ¡ **ä¸­**

**å®ç°ä»£ç **:
```python
# llm_classifier.py
class LLMClassifier:
    def cleanup(self):
        """æ¸…ç† LLM åˆ†ç±»å™¨èµ„æº"""
        # 1. ä¿å­˜ç¼“å­˜
        try:
            self._save_cache()
            log.info("ğŸ’¾ LLM cache saved")
        except Exception as e:
            log.warning(f"Failed to save cache: {e}")
        
        # 2. ä¿å­˜å­¦ä¹ æ•°æ®
        try:
            self.evaluator._save_learning_data()
            log.info("ğŸ’¾ Learning data saved")
        except Exception as e:
            log.warning(f"Failed to save learning data: {e}")
        
        # 3. å…³é—­ HTTP ä¼šè¯
        if hasattr(self, 'session'):
            try:
                self.session.close()
                log.info("ğŸ”Œ HTTP session closed")
            except Exception as e:
                log.warning(f"Failed to close session: {e}")
```

### å»ºè®® 3: å®ç° HTTP ä¼šè¯å¤ç”¨

**ä¼˜å…ˆçº§**: ğŸŸ¡ **ä¸­**

**å®ç°ä»£ç **:
```python
# llm_classifier.py
class LLMClassifier:
    def __init__(self, ...):
        # ... ç°æœ‰åˆå§‹åŒ–ä»£ç 
        
        # åˆ›å»ºå¤ç”¨ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({'Content-Type': 'application/json'})
        
        # é…ç½®è¿æ¥æ± 
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
    
    def _call_ollama(self, messages: List[Dict], stream: bool = False) -> Tuple[Optional[Dict], Optional[FallbackReason]]:
        # ä½¿ç”¨ä¼šè¯è€Œéç›´æ¥ requests.post
        try:
            response = self.session.post(
                'http://localhost:11434/api/chat',
                json={'model': self.model, 'messages': messages, 'stream': stream},
                timeout=self.timeout
            )
            # ... å¤„ç†å“åº”
        except requests.exceptions.RequestException as e:
            # ... é”™è¯¯å¤„ç†
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()
```

### å»ºè®® 4: æ·»åŠ ä¿¡å·å¤„ç†

**ä¼˜å…ˆçº§**: ğŸŸ¡ **ä¸­**

**å®ç°ä»£ç **:
```python
# TheWorldOfAI.py
import signal
import sys

# å…¨å±€è¿½è¸ªå™¨å¼•ç”¨
_tracker_instance = None

def signal_handler(signum, frame):
    """ä¼˜é›…å¤„ç†ç³»ç»Ÿä¿¡å·"""
    signal_names = {
        signal.SIGINT: 'SIGINT (Ctrl+C)',
        signal.SIGTERM: 'SIGTERM (kill)'
    }
    signal_name = signal_names.get(signum, f'Signal {signum}')
    
    log.warning(f"âš ï¸ Received {signal_name}, initiating graceful shutdown...")
    
    if _tracker_instance is not None:
        _tracker_instance.cleanup()
    
    log.dual_section("ğŸ‘‹ Goodbye!")
    sys.exit(0)

def main():
    global _tracker_instance
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨ (Windows åªæ”¯æŒ SIGINT)
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGTERM'):  # Unix/Linux
        signal.signal(signal.SIGTERM, signal_handler)
    
    tracker = None
    try:
        # ... ç°æœ‰ä»£ç 
        tracker = AIWorldTracker(auto_mode=auto_mode)
        _tracker_instance = tracker
        
        # ... åº”ç”¨é€»è¾‘
        
    except KeyboardInterrupt:
        log.warning(t('user_interrupted'))
        
    finally:
        if tracker is not None:
            tracker.cleanup()
        
        _tracker_instance = None
        log.dual_section(t('exit_message'))
```

### å»ºè®® 5: å®ç°ä¸Šä¸‹æ–‡ç®¡ç†å™¨

**ä¼˜å…ˆçº§**: ğŸŸ¢ **ä½** (ä»£ç ä¼˜é›…æ€§æå‡)

**å®ç°ä»£ç **:
```python
# TheWorldOfAI.py
class AIWorldTracker:
    def __enter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¡®ä¿æ¸…ç†"""
        self.cleanup()
        
        # ä¸æŠ‘åˆ¶å¼‚å¸¸
        return False

# ä½¿ç”¨ç¤ºä¾‹ 1: è‡ªåŠ¨æ¨¡å¼
def main_with_context():
    with AIWorldTracker(auto_mode=False) as tracker:
        tracker.run()
    # è‡ªåŠ¨è°ƒç”¨ cleanup()

# ä½¿ç”¨ç¤ºä¾‹ 2: æ‰‹åŠ¨æ¨¡å¼
def main():
    try:
        with AIWorldTracker(auto_mode=False) as tracker:
            while True:
                # ... äº¤äº’é€»è¾‘
                pass
    except KeyboardInterrupt:
        log.warning("User interrupted")
    # cleanup() å·²åœ¨ __exit__ ä¸­è°ƒç”¨
```

---

## ğŸ“Š é£é™©è¯„ä¼°çŸ©é˜µ (Risk Assessment Matrix)

| èµ„æºç±»å‹ | å½“å‰çŠ¶æ€ | æ³„éœ²é£é™© | æ•°æ®ä¸¢å¤±é£é™© | ä¼˜å…ˆçº§ |
|---------|---------|---------|------------|--------|
| LLM åˆ†ç±»ç¼“å­˜ | âŒ æœªä¿å­˜ | ğŸŸ¢ ä½ | ğŸ”´ é«˜ | ğŸ”´ é«˜ |
| ImportanceEvaluator å­¦ä¹ æ•°æ® | âš ï¸ å®šæœŸä¿å­˜ | ğŸŸ¢ ä½ | ğŸŸ¡ ä¸­ | ğŸŸ¡ ä¸­ |
| é‡‡é›†å†å²ç¼“å­˜ | âœ… å·²ä¿å­˜ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ |
| HTTP è¿æ¥ | âš ï¸ æœªå¤ç”¨ | ğŸŸ¡ ä¸­ | ğŸŸ¢ ä½ | ğŸŸ¡ ä¸­ |
| LLM æ¨¡å‹å†…å­˜ | âœ… å·²å¸è½½ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ |
| å¼‚æ­¥äº‹ä»¶å¾ªç¯ | âœ… å·²å…³é—­ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ |
| æ–‡ä»¶å¥æŸ„ | âœ… with è¯­å¥ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ |
| Matplotlib å›¾è¡¨ | âœ… plt.close() | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ |
| çº¿ç¨‹æ±  | âœ… with è¯­å¥ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ | ğŸŸ¢ ä½ |

---

## ğŸš€ å®æ–½è·¯çº¿å›¾ (Implementation Roadmap)

### é˜¶æ®µ 1: ç´§æ€¥ä¿®å¤ (1-2 å°æ—¶)
1. âœ… **å®Œå–„ cleanup() æ–¹æ³•**
   - æ·»åŠ  LLM ç¼“å­˜ä¿å­˜
   - æ·»åŠ å­¦ä¹ æ•°æ®ä¿å­˜
   - æ·»åŠ è¯¦ç»†æ—¥å¿—

2. âœ… **ä¸º LLMClassifier æ·»åŠ  cleanup()**
   - é›†ä¸­æ¸…ç†é€»è¾‘
   - ä¾¿äºç»´æŠ¤

### é˜¶æ®µ 2: æ€§èƒ½ä¼˜åŒ– (2-3 å°æ—¶)
3. âœ… **å®ç° HTTP ä¼šè¯å¤ç”¨**
   - åˆ›å»º requests.Session
   - é…ç½®è¿æ¥æ± 
   - åœ¨ cleanup() ä¸­å…³é—­

4. âœ… **æ·»åŠ ä¿¡å·å¤„ç†**
   - SIGINT (Ctrl+C)
   - SIGTERM (kill)

### é˜¶æ®µ 3: ä»£ç ä¼˜é›…æ€§ (1-2 å°æ—¶)
5. âœ… **å®ç°ä¸Šä¸‹æ–‡ç®¡ç†å™¨**
   - `__enter__` å’Œ `__exit__`
   - ç®€åŒ–ä¸»å‡½æ•°é€»è¾‘

6. âœ… **æ·»åŠ èµ„æºæ¸…ç†æµ‹è¯•**
   - å•å…ƒæµ‹è¯•éªŒè¯æ¸…ç†é€»è¾‘
   - é›†æˆæµ‹è¯•æ¨¡æ‹Ÿå¼‚å¸¸é€€å‡º

---

## ğŸ“ æœ€ä½³å®è·µå»ºè®® (Best Practices)

### 1. ç»Ÿä¸€æ¸…ç†æ¨¡å¼
**åŸåˆ™**: æ‰€æœ‰æ¨¡å—åº”å®ç° `cleanup()` æ–¹æ³•

```python
class BaseModule:
    def cleanup(self):
        """æ¸…ç†èµ„æº - å­ç±»åº”é‡å†™æ­¤æ–¹æ³•"""
        raise NotImplementedError

class DataCollector(BaseModule):
    def cleanup(self):
        self._save_history_cache()

class LLMClassifier(BaseModule):
    def cleanup(self):
        self._save_cache()
        self.evaluator._save_learning_data()
        if hasattr(self, 'session'):
            self.session.close()
```

### 2. èµ„æºè·å–å³åˆå§‹åŒ– (RAII)
**åŸåˆ™**: ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨æ¸…ç†

```python
# âœ… æ¨è
with requests.Session() as session:
    response = session.get(url)

# âŒ ä¸æ¨è
session = requests.Session()
response = session.get(url)
session.close()  # å¯èƒ½è¢«é—å¿˜æˆ–å¼‚å¸¸è·³è¿‡
```

### 3. é˜²å¾¡æ€§ç¼–ç¨‹
**åŸåˆ™**: æ¸…ç†é€»è¾‘åº”å®¹é”™

```python
def cleanup(self):
    # âœ… æ¯ä¸ªæ¸…ç†æ­¥éª¤éƒ½æœ‰ç‹¬ç«‹çš„ try-except
    try:
        self._save_cache()
    except Exception as e:
        log.warning(f"Cache save failed: {e}")
    
    try:
        self._close_connections()
    except Exception as e:
        log.warning(f"Connection close failed: {e}")
```

### 4. æ¸…ç†æ—¥å¿—
**åŸåˆ™**: è®°å½•æ¸…ç†è¿‡ç¨‹ä»¥ä¾¿è°ƒè¯•

```python
def cleanup(self):
    log.dual_info("ğŸ§¹ Starting cleanup...")
    
    log.dual_info("  â†³ Saving cache...")
    self._save_cache()
    
    log.dual_info("  â†³ Closing connections...")
    self._close_connections()
    
    log.dual_success("âœ… Cleanup completed")
```

---

## ğŸ§ª æµ‹è¯•å»ºè®® (Testing Recommendations)

### å•å…ƒæµ‹è¯•: æ¸…ç†é€»è¾‘éªŒè¯
```python
# tests/test_cleanup.py
import unittest
from unittest.mock import Mock, patch
from TheWorldOfAI import AIWorldTracker

class TestCleanup(unittest.TestCase):
    def test_cleanup_saves_llm_cache(self):
        """æµ‹è¯•æ¸…ç†æ—¶ä¿å­˜ LLM ç¼“å­˜"""
        tracker = AIWorldTracker(auto_mode=True)
        tracker.llm_classifier = Mock()
        
        tracker.cleanup()
        
        tracker.llm_classifier._save_cache.assert_called_once()
    
    def test_cleanup_handles_exceptions(self):
        """æµ‹è¯•æ¸…ç†æ—¶çš„å¼‚å¸¸å¤„ç†"""
        tracker = AIWorldTracker(auto_mode=True)
        tracker.llm_classifier = Mock()
        tracker.llm_classifier._save_cache.side_effect = Exception("Test error")
        
        # ä¸åº”æŠ›å‡ºå¼‚å¸¸
        tracker.cleanup()
```

### é›†æˆæµ‹è¯•: æ¨¡æ‹Ÿå¼‚å¸¸é€€å‡º
```python
# tests/test_graceful_shutdown.py
import signal
import subprocess
import time

def test_sigterm_handling():
    """æµ‹è¯• SIGTERM ä¿¡å·å¤„ç†"""
    # å¯åŠ¨åº”ç”¨
    proc = subprocess.Popen(['python', 'TheWorldOfAI.py', '--auto'])
    time.sleep(2)
    
    # å‘é€ SIGTERM
    proc.send_signal(signal.SIGTERM)
    proc.wait(timeout=10)
    
    # éªŒè¯ç¼“å­˜æ–‡ä»¶å·²ä¿å­˜
    assert os.path.exists('data/cache/llm_classification_cache.json')
    assert os.path.exists('data/cache/importance_learning.json')
```

---

## ğŸ“š å‚è€ƒèµ„æº (References)

### Python èµ„æºç®¡ç†
- [Context Managers (PEP 343)](https://peps.python.org/pep-0343/)
- [requests.Session æ–‡æ¡£](https://requests.readthedocs.io/en/latest/user/advanced/#session-objects)
- [signal æ¨¡å—æ–‡æ¡£](https://docs.python.org/3/library/signal.html)

### æœ€ä½³å®è·µ
- [Python Best Practices: Resource Management](https://realpython.com/python-with-statement/)
- [Graceful Shutdown in Python](https://medium.com/@ageitgey/graceful-shutdown-in-python-60e4a9b3c4e3)

---

## ğŸ“… æ–‡æ¡£ç‰ˆæœ¬ (Document Version)

- **ç‰ˆæœ¬**: 1.0
- **åˆ›å»ºæ—¥æœŸ**: 2024-12-10
- **æœ€åæ›´æ–°**: 2024-12-10
- **ä½œè€…**: GitHub Copilot
- **å®¡æ ¸çŠ¶æ€**: å¾…å®¡æ ¸

---

## ğŸ“§ åé¦ˆä¸é—®é¢˜ (Feedback)

å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼åé¦ˆï¼š
- ä»£ç å®¡æŸ¥: åˆ›å»º Pull Request
- é—®é¢˜æŠ¥å‘Š: åˆ›å»º GitHub Issue
- ä¼˜åŒ–å»ºè®®: è”ç³»å¼€å‘å›¢é˜Ÿ

"""
åˆå¹¶ data_collector.py å’Œ async_data_collector.py
å°†å¼‚æ­¥åŠŸèƒ½æ•´åˆåˆ°ä¸»æ¨¡å—ä¸­
"""

# è¯»å–async_data_collector.pyä¸­çš„å…³é”®ä»£ç æ®µ
print("æ­£åœ¨åˆå¹¶æ•°æ®é‡‡é›†æ¨¡å—...")

# ç”Ÿæˆæ–°çš„data_collector.pyå†…å®¹
new_content = '''"""
AIä¸–ç•Œè¿½è¸ªå™¨ - æ•°æ®é‡‡é›†æ¨¡å—ï¼ˆç»Ÿä¸€ç‰ˆæœ¬ï¼‰
ä¸“æ³¨äºæ”¶é›†æœ€æ–°AIç ”ç©¶ã€äº§å“ã€å¼€å‘è€…ç¤¾åŒºå’Œè¡Œä¸šä¿¡æ¯

æ”¯æŒä¸¤ç§æ¨¡å¼:
- åŒæ­¥æ¨¡å¼ (ThreadPoolExecutor): å…¼å®¹æ—§ä»£ç 
- å¼‚æ­¥æ¨¡å¼ (asyncio + aiohttp): é«˜æ€§èƒ½é‡‡é›†ï¼ˆæ¨èï¼Œé»˜è®¤ï¼‰

ä½¿ç”¨æ–¹å¼:
    # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å¼ï¼ˆé»˜è®¤å¼‚æ­¥ï¼‰
    collector = DataCollector()
    data = collector.collect_all()
    
    # å¼ºåˆ¶ä½¿ç”¨åŒæ­¥æ¨¡å¼
    collector = DataCollector(async_mode=False)
"""

import requests
import feedparser
import arxiv
import json
import os
import yaml
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from typing import List, Dict, Optional, Callable, Tuple, Any
from dataclasses import dataclass
import time
import random
import difflib
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from config import config
from logger import get_log_helper

# å¯¼å…¥å›½é™…åŒ–æ¨¡å—
try:
    from i18n import t, get_language
except ImportError:
    def t(key, **kwargs): return key
    def get_language(): return 'zh'

# å°è¯•å¯¼å…¥å¼‚æ­¥åº“
try:
    import asyncio
    import aiohttp
    ASYNC_AVAILABLE = True
except ImportError:
    ASYNC_AVAILABLE = False
    print("âš ï¸  Warning: aiohttp not available, async mode disabled")

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('data_collector')

# åŠ è½½ç¼“å­˜ç›®å½•é…ç½®
def _get_cache_dir():
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    cache_dir = 'data/cache'
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                cache_dir = cfg.get('data', {}).get('cache_dir', cache_dir)
    except Exception:
        pass
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir

DATA_CACHE_DIR = _get_cache_dir()


# ============== å¼‚æ­¥é‡‡é›†å™¨é…ç½® ==============

@dataclass
class AsyncCollectorConfig:
    """å¼‚æ­¥é‡‡é›†å™¨é…ç½®"""
    # å¹¶å‘æ§åˆ¶
    max_concurrent_requests: int = 20      # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    max_concurrent_per_host: int = 3       # æ¯ä¸ªä¸»æœºæœ€å¤§å¹¶å‘æ•°
    
    # è¶…æ—¶è®¾ç½®ï¼ˆç§’ï¼‰
    request_timeout: int = 15              # å•ä¸ªè¯·æ±‚è¶…æ—¶
    total_timeout: int = 120               # æ€»é‡‡é›†è¶…æ—¶
    
    # é‡è¯•è®¾ç½®
    max_retries: int = 2                   # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0               # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    # é€Ÿç‡é™åˆ¶
    rate_limit_delay: float = 0.2          # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    
    # æ•°æ®ç›®å½•
    cache_dir: str = 'data/cache'


def _load_async_config() -> AsyncCollectorConfig:
    """ä» config.yaml åŠ è½½å¼‚æ­¥é‡‡é›†é…ç½®"""
    cfg = AsyncCollectorConfig()
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                yaml_cfg = yaml.safe_load(f)
                async_cfg = yaml_cfg.get('async_collector', {})
                
                cfg.max_concurrent_requests = async_cfg.get('max_concurrent_requests', cfg.max_concurrent_requests)
                cfg.max_concurrent_per_host = async_cfg.get('max_concurrent_per_host', cfg.max_concurrent_per_host)
                cfg.request_timeout = async_cfg.get('request_timeout', cfg.request_timeout)
                cfg.total_timeout = async_cfg.get('total_timeout', cfg.total_timeout)
                cfg.max_retries = async_cfg.get('max_retries', cfg.max_retries)
                cfg.cache_dir = yaml_cfg.get('data', {}).get('cache_dir', cfg.cache_dir)
    except Exception:
        pass
    
    os.makedirs(cfg.cache_dir, exist_ok=True)
    return cfg


def _check_async_mode() -> bool:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å¼‚æ­¥æ¨¡å¼"""
    if not ASYNC_AVAILABLE:
        return False
    
    # ä»é…ç½®è¯»å–
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                return cfg.get('collector', {}).get('async_mode', True)
    except Exception:
        pass
    
    return True  # é»˜è®¤ä½¿ç”¨å¼‚æ­¥æ¨¡å¼


# ============== AIç›¸å…³å¸¸é‡å®šä¹‰ ==============

# AIé¢†è¢–åˆ—è¡¨
AI_LEADERS = {
    "Sam Altman": "OpenAI CEO",
    "Elon Musk": "xAI Founder",
    "Jensen Huang": "NVIDIA CEO",
    "Demis Hassabis": "Google DeepMind CEO",
    "Yann LeCun": "Meta Chief AI Scientist",
    "Geoffrey Hinton": "AI Pioneer",
    "Andrew Ng": "AI Fund Managing General Partner",
    "Kai-Fu Lee": "01.AI CEO",
    "Robin Li": "Baidu CEO"
}

# AIç›¸å…³å…³é”®è¯
AI_KEYWORDS = [
    'ai', 'artificial intelligence', 'machine learning', 'deep learning',
    'neural network', 'llm', 'gpt', 'transformer', 'chatgpt', 'claude',
    'gemini', 'llama', 'anthropic', 'openai',
    'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'å¤§æ¨¡å‹'
]

# HNæœç´¢å…³é”®è¯
HN_SEARCH_TERMS = [
    'ai', 'llm', 'gpt', 'chatgpt', 'openai', 'anthropic', 'claude',
    'gemini', 'llama', 'transformer', 'machine learning', 'deep learning',
    'neural', 'diffusion', 'stable diffusion', 'midjourney', 'copilot',
    'langchain', 'rag', 'vector', 'embedding', 'fine-tune', 'rlhf'
]

# RSSæºé…ç½® - ç»Ÿä¸€é…ç½®
RSS_FEEDS = {
    'research': [
        'http://export.arxiv.org/rss/cs.AI',
        'http://export.arxiv.org/rss/cs.CL',
        'http://export.arxiv.org/rss/cs.CV',
        'http://export.arxiv.org/rss/cs.LG',
    ],
    'news': [
        'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml',
        'https://techcrunch.com/category/artificial-intelligence/feed/',
        'https://www.wired.com/feed/tag/ai/latest/rss',
        'https://spectrum.ieee.org/rss/topic/artificial-intelligence',
        'https://www.technologyreview.com/feed/',
        'https://artificialintelligence-news.com/feed/',
        'https://syncedreview.com/feed/',
        'https://www.36kr.com/feed',
        'https://www.ithome.com/rss/',
        'https://www.jiqizhixin.com/rss',
        'https://www.qbitai.com/feed',
        'https://www.infoq.cn/feed/topic/18',
    ],
    'developer': [
        'https://github.blog/feed/',
        'https://huggingface.co/blog/feed.xml',
        'https://openai.com/blog/rss.xml',
        'https://blog.google/technology/ai/rss/',
    ],
    'product_news': [
        'https://openai.com/blog/rss.xml',
        'https://blog.google/technology/ai/rss/',
        'https://blogs.microsoft.com/ai/feed/',
    ],
    'community': [
        'https://www.producthunt.com/feed?category=artificial-intelligence',
    ],
    'leader_blogs': [
        {'url': 'http://blog.samaltman.com/posts.atom', 'author': 'Sam Altman', 'title': 'OpenAI CEO'},
        {'url': 'https://karpathy.github.io/feed.xml', 'author': 'Andrej Karpathy', 'title': 'AI Researcher'},
        {'url': 'https://lexfridman.com/feed/podcast/', 'author': 'Lex Fridman', 'title': 'Podcast Host', 'type': 'podcast'},
    ]
}
'''

# å†™å…¥æ–‡ä»¶å¤´éƒ¨
with open('data_collector_new.py', 'w', encoding='utf-8') as f:
    f.write(new_content)

print("âœ… å·²ç”Ÿæˆæ–°æ–‡ä»¶å¤´éƒ¨")
print("ğŸ“ è¯·æ‰‹åŠ¨å®Œæˆå‰©ä½™éƒ¨åˆ†çš„åˆå¹¶ï¼Œå› ä¸ºæ–‡ä»¶å¤ªå¤§")
print("ğŸ’¡ å»ºè®®ï¼š")
print("   1. ä¿ç•™ data_collector.py ä¸­çš„åŒæ­¥æ–¹æ³•")
print("   2. ä» async_data_collector.py å¤åˆ¶å¼‚æ­¥æ–¹æ³•")
print("   3. ç»Ÿä¸€ä½¿ç”¨ RSS_FEEDS é…ç½®")

"""
AI World Tracker - MVPç‰ˆæœ¬
å…¨çƒAIç ”ç©¶ã€äº§å“ã€å¸‚åœºåŠ¨æ€è¿½è¸ªåº”ç”¨

ä¸»è¦åŠŸèƒ½:
1. æ•°æ®é‡‡é›†æ¨¡å— - ä»arXivã€GitHubã€RSSç­‰æºé‡‡é›†AIèµ„è®¯
2. å†…å®¹åˆ†ç±»ç³»ç»Ÿ - è‡ªåŠ¨åˆ†ç±»ä¸ºç ”ç©¶/äº§å“/å¸‚åœºç»´åº¦ï¼ˆæ”¯æŒLLMå’Œè§„åˆ™ä¸¤ç§æ¨¡å¼ï¼‰
3. æ™ºèƒ½åˆ†æåŠŸèƒ½ - ç”Ÿæˆè¶‹åŠ¿åˆ†æå’Œæ´å¯ŸæŠ¥å‘Š
4. æ•°æ®å¯è§†åŒ– - ç”Ÿæˆå„ç±»å›¾è¡¨å±•ç¤ºæ•°æ®

ä½œè€…: AI World Tracker Team
æ—¥æœŸ: 2025-12-01
æ›´æ–°: 2025-12-06 - æ·»åŠ LLMåˆ†ç±»æ”¯æŒ
"""

import sys
import json
import os
import glob
import yaml
from datetime import datetime
from typing import Optional, Dict

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_collector import DataCollector
from content_classifier import ContentClassifier
from ai_analyzer import AIAnalyzer
from visualizer import DataVisualizer
from web_publisher import WebPublisher
from manual_reviewer import ManualReviewer
from learning_feedback import LearningFeedback, create_feedback_loop
from i18n import set_language, get_language, t, select_language_interactive
from logger import get_log_helper, configure_logging

# é…ç½®æ—¥å¿—
configure_logging(log_level='INFO')

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('main')

# ç”¨æˆ·é…ç½®æ–‡ä»¶
CONFIG_FILE = 'ai_tracker_config.json'

# Ollama å¯åŠ¨é…ç½®
OLLAMA_STARTUP_TIMEOUT = 10  # å¯åŠ¨ç­‰å¾…è¶…æ—¶ï¼ˆç§’ï¼‰

# æ•°æ®ç›®å½•é…ç½®ï¼ˆä»config.yamlåŠ è½½ï¼‰
def _load_data_paths():
    """åŠ è½½æ•°æ®ç›®å½•é…ç½®"""
    exports_dir = 'data/exports'
    cache_dir = 'data/cache'
    
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                data_config = config.get('data', {})
                exports_dir = data_config.get('exports_dir', exports_dir)
                cache_dir = data_config.get('cache_dir', cache_dir)
    except Exception:
        pass
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(exports_dir, exist_ok=True)
    os.makedirs(cache_dir, exist_ok=True)
    
    return exports_dir, cache_dir

DATA_EXPORTS_DIR, DATA_CACHE_DIR = _load_data_paths()

# LLMåˆ†ç±»å™¨ï¼ˆå¯é€‰å¯¼å…¥ï¼‰
try:
    from llm_classifier import LLMClassifier, check_ollama_status, AVAILABLE_MODELS, LLMProvider
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    log.warning(t('llm_not_installed'))


class AIWorldTracker:
    """AIä¸–ç•Œè¿½è¸ªå™¨ä¸»åº”ç”¨"""
    
    def __init__(self, auto_mode: bool = False):
        """
        åˆå§‹åŒ–AIä¸–ç•Œè¿½è¸ªå™¨
        
        Args:
            auto_mode: æ˜¯å¦ä¸ºè‡ªåŠ¨æ¨¡å¼ï¼Œè‡ªåŠ¨æ¨¡å¼ä¸‹è·³è¿‡äº¤äº’å¼æç¤º
        """
        self.auto_mode = auto_mode
        
        log.dual_section(f"     {t('app_title')}\n     {t('app_subtitle')}")
        
        self.collector = DataCollector()
        self.classifier = ContentClassifier()  # è§„åˆ™åˆ†ç±»å™¨
        self.llm_classifier = None  # LLMåˆ†ç±»å™¨ï¼ˆæŒ‰éœ€åˆå§‹åŒ–ï¼‰
        self.analyzer = AIAnalyzer()
        self.visualizer = DataVisualizer()
        self.web_publisher = WebPublisher()
        self.reviewer = ManualReviewer()
        self.learner = LearningFeedback()
        
        self.data = []
        self.trends = {}
        self.chart_files = {}
        
        # åˆ†ç±»æ¨¡å¼: 'rule' æˆ– 'llm'
        self.classification_mode = 'rule'
        self.llm_provider = 'ollama'
        self.llm_model = 'qwen3:8b'
        
        # è‡ªåŠ¨æ¨¡å¼ä¸‹å¼ºåˆ¶ä½¿ç”¨è§„åˆ™åˆ†ç±»ï¼Œè·³è¿‡LLMç›¸å…³åˆå§‹åŒ–
        if self.auto_mode:
            log.dual_config(t('auto_mode'))
            self._load_latest_data()
            return
        
        # åŠ è½½ç”¨æˆ·é…ç½®ï¼ˆåŒ…æ‹¬ä¸Šæ¬¡çš„åˆ†ç±»æ¨¡å¼ï¼‰
        self._load_user_config()
        
        # å°è¯•åŠ è½½æœ€æ–°æ•°æ®
        self._load_latest_data()
        
        # æ£€æŸ¥LLMå¯ç”¨æ€§ï¼ˆè‡ªåŠ¨æ¨¡å¼ä¸‹è·³è¿‡äº¤äº’å¼æç¤ºï¼‰
        if LLM_AVAILABLE:
            self._check_llm_availability()
        
        # å°è¯•æ¢å¤ä¸Šæ¬¡çš„LLMåˆ†ç±»å™¨
        self._try_restore_llm_classifier()
    
    def _load_user_config(self):
        """åŠ è½½ç”¨æˆ·é…ç½®ï¼ˆåŒ…æ‹¬ä¸Šæ¬¡çš„åˆ†ç±»æ¨¡å¼é€‰æ‹©ï¼‰"""
        try:
            if os.path.exists(CONFIG_FILE):
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    
                # æ¢å¤åˆ†ç±»æ¨¡å¼è®¾ç½®
                saved_mode = config.get('classification_mode', 'rule')
                saved_provider = config.get('llm_provider', 'ollama')
                saved_model = config.get('llm_model', 'qwen3:8b')
                
                # éªŒè¯æ¨¡å¼æœ‰æ•ˆæ€§
                if saved_mode in ['rule', 'llm']:
                    self.classification_mode = saved_mode
                    self.llm_provider = saved_provider
                    self.llm_model = saved_model
                    
                    if saved_mode == 'llm':
                        log.config(t('config_loaded_llm', provider=saved_provider, model=saved_model))
                    else:
                        log.config(t('config_loaded_rule'))
        except (FileNotFoundError, json.JSONDecodeError, KeyError):
            # é…ç½®æ–‡ä»¶æŸåæˆ–ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
            pass
    
    def _save_user_config(self):
        """ä¿å­˜ç”¨æˆ·é…ç½®"""
        try:
            config = {
                'classification_mode': self.classification_mode,
                'llm_provider': self.llm_provider,
                'llm_model': self.llm_model,
                'last_updated': datetime.now().isoformat()
            }
            with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(t('config_save_failed', error=str(e)))
    
    def cleanup(self):
        """æ¸…ç†èµ„æºï¼Œé‡Šæ”¾å†…å­˜/æ˜¾å­˜"""
        # å¸è½½LLMæ¨¡å‹ï¼ˆå¦‚æœå·²åŠ è½½ï¼‰
        if self.llm_classifier is not None:
            try:
                self.llm_classifier.unload_model()
            except Exception as e:
                log.warning(t('cleanup_error', error=str(e)))
        
        # ä¿å­˜é‡‡é›†å†å²ç¼“å­˜
        try:
            self.collector._save_history_cache()
        except Exception:
            pass
    
    def _try_restore_llm_classifier(self, clear_cache: bool = False):
        """å°è¯•æ¢å¤ä¸Šæ¬¡çš„LLMåˆ†ç±»å™¨
        
        Args:
            clear_cache: æ˜¯å¦åœ¨åˆå§‹åŒ–å‰å¼ºåˆ¶æ¸…é™¤ç¼“å­˜æ–‡ä»¶
        """
        if self.classification_mode == 'llm' and LLM_AVAILABLE:
            try:
                # å¼ºåˆ¶æ¸…é™¤ç¼“å­˜æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if clear_cache:
                    self._force_clear_llm_cache()
                    
                # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
                if self.llm_provider == 'ollama':
                    status = check_ollama_status()
                    if status['running'] and self.llm_model in status.get('models', []):
                        self.llm_classifier = LLMClassifier(
                            provider='ollama',
                            model=self.llm_model
                        )
                        log.dual_success(t('llm_restored', model=self.llm_model))
                    else:
                        log.warning(t('llm_restore_failed'))
                        self.classification_mode = 'rule'
                        self._save_user_config()
                else:
                    # OpenAI/Anthropic ç­‰äº‘æœåŠ¡ï¼Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨é…ç½®
                    log.warning(t('llm_cloud_reconfig', provider=self.llm_provider))
                    self.classification_mode = 'rule'
            except Exception as e:
                log.error(t('llm_restore_error', error=str(e)))
                self.classification_mode = 'rule'
                self._save_user_config()
    
    def _force_clear_llm_cache(self):
        """å¼ºåˆ¶æ¸…é™¤LLMåˆ†ç±»ç¼“å­˜æ–‡ä»¶"""
        cache_file = os.path.join(DATA_CACHE_DIR, 'llm_classification_cache.json')
        try:
            if os.path.exists(cache_file):
                os.remove(cache_file)
                log.success(t('llm_cache_force_cleared'))
            else:
                log.info(t('llm_cache_not_found'), emoji="â„¹ï¸")
        except Exception as e:
            log.error(t('llm_cache_clear_error', error=str(e)))
    
    def _clear_export_history(self):
        """æ¸…é™¤é‡‡é›†ç»“æœå†å²ï¼ˆéœ€è¦ç”¨æˆ·ç¡®è®¤ï¼‰"""
        import glob
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯¼å‡ºæ–‡ä»¶
        json_pattern = os.path.join(DATA_EXPORTS_DIR, 'ai_tracker_data_*.json')
        txt_pattern = os.path.join(DATA_EXPORTS_DIR, 'ai_tracker_report_*.txt')
        
        json_files = glob.glob(json_pattern)
        txt_files = glob.glob(txt_pattern)
        all_files = json_files + txt_files
        
        if not all_files:
            log.info(t('clear_export_history_empty'), emoji="â„¹ï¸")
            return
        
        # æ˜¾ç¤ºè­¦å‘Šå¹¶è¯·æ±‚ç¡®è®¤
        log.warning(t('clear_export_history_confirm'))
        log.info(f"   ğŸ“ {len(json_files)} JSON + {len(txt_files)} TXT = {len(all_files)} files", emoji="")
        
        confirm = input(f"\n{t('clear_export_history_prompt')}").strip().lower()
        
        if confirm != 'y':
            log.info(t('clear_export_history_cancelled'), emoji="")
            return
        
        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        for f in all_files:
            try:
                os.remove(f)
                deleted_count += 1
            except Exception as e:
                log.error(f"Failed to delete {f}: {e}")
        
        # æ¸…ç©ºå†…å­˜ä¸­çš„æ•°æ®
        self.data = []
        self.trends = {}
        self.chart_files = {}
        
        log.success(t('clear_export_history_done', count=deleted_count))
    
    def _clear_review_history(self):
        """æ¸…é™¤äººå·¥å®¡æ ¸è®°å½•å’Œå­¦ä¹ æŠ¥å‘Šï¼ˆéœ€è¦ç”¨æˆ·ç¡®è®¤ï¼‰"""
        import glob
        
        # æŸ¥æ‰¾æ‰€æœ‰å®¡æ ¸å†å²å’Œå­¦ä¹ æŠ¥å‘Šæ–‡ä»¶
        review_pattern = os.path.join(DATA_EXPORTS_DIR, 'review_history_*.json')
        learning_pattern = os.path.join(DATA_EXPORTS_DIR, 'learning_report_*.json')
        
        review_files = glob.glob(review_pattern)
        learning_files = glob.glob(learning_pattern)
        all_files = review_files + learning_files
        
        if not all_files:
            log.dual_info(t('clear_review_history_empty'), emoji="â„¹ï¸")
            return
        
        # æ˜¾ç¤ºè­¦å‘Šå¹¶è¯·æ±‚ç¡®è®¤
        log.dual_warning(t('clear_review_history_confirm'))
        log.dual_info(f"   ğŸ“ {len(review_files)} review_history + {len(learning_files)} learning_report = {len(all_files)} files", emoji="")
        
        confirm = input(f"\n{t('clear_export_history_prompt')}").strip().lower()
        
        if confirm != 'y':
            log.dual_info(t('clear_export_history_cancelled'), emoji="")
            return
        
        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        for f in all_files:
            try:
                os.remove(f)
                deleted_count += 1
            except Exception as e:
                log.dual_error(f"Failed to delete {f}: {e}")
        
        log.dual_success(t('clear_review_history_done', count=deleted_count))
    
    def _clear_all_data(self):
        """æ¸…é™¤æ‰€æœ‰æ•°æ®ï¼ˆéœ€è¦ç”¨æˆ·äºŒæ¬¡ç¡®è®¤ï¼‰"""
        import glob
        
        # æ˜¾ç¤ºä¸¥é‡è­¦å‘Š
        log.dual_separator()
        log.dual_warning(t('clear_all_data_confirm'))
        print(t('clear_all_data_list'))
        log.file(t('clear_all_data_list'))  # æ—¥å¿—è®°å½•
        print()
        log.dual_warning(t('clear_all_data_warning'))
        log.dual_separator()
        
        # è¦æ±‚è¾“å…¥ "YES" ç¡®è®¤
        confirm = input(f"\n{t('clear_all_data_prompt')}").strip()
        
        if confirm != 'YES':
            log.dual_info(t('clear_all_data_cancelled'), emoji="")
            return
        
        print()
        log.file("User confirmed: clearing all data...")  # æ—¥å¿—è®°å½•ç”¨æˆ·ç¡®è®¤
        deleted_total = 0
        
        # 1. æ¸…é™¤LLMåˆ†ç±»ç¼“å­˜
        cache_file = os.path.join(DATA_CACHE_DIR, 'llm_classification_cache.json')
        if os.path.exists(cache_file):
            try:
                os.remove(cache_file)
                deleted_total += 1
                log.dual_success(t('llm_cache_force_cleared'))
            except Exception as e:
                log.dual_error(f"Failed to delete LLM cache: {e}")
        
        # 2. æ¸…é™¤é‡‡é›†å†å²ç¼“å­˜
        self.collector.clear_history_cache()
        deleted_total += 1
        
        # 3. æ¸…é™¤é‡‡é›†ç»“æœå†å²
        json_pattern = os.path.join(DATA_EXPORTS_DIR, 'ai_tracker_data_*.json')
        txt_pattern = os.path.join(DATA_EXPORTS_DIR, 'ai_tracker_report_*.txt')
        export_files = glob.glob(json_pattern) + glob.glob(txt_pattern)
        for f in export_files:
            try:
                os.remove(f)
                deleted_total += 1
            except Exception as e:
                log.dual_error(f"Failed to delete {f}: {e}")
        
        if export_files:
            log.dual_info(f"Cleared {len(export_files)} export files", emoji="ğŸ—‘ï¸")
        
        # 4. æ¸…é™¤äººå·¥å®¡æ ¸è®°å½•
        review_pattern = os.path.join(DATA_EXPORTS_DIR, 'review_history_*.json')
        learning_pattern = os.path.join(DATA_EXPORTS_DIR, 'learning_report_*.json')
        review_files = glob.glob(review_pattern) + glob.glob(learning_pattern)
        for f in review_files:
            try:
                os.remove(f)
                deleted_total += 1
            except Exception as e:
                log.dual_error(f"Failed to delete {f}: {e}")
        
        if review_files:
            log.dual_info(f"Cleared {len(review_files)} review files", emoji="ğŸ—‘ï¸")
        
        # æ¸…ç©ºå†…å­˜ä¸­çš„æ•°æ®
        self.data = []
        self.trends = {}
        self.chart_files = {}
        
        print()
        log.dual_success(t('clear_all_data_done'))
        log.dual_info(f"   ğŸ“ {deleted_total} files deleted", emoji="")
    
    def _check_llm_availability(self):
        """æ£€æŸ¥LLMæœåŠ¡å¯ç”¨æ€§ï¼Œæä¾›å¯åŠ¨å¸®åŠ©"""
        status = check_ollama_status()
        
        if status['running']:
            if status['models']:
                log.dual_success(t('ollama_running') + ", " + t('ollama_available_models', models=', '.join(status['models'][:3])))
                if status['recommended']:
                    self.llm_model = status['recommended']
            else:
                log.warning(t('ollama_no_models_warning'))
                log.dual_info(t('ollama_install_hint'), emoji="ğŸ’¡")
                log.dual_info(t('ollama_no_llm_hint'), emoji="â„¹ï¸")
        else:
            log.warning(t('ollama_not_running_info'))
            self._offer_ollama_startup_help()
    
    def _start_ollama_service(self, show_progress: bool = True) -> dict:
        """
        å¯åŠ¨ Ollama æœåŠ¡çš„æ ¸å¿ƒé€»è¾‘ï¼ˆå…¬å…±æ–¹æ³•ï¼‰
        
        Args:
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ç‚¹
            
        Returns:
            dict: {
                'success': bool,      # æ˜¯å¦å¯åŠ¨æˆåŠŸ
                'status': dict|None,  # OllamaçŠ¶æ€ä¿¡æ¯ï¼ˆæˆåŠŸæ—¶ï¼‰
                'error': str|None     # é”™è¯¯ç±»å‹: 'timeout', 'not_found', æˆ–å…·ä½“é”™è¯¯ä¿¡æ¯
            }
        """
        import subprocess
        import platform
        import time
        
        try:
            # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©å¯åŠ¨æ–¹å¼
            system = platform.system()
            if system == 'Windows':
                subprocess.Popen(
                    ['ollama', 'serve'],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    creationflags=subprocess.CREATE_NO_WINDOW
                )
            else:
                subprocess.Popen(
                    ['ollama', 'serve'],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    start_new_session=True
                )
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            for _ in range(OLLAMA_STARTUP_TIMEOUT):
                time.sleep(1)
                if show_progress:
                    print('.', end='', flush=True)
                status = check_ollama_status()
                if status['running']:
                    return {'success': True, 'status': status, 'error': None}
            
            return {'success': False, 'status': None, 'error': 'timeout'}
            
        except FileNotFoundError:
            return {'success': False, 'status': None, 'error': 'not_found'}
        except Exception as e:
            return {'success': False, 'status': None, 'error': str(e)}
    
    def _handle_ollama_start_error(self, error: str, indent: str = ""):
        """
        ç»Ÿä¸€å¤„ç† Ollama å¯åŠ¨é”™è¯¯
        
        Args:
            error: é”™è¯¯ç±»å‹æˆ–ä¿¡æ¯
            indent: è¾“å‡ºç¼©è¿›
        """
        if error == 'timeout':
            print(f"\n{indent}" + t('ollama_timeout'))
        elif error == 'not_found':
            print(f"\n{indent}" + t('ollama_not_found'))
            print(f"{indent}" + t('ollama_download'))
        else:
            print(f"\n{indent}" + t('ollama_start_failed', error=error))
            print(f"{indent}" + t('ollama_manual_start'))
    
    def _offer_ollama_startup_help(self):
        """æä¾›Ollamaå¯åŠ¨å¸®åŠ©"""
        print("\n   " + t('ollama_hint'))
        
        # è‡ªåŠ¨æ¨¡å¼ä¸‹è·³è¿‡äº¤äº’å¼æç¤º
        if self.auto_mode:
            print("   " + t('ollama_skip_auto'))
            return
        
        prompt = "   " + t('ollama_start_prompt')
        choice = input(prompt).strip().lower()
        
        if choice == 'y':
            print("\n   " + t('ollama_starting'))
            print("   " + t('ollama_waiting'), end='', flush=True)
            
            result = self._start_ollama_service(show_progress=True)
            
            if result['success']:
                print("\n   " + t('ollama_started'))
                status = result['status']
                if status.get('models'):
                    print(f"   " + t('ollama_available_models', models=', '.join(status['models'][:3])))
                    if status.get('recommended'):
                        self.llm_model = status['recommended']
                else:
                    print("   " + t('no_models'))
                    print("   " + t('ollama_no_local_llm'))
            else:
                self._handle_ollama_start_error(result['error'], indent="   ")
                if result['error'] == 'not_found':
                    print("   " + t('ollama_no_local_llm'))
        else:
            print("   " + t('ollama_no_local_llm'))
            print("   " + t('ollama_later_hint'))
    
    def _offer_ollama_startup_help_in_menu(self):
        """åœ¨èœå•ä¸­æä¾›Ollamaå¯åŠ¨å¸®åŠ©ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        prompt = "Start Ollama service? (y/n) [n]: " if get_language() == 'en' else "æ˜¯å¦å°è¯•å¯åŠ¨OllamaæœåŠ¡? (y/n) [n]: "
        choice = input(prompt).strip().lower()
        
        if choice == 'y':
            print("\n" + t('ollama_starting'))
            log.info(t('ollama_waiting'), emoji="â³")
            
            result = self._start_ollama_service(show_progress=True)
            
            if result['success']:
                print("\n" + t('ollama_started'))
            else:
                self._handle_ollama_start_error(result['error'], indent="")
    
    def _install_ollama_model(self, model_name: str):
        """å®‰è£…Ollamaæ¨¡å‹"""
        import subprocess
        
        print("\n" + t('model_installing', model=model_name))
        log.info(t('model_install_wait'), emoji="â³")
        print()
        
        try:
            # å®æ—¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦
            process = subprocess.Popen(
                ['ollama', 'pull', model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )
            
            for line in process.stdout:
                print(f"  {line.strip()}")
            
            process.wait()
            
            if process.returncode == 0:
                print("\n" + t('model_installed', model=model_name))
                self.llm_model = model_name
            else:
                print("\n" + t('model_install_failed', code=process.returncode))
                
        except FileNotFoundError:
            print("\n" + t('ollama_not_found'))
        except Exception as e:
            print("\n" + t('model_install_error', error=str(e)))
    
    def _load_latest_data(self):
        """å°è¯•åŠ è½½æœ€æ–°çš„æ•°æ®æ–‡ä»¶"""
        try:
            # ä» exports ç›®å½•åŠ è½½æ•°æ®
            if not os.path.exists(DATA_EXPORTS_DIR):
                return
            files = [f for f in os.listdir(DATA_EXPORTS_DIR) if f.startswith('ai_tracker_data_') and f.endswith('.json')]
            if not files:
                return
            
            latest_file = os.path.join(DATA_EXPORTS_DIR, max(files))
            log.data(t('loading_history', file=os.path.basename(latest_file)))
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                saved_data = json.load(f)
                
            self.data = saved_data.get('data', [])
            self.trends = saved_data.get('trends', {})
            
            # å°è¯•åŠ è½½å›¾è¡¨æ–‡ä»¶
            if os.path.exists('visualizations'):
                self.chart_files = {
                    'tech_hotspots': os.path.join('visualizations', 'tech_hotspots.png'),
                    'content_distribution': os.path.join('visualizations', 'content_distribution.png'),
                    'region_distribution': os.path.join('visualizations', 'region_distribution.png'),
                    'daily_trends': os.path.join('visualizations', 'daily_trends.png'),
                    'dashboard': os.path.join('visualizations', 'dashboard.png')
                }
                # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                self.chart_files = {k: v for k, v in self.chart_files.items() if os.path.exists(v)}
            
            log.dual_success(t('history_loaded', count=len(self.data)))
        except Exception as e:
            log.warning(t('history_load_failed', error=str(e)))
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†æµç¨‹"""
        import time
        start_time = time.time()
        timing_stats = {}  # æ”¶é›†è€—æ—¶ç»Ÿè®¡
        
        log.dual_start(t('start_pipeline'))
        
        # æ­¥éª¤1: æ•°æ®é‡‡é›†
        step_start = time.time()
        log.step(1, 5, t('step_collect'))
        raw_data = self.collector.collect_all()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_items = []
        for category, items in raw_data.items():
            all_items.extend(items)
        
        timing_stats['data_collection'] = round(time.time() - step_start, 1)
        log.data(t('collected_items', count=len(all_items)))
        
        # æ­¥éª¤2: å†…å®¹åˆ†ç±»ï¼ˆæ ¹æ®å½“å‰æ¨¡å¼é€‰æ‹©åˆ†ç±»å™¨ï¼‰
        step_start = time.time()
        log.step(2, 5, t('step_classify'))
        self.data = self._classify_data(all_items)
        timing_stats['classification'] = round(time.time() - step_start, 1)
        log.timing(t('classification_time', time=timing_stats['classification']), timing_stats['classification'])
        
        # æ­¥éª¤3: æ™ºèƒ½åˆ†æ
        step_start = time.time()
        log.step(3, 5, t('step_analyze'))
        self.trends = self.analyzer.analyze_trends(self.data)
        timing_stats['analysis'] = round(time.time() - step_start, 1)
        
        # æ­¥éª¤4: æ•°æ®å¯è§†åŒ–
        step_start = time.time()
        log.step(4, 5, t('step_visualize'))
        self.chart_files = self.visualizer.visualize_all(self.trends)
        timing_stats['visualization'] = round(time.time() - step_start, 1)
        
        # æ­¥éª¤5: ç”ŸæˆWebé¡µé¢
        step_start = time.time()
        log.step(5, 5, t('step_web'))
        web_file = self.web_publisher.generate_html_page(self.data, self.trends, self.chart_files)
        timing_stats['web_generation'] = round(time.time() - step_start, 1)
        
        # è®¡ç®—æ€»è€—æ—¶
        timing_stats['total'] = round(time.time() - start_time, 1)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.analyzer.generate_report(self.data, self.trends)
        
        # ä¿å­˜æ•°æ®å’ŒæŠ¥å‘Šï¼ˆåŒ…å«è€—æ—¶ç»Ÿè®¡ï¼‰
        self._save_results(report, web_file, timing_stats)
        
        log.dual_separator()
        log.dual_done(t('process_complete'))
        log.dual_separator()
        log.dual_chart(t('charts_generated', count=len([f for f in self.chart_files.values() if f])))
        log.dual_file(t('report_saved'))
        log.dual_data(t('data_saved'))
        log.dual_info(t('web_generated'), emoji="ğŸŒ")
        
        # è¯¢é—®æ˜¯å¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ç½‘é¡µ
        self._ask_open_web_page(web_file)
        
        return report
    
    def show_menu(self):
        """æ˜¾ç¤ºäº¤äº’èœå•"""
        while True:
            # æ˜¾ç¤ºå½“å‰åˆ†ç±»æ¨¡å¼
            mode_str = self._get_mode_display()
            
            log.dual_section(t('menu_title') + f"\n   {t('menu_current_mode')}: {mode_str}")
            log.menu(t('menu_option_1'))
            log.menu(t('menu_option_2'))
            log.menu(t('menu_option_3'))
            log.menu(t('menu_option_4'))
            log.menu(t('menu_option_5'))
            log.menu(t('menu_option_0'))
            log.dual_separator()
            
            choice = input(f"\n{t('menu_choice')}: ").strip()
            
            if choice == '1':
                self.run_full_pipeline()
            elif choice == '2':
                self._generate_web_page()
            elif choice == '3':
                self._manual_review()
            elif choice == '4':
                self._learning_feedback()
            elif choice == '5':
                self._switch_classification_mode()
            elif choice == '0':
                log.dual_success(t('menu_goodbye'))
                break
            else:
                log.warning(t('menu_invalid'))
    
    def _get_mode_display(self) -> str:
        """è·å–å½“å‰æ¨¡å¼çš„æ˜¾ç¤ºå­—ç¬¦ä¸²"""
        if self.classification_mode == 'llm':
            if get_language() == 'en':
                return f"ğŸ¤– LLM Mode ({self.llm_provider}/{self.llm_model})"
            return f"ğŸ¤– LLMæ¨¡å¼ ({self.llm_provider}/{self.llm_model})"
        else:
            if get_language() == 'en':
                return "ğŸ“ Rule Mode (Rule-based)"
            return "ğŸ“ è§„åˆ™æ¨¡å¼ (Rule-based)"
    
    def _switch_classification_mode(self):
        """è®¾ç½®ä¸ç®¡ç†èœå•"""
        log.section(t('switch_mode_title'))
        
        log.menu(f"\n{t('current_mode')}: {self._get_mode_display()}")
        
        # åˆ†ç±»æ¨¡å¼åˆ†ç»„
        log.menu(f"\n{t('settings_classification_mode')}:")
        log.menu(f"  1. {t('mode_rule_desc')}")
        
        if LLM_AVAILABLE:
            log.menu(f"  2. {t('mode_ollama_desc')}")
            log.menu(f"  3. {t('mode_openai_desc')}")
        else:
            log.menu(f"  {t('llm_not_available')}")
        
        # æ•°æ®ç»´æŠ¤åˆ†ç»„
        log.menu(f"\n{t('settings_data_maintenance')}:")
        if LLM_AVAILABLE:
            log.menu(f"  4. {t('clear_llm_cache')}")
        log.menu(f"  5. {t('clear_collection_cache')}")
        log.menu(f"  6. {t('clear_export_history')}")
        log.menu(f"  7. {t('clear_review_history')}")
        log.menu(f"  8. {t('clear_all_data')}")
        
        log.menu(f"\n  0. {t('back_to_main_menu')}")
        
        choice = input(f"\n{t('select_model')} (0-8): ").strip()
        
        if choice == '0' or choice == '':
            return  # è¿”å›ä¸»èœå•
        
        elif choice == '1':
            self.classification_mode = 'rule'
            self.llm_classifier = None
            self._save_user_config()
            log.success(t('switched_to_rule'))
        
        elif choice == '2' and LLM_AVAILABLE:
            self._setup_ollama_mode()
        
        elif choice == '3' and LLM_AVAILABLE:
            self._setup_openai_mode()
        
        elif choice == '4' and LLM_AVAILABLE:
            self._force_clear_llm_cache()
            # é‡æ–°åŠ è½½LLMåˆ†ç±»å™¨ï¼ˆå¦‚æœå½“å‰æ˜¯LLMæ¨¡å¼ï¼‰
            if self.llm_classifier:
                log.ai(t('reinit_llm_classifier'))
                self._try_restore_llm_classifier(clear_cache=False)  # ä¸éœ€è¦å†æ¸…é™¤ï¼Œå·²ç»æ¸…é™¤äº†
        
        elif choice == '5':
            self.collector.clear_history_cache()
        
        elif choice == '6':
            self._clear_export_history()
        
        elif choice == '7':
            self._clear_review_history()
        
        elif choice == '8':
            self._clear_all_data()
        
        else:
            log.warning(t('invalid_choice'))
    
    def _setup_ollama_mode(self):
        """è®¾ç½®Ollamaæ¨¡å¼"""
        status = check_ollama_status()
        
        if not status['running']:
            log.warning(t('ollama_not_running'))
            self._offer_ollama_startup_help_in_menu()
            
            # é‡æ–°æ£€æŸ¥çŠ¶æ€
            status = check_ollama_status()
            if not status['running']:
                log.error(t('ollama_cannot_connect'))
                return
        
        log.success(t('ollama_running'))
        log.menu(f"\n{t('available_models')}:")
        
        models = status['models']
        if not models:
            log.menu("  " + t('no_models'))
            log.menu("  " + t('install_model_hint'))
            
            prompt = "\nInstall recommended model qwen3:8b now? (y/n) [n]: " if get_language() == 'en' else "\næ˜¯å¦ç°åœ¨å®‰è£…æ¨èæ¨¡å‹ qwen3:8b? (y/n) [n]: "
            choice = input(prompt).strip().lower()
            if choice == 'y':
                self._install_ollama_model('qwen3:8b')
                # é‡æ–°è·å–æ¨¡å‹åˆ—è¡¨
                status = check_ollama_status()
                models = status['models']
            
            if not models:
                log.warning(t('no_available_models'))
                return
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
        recommended_label = " â­ " + ("recommended" if get_language() == 'en' else "æ¨è")
        for i, model in enumerate(models, 1):
            recommended = recommended_label if model == status['recommended'] else ""
            log.menu(f"  {i}. {model}{recommended}")
        
        prompt = f"\n{t('select_model')} (1-{len(models)}) [" + ("default: 1" if get_language() == 'en' else "é»˜è®¤: 1") + "]: "
        model_choice = input(prompt).strip() or '1'
        
        try:
            idx = int(model_choice) - 1
            selected_model = models[idx] if 0 <= idx < len(models) else models[0]
        except (ValueError, IndexError):
            selected_model = models[0]
        
        # åˆå§‹åŒ–LLMåˆ†ç±»å™¨
        self.classification_mode = 'llm'
        self.llm_provider = 'ollama'
        self.llm_model = selected_model
        
        try:
            self.llm_classifier = LLMClassifier(
                provider='ollama',
                model=selected_model,
                enable_cache=True,
                max_workers=3,  # é»˜è®¤å¹¶å‘æ•°ï¼ŒGPUæ¨¡å¼è‡ªåŠ¨æå‡è‡³6
                batch_size=5    # å¯ç”¨æ‰¹é‡åˆ†ç±»
            )
            self._save_user_config()
            log.success(t('switched_to_llm', provider='Ollama', model=selected_model))
            
            # é¢„çƒ­æ¨¡å‹
            warmup_prompt = "\nWarm up the model now? (Y/n): " if get_language() == 'en' else "\næ˜¯å¦ç°åœ¨é¢„çƒ­æ¨¡å‹? (Y/n): "
            warmup = input(warmup_prompt).strip().lower()
            if warmup != 'n':
                self.llm_classifier.warmup_model()
                
        except Exception as e:
            log.error(t('llm_init_failed', error=str(e)))
            self.classification_mode = 'rule'
            self._save_user_config()
    
    def _setup_openai_mode(self):
        """è®¾ç½®Azure OpenAIæ¨¡å¼"""
        # ç›´æ¥è°ƒç”¨Azure OpenAIè®¾ç½®
        self._setup_azure_openai_mode()
    
    def _setup_standard_openai_mode(self):
        """è®¾ç½®æ ‡å‡†OpenAIæ¨¡å¼"""
        is_zh = get_language() == 'zh'
        
        # æ”¶é›† API Key
        log.info("è¯·è¾“å…¥OpenAI APIå¯†é’¥:" if is_zh else "Enter OpenAI API key:", emoji="ğŸ”‘")
        api_key = input("API Key: ").strip()
        if not api_key:
            log.info("å·²å–æ¶ˆè®¾ç½®" if is_zh else "Setup cancelled", emoji="â„¹ï¸")
            return
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
        log.menu("\n" + t('available_openai_models'))
        models = list(AVAILABLE_MODELS[LLMProvider.OPENAI].keys())
        for i, model in enumerate(models, 1):
            info = AVAILABLE_MODELS[LLMProvider.OPENAI][model]
            log.menu(f"  {i}. {info['name']} - {info['description']}")
        
        # é€‰æ‹©æ¨¡å‹
        prompt = f"\n" + ("è¯·é€‰æ‹©æ¨¡å‹" if is_zh else "Select model") + f" (1-{len(models)}): "
        model_choice = input(prompt).strip()
        
        try:
            idx = int(model_choice) - 1
            if not (0 <= idx < len(models)):
                log.warning("æ— æ•ˆé€‰æ‹©" if is_zh else "Invalid choice")
                return
            selected_model = models[idx]
        except (ValueError, IndexError):
            log.warning("æ— æ•ˆé€‰æ‹©" if is_zh else "Invalid choice")
            return
        
        # åˆ›å»ºåˆ†ç±»å™¨
        self.classification_mode = 'llm'
        self.llm_provider = 'openai'
        self.llm_model = selected_model
        
        try:
            self.llm_classifier = LLMClassifier(
                provider='openai',
                model=selected_model,
                api_key=api_key,
                enable_cache=True,
                max_workers=3
            )
            self._save_user_config()
            log.success(t('switched_to_llm', provider='OpenAI', model=selected_model))
        except Exception as e:
            log.error(t('llm_init_failed', error=str(e)))
            self.classification_mode = 'rule'
            self._save_user_config()
    
    def _setup_azure_openai_mode(self):
        """è®¾ç½®Azure OpenAIæ¨¡å¼ - éœ€è¦æ”¶é›†æ‰€æœ‰å¿…è¦å‚æ•°"""
        is_zh = get_language() == 'zh'
        
        log.section("Azure OpenAI " + ("é…ç½®" if is_zh else "Configuration"))
        log.info("è¯·ä¾æ¬¡è¾“å…¥ä»¥ä¸‹å‚æ•° (ä»Azureé—¨æˆ·è·å–):" if is_zh else "Enter the following parameters (from Azure Portal):", emoji="ğŸ“‹")
        
        # 1. æ”¶é›† Endpoint
        log.menu("\n1. Azure OpenAI Endpoint")
        log.menu("   " + ("æ ¼å¼: https://ä½ çš„èµ„æºå.openai.azure.com/" if is_zh else "Format: https://your-resource-name.openai.azure.com/"))
        endpoint = input("Endpoint: ").strip()
        if not endpoint:
            log.info("å·²å–æ¶ˆè®¾ç½®" if is_zh else "Setup cancelled", emoji="â„¹ï¸")
            return
        
        # éªŒè¯endpointæ ¼å¼
        if not endpoint.startswith('https://') or not endpoint.endswith('.openai.azure.com/'):
            if not endpoint.endswith('/'):
                endpoint += '/'
            if not endpoint.startswith('https://'):
                log.warning("Endpointåº”ä»¥ https:// å¼€å¤´" if is_zh else "Endpoint should start with https://")
        
        # 2. æ”¶é›† API Key
        log.menu("\n2. Azure OpenAI API Key")
        log.menu("   " + ("ä» Azureé—¨æˆ· -> ä½ çš„OpenAIèµ„æº -> å¯†é’¥å’Œç»ˆç»“ç‚¹ è·å–" if is_zh else "Get from Azure Portal -> Your OpenAI Resource -> Keys and Endpoint"))
        api_key = input("API Key: ").strip()
        if not api_key:
            log.info("å·²å–æ¶ˆè®¾ç½®" if is_zh else "Setup cancelled", emoji="â„¹ï¸")
            return
        
        # 3. æ”¶é›† Deployment Name
        log.menu("\n3. Deployment Name (" + ("éƒ¨ç½²åç§°" if is_zh else "Deployment Name") + ")")
        log.menu("   " + ("è¿™æ˜¯ä½ åœ¨Azureä¸­åˆ›å»ºçš„æ¨¡å‹éƒ¨ç½²åç§°ï¼Œä¸æ˜¯æ¨¡å‹åç§°" if is_zh else "This is the deployment name you created in Azure, not the model name"))
        deployment_name = input("Deployment Name: ").strip()
        if not deployment_name:
            log.info("å·²å–æ¶ˆè®¾ç½®" if is_zh else "Setup cancelled", emoji="â„¹ï¸")
            return
        
        # 4. æ”¶é›† API Version
        log.menu("\n4. API Version")
        log.menu("   " + ("å¸¸ç”¨ç‰ˆæœ¬: 2024-02-15-preview, 2024-05-01-preview, 2024-08-01-preview" if is_zh else "Common versions: 2024-02-15-preview, 2024-05-01-preview, 2024-08-01-preview"))
        api_version = input("API Version: ").strip()
        if not api_version:
            log.info("å·²å–æ¶ˆè®¾ç½®" if is_zh else "Setup cancelled", emoji="â„¹ï¸")
            return
        
        # æ˜¾ç¤ºé…ç½®æ‘˜è¦
        log.section("é…ç½®æ‘˜è¦" if is_zh else "Configuration Summary")
        log.menu(f"  Endpoint: {endpoint}")
        log.menu(f"  API Key: {api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else f"  API Key: ***")
        log.menu(f"  Deployment: {deployment_name}")
        log.menu(f"  API Version: {api_version}")
        
        confirm = input("\n" + ("ç¡®è®¤é…ç½®? (y/N): " if is_zh else "Confirm configuration? (y/N): ")).strip().lower()
        if confirm != 'y':
            log.info("å·²å–æ¶ˆè®¾ç½®" if is_zh else "Setup cancelled", emoji="â„¹ï¸")
            return
        
        # åˆ›å»ºåˆ†ç±»å™¨
        self.classification_mode = 'llm'
        self.llm_provider = 'azure_openai'
        self.llm_model = deployment_name
        
        try:
            self.llm_classifier = LLMClassifier(
                provider='azure_openai',
                model=deployment_name,
                api_key=api_key,
                azure_endpoint=endpoint,
                azure_api_version=api_version,
                enable_cache=True,
                max_workers=3
            )
            self._save_user_config()
            log.success(t('switched_to_llm', provider='Azure OpenAI', model=deployment_name))
        except Exception as e:
            log.error(t('llm_init_failed', error=str(e)))
            self.classification_mode = 'rule'
            self._save_user_config()
    
    def _setup_anthropic_mode(self):
        """è®¾ç½®Anthropicæ¨¡å¼"""
        api_key = os.getenv('ANTHROPIC_API_KEY')
        
        if not api_key:
            log.warning(t('llm_api_key_missing', provider='ANTHROPIC'))
            prompt = "Enter Anthropic API key (or press Enter to cancel): " if get_language() == 'en' else "è¯·è¾“å…¥Anthropic APIå¯†é’¥ (æˆ–æŒ‰Enterå–æ¶ˆ): "
            api_key = input(prompt).strip()
            if not api_key:
                return
        
        log.menu("\n" + t('available_anthropic_models'))
        models = list(AVAILABLE_MODELS[LLMProvider.ANTHROPIC].keys())
        for i, model in enumerate(models, 1):
            info = AVAILABLE_MODELS[LLMProvider.ANTHROPIC][model]
            log.menu(f"  {i}. {info['name']} - {info['description']}")
        
        prompt = f"\n{t('select_model')} (1-{len(models)}) [" + ("default: 1" if get_language() == 'en' else "é»˜è®¤: 1") + "]: "
        model_choice = input(prompt).strip() or '1'
        
        try:
            idx = int(model_choice) - 1
            selected_model = models[idx] if 0 <= idx < len(models) else models[0]
        except (ValueError, IndexError):
            selected_model = models[0]
        
        self.classification_mode = 'llm'
        self.llm_provider = 'anthropic'
        self.llm_model = selected_model
        
        try:
            self.llm_classifier = LLMClassifier(
                provider='anthropic',
                model=selected_model,
                api_key=api_key,
                enable_cache=True,
                max_workers=3
            )
            self._save_user_config()
            log.success(t('switched_to_llm', provider='Anthropic', model=selected_model))
        except Exception as e:
            log.error(t('llm_init_failed', error=str(e)))
            self.classification_mode = 'rule'
            self._save_user_config()
    
    def _classify_data(self, items: list) -> list:
        """æ ¹æ®å½“å‰æ¨¡å¼åˆ†ç±»æ•°æ®"""
        if self.classification_mode == 'llm' and self.llm_classifier:
            print(f"\n" + t('using_llm', provider=self.llm_provider, model=self.llm_model))
            return self.llm_classifier.classify_batch(items)
        else:
            print("\n" + t('using_rule'))
            return self.classifier.classify_batch(items)
    
    def _collect_only(self):
        """ä»…é‡‡é›†æ•°æ®"""
        print("\n" + t('collecting') + "\n")
        raw_data = self.collector.collect_all()
        
        all_items = []
        for items in raw_data.values():
            all_items.extend(items)
        
        self.data = self.classifier.classify_batch(all_items)
        print(f"\n" + t('collect_done', count=len(self.data)))
    
    def _show_statistics(self):
        """æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡"""
        if not self.data:
            print("\n" + t('no_data'))
            return
        
        print("\n" + t('stats_overview'))
        print("   " + t('stats_total', count=len(self.data)))
        
        # å†…å®¹ç±»å‹ç»Ÿè®¡
        type_count = {}
        for item in self.data:
            ct = item.get('content_type', 'unknown')
            type_count[ct] = type_count.get(ct, 0) + 1
        
        print("\n   " + t('stats_by_type'))
        for ctype, count in type_count.items():
            print("   " + t('stats_item', name=ctype, count=count))
        
        # åœ°åŒºç»Ÿè®¡
        region_count = {}
        for item in self.data:
            region = item.get('region', 'unknown')
            region_count[region] = region_count.get(region, 0) + 1
        
        print("\n   " + t('stats_by_region'))
        for region, count in region_count.items():
            print("   " + t('stats_item', name=region, count=count))
    
    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        if not self.data:
            print("\n" + t('no_data'))
            return
        
        if not self.trends:
            print("\n" + t('analyzing'))
            self.trends = self.analyzer.analyze_trends(self.data)
        
        print("\n" + t('generating_charts'))
        self.chart_files = self.visualizer.visualize_all(self.trends)
    
    def _show_report(self):
        """æ˜¾ç¤ºåˆ†ææŠ¥å‘Š"""
        if not self.data:
            print("\n" + t('no_data'))
            return
        
        if not self.trends:
            print("\n" + t('generating_analysis'))
            self.trends = self.analyzer.analyze_trends(self.data)
        
        report = self.analyzer.generate_report(self.data, self.trends)
        print("\n" + report)
    
    def _filter_data(self):
        """æŒ‰æ¡ä»¶ç­›é€‰æ•°æ®"""
        if not self.data:
            print("\n" + t('no_data'))
            return
        
        print("\n" + t('filter_title'))
        print(t('filter_by_type'))
        print(t('filter_by_region'))
        print(t('filter_by_tech'))
        
        filter_prompt = "\nSelect filter method (1-3): " if get_language() == 'en' else "\né€‰æ‹©ç­›é€‰æ–¹å¼ (1-3): "
        filter_choice = input(filter_prompt).strip()
        
        if filter_choice == '1':
            ctype_prompt = "Enter content type (research/product/market): " if get_language() == 'en' else "è¾“å…¥å†…å®¹ç±»å‹ (research/product/market): "
            ctype = input(ctype_prompt).strip()
            filtered = self.classifier.get_filtered_items(self.data, content_type=ctype)
        elif filter_choice == '2':
            region_prompt = "Enter region (China/USA/Europe/Global): " if get_language() == 'en' else "è¾“å…¥åœ°åŒº (China/USA/Europe/Global): "
            region = input(region_prompt).strip()
            filtered = self.classifier.get_filtered_items(self.data, region=region)
        elif filter_choice == '3':
            tech_prompt = "Enter tech field (e.g., NLP, Computer Vision): " if get_language() == 'en' else "è¾“å…¥æŠ€æœ¯é¢†åŸŸ (å¦‚: NLP, Computer Vision): "
            tech = input(tech_prompt).strip()
            filtered = self.classifier.get_filtered_items(self.data, tech_category=tech)
        else:
            log.warning(t('invalid_choice'))
            return
        
        print(f"\n" + t('filter_result', count=len(filtered)) + "\n")
        
        # æ˜¾ç¤ºå‰5æ¡
        for i, item in enumerate(filtered[:5], 1):
            print(f"{i}. {item.get('title', 'No title')}")
            type_label = "Type" if get_language() == 'en' else "ç±»å‹"
            region_label = "Region" if get_language() == 'en' else "åœ°åŒº"
            source_label = "Source" if get_language() == 'en' else "æ¥æº"
            date_label = "Date" if get_language() == 'en' else "æ—¥æœŸ"
            print(f"   {type_label}: {item.get('content_type')} | {region_label}: {item.get('region')}")
            print(f"   {source_label}: {item.get('source')} | {date_label}: {item.get('published', 'N/A')}\n")
        
        if len(filtered) > 5:
            print("   " + t('filter_more', count=len(filtered) - 5))
    
    def _ask_open_web_page(self, web_file: str):
        """è¯¢é—®ç”¨æˆ·æ˜¯å¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ç½‘é¡µ"""
        if not web_file or not os.path.exists(web_file):
            return
        
        try:
            import webbrowser
            prompt = "\nOpen web page in browser? (Y/N): " if get_language() == 'en' else "\næ˜¯å¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€Webé¡µé¢? (Y/N): "
            choice = input(prompt).strip().lower()
            if choice in ['y', 'yes', 'æ˜¯']:
                webbrowser.open(f'file://{os.path.abspath(web_file)}')
                log.success(t('opened_browser'))
        except Exception as e:
            log.error(t('browser_error', error=str(e)))
            log.info(t('manual_open', file=os.path.abspath(web_file)), emoji="ğŸ“„")
    
    def _generate_web_page(self):
        """ç”ŸæˆWebé¡µé¢"""
        if not self.data:
            print("\n" + t('no_data'))
            return
        
        if not self.trends:
            print("\n" + t('generating_analysis'))
            self.trends = self.analyzer.analyze_trends(self.data)
        
        if not self.chart_files:
            print("\n" + t('generating_charts'))
            self.chart_files = self.visualizer.visualize_all(self.trends)
        
        print("\n" + t('generating_web'))
        web_file = self.web_publisher.generate_html_page(self.data, self.trends, self.chart_files)
        
        # è¯¢é—®æ˜¯å¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€
        self._ask_open_web_page(web_file)
    
    def _manual_review(self):
        """äººå·¥å®¡æ ¸åˆ†ç±»"""
        if not self.data:
            print("\n" + t('no_data'))
            return
        
        print("\n" + "="*60)
        print(t('manual_review_title'))
        print("="*60)
        
        # æ£€æŸ¥éœ€è¦å®¡æ ¸çš„å†…å®¹
        review_items = self.reviewer.get_items_for_review(self.data, min_confidence=0.6)
        
        print(f"\n" + t('review_stats'))
        print("   " + t('review_total', count=len(self.data)))
        print("   " + t('review_need', count=len(review_items), percent=f"{len(review_items)/len(self.data):.1%}"))
        
        if not review_items:
            print("\n" + t('review_not_needed'))
            return
        
        # æ˜¾ç¤ºéœ€è¦å®¡æ ¸çš„å†…å®¹æ¦‚è§ˆ
        print("\n" + t('review_list'))
        conf_label = "confidence" if get_language() == 'en' else "ç½®ä¿¡åº¦"
        for i, item in enumerate(review_items[:5], 1):
            print(f"   {i}. {item.get('title', 'N/A')[:50]}... ({conf_label}: {item.get('confidence', 0):.1%})")
        
        if len(review_items) > 5:
            print("   " + t('review_more', count=len(review_items)-5))
        
        print("\n" + t('review_options'))
        print("   " + t('review_opt_1'))
        print("   " + t('review_opt_2'))
        print("   " + t('review_opt_3'))
        print("   " + t('review_opt_0'))
        
        choice_prompt = "\nPlease select (0-3): " if get_language() == 'en' else "\nè¯·é€‰æ‹© (0-3): "
        choice = input(choice_prompt).strip()
        
        if choice == '1':
            # æ‰¹é‡å®¡æ ¸
            self.data = self.reviewer.batch_review(self.data, min_confidence=0.6)
            
            # ä¿å­˜å®¡æ ¸åçš„æ•°æ®
            save_prompt = "\nSave reviewed data? (Y/N): " if get_language() == 'en' else "\næ˜¯å¦ä¿å­˜å®¡æ ¸åçš„æ•°æ®? (Y/N): "
            save = input(save_prompt).strip().lower()
            if save == 'y':
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = os.path.join(DATA_EXPORTS_DIR, f'ai_tracker_data_reviewed_{timestamp}.json')
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump({
                        'metadata': {
                            'timestamp': timestamp,
                            'total_items': len(self.data),
                            'reviewed': True
                        },
                        'data': self.data,
                        'trends': self.trends
                    }, f, ensure_ascii=False, indent=2)
                log.file(t('review_saved', file=os.path.basename(filename)))
            
            # ä¿å­˜å®¡æ ¸å†å²
            self.reviewer.save_review_history()
            
            # æ˜¾ç¤ºå®¡æ ¸æ‘˜è¦
            summary = self.reviewer.get_review_summary()
            print(f"\n" + t('review_summary'))
            print("   " + t('review_summary_total', count=summary['total']))
            for action, count in summary['actions'].items():
                print(f"   - {action}: {count}")
            
            # è¯¢é—®æ˜¯å¦é‡æ–°ç”Ÿæˆåˆ†æå’ŒWebé¡µé¢
            print("\n" + "="*60)
            regen_prompt = "\nRegenerate report and web page based on reviewed data? (Y/N): " if get_language() == 'en' else "\næ˜¯å¦åŸºäºå®¡æ ¸åçš„æ•°æ®é‡æ–°ç”ŸæˆæŠ¥å‘Šå’ŒWebé¡µé¢? (Y/N): "
            regenerate = input(regen_prompt).strip().lower()
            if regenerate == 'y':
                self._regenerate_after_review()
        
        elif choice == '2':
            # è‡ªå®šä¹‰é˜ˆå€¼
            try:
                threshold_prompt = "\nEnter confidence threshold (0.0-1.0, e.g., 0.7): " if get_language() == 'en' else "\nè¯·è¾“å…¥ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0, å¦‚ 0.7): "
                threshold = float(input(threshold_prompt).strip())
                if 0 <= threshold <= 1:
                    self.data = self.reviewer.batch_review(self.data, min_confidence=threshold)
                else:
                    log.warning(t('review_threshold_error'))
            except ValueError:
                log.error(t('review_input_error'))
        
        elif choice == '3':
            # ä»…æŸ¥çœ‹åˆ—è¡¨
            print("\n" + "="*70)
            print(t('review_list_title'))
            print("="*70)
            cat_label = "Category" if get_language() == 'en' else "åˆ†ç±»"
            conf_label = "Confidence" if get_language() == 'en' else "ç½®ä¿¡åº¦"
            source_label = "Source" if get_language() == 'en' else "æ¥æº"
            for i, item in enumerate(review_items, 1):
                print(f"\n[{i}] {item.get('title', 'N/A')}")
                print(f"    {cat_label}: {item.get('content_type')} | {conf_label}: {item.get('confidence', 0):.1%}")
                print(f"    {source_label}: {item.get('source', 'N/A')}")
        
        elif choice == '0':
            return
        else:
            log.warning(t('invalid_choice'))
    
    def _regenerate_after_review(self):
        """å®¡æ ¸åé‡æ–°ç”Ÿæˆåˆ†æå’ŒWebé¡µé¢"""
        print("\n" + "="*60)
        print(t('regenerate_title'))
        print("="*60)
        
        try:
            # æ­¥éª¤1: é‡æ–°åˆ†æ
            print("\n" + t('regenerate_step1'))
            self.trends = self.analyzer.analyze_trends(self.data)
            
            # æ­¥éª¤2: é‡æ–°ç”Ÿæˆå›¾è¡¨
            log.step(2, 3, t('regenerate_step2'))
            self.chart_files = self.visualizer.visualize_all(self.trends)
            
            # æ­¥éª¤3: é‡æ–°ç”ŸæˆWebé¡µé¢
            log.step(3, 3, t('regenerate_step3'))
            web_file = self.web_publisher.generate_html_page(self.data, self.trends, self.chart_files)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.analyzer.generate_report(self.data, self.trends)
            
            # ä¿å­˜ï¼ˆä½¿ç”¨reviewedæ ‡è®°ï¼‰
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            data_file = os.path.join(DATA_EXPORTS_DIR, f'ai_tracker_data_reviewed_{timestamp}.json')
            with open(data_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'metadata': {
                        'timestamp': timestamp,
                        'total_items': len(self.data),
                        'reviewed': True
                    },
                    'data': self.data,
                    'trends': self.trends
                }, f, ensure_ascii=False, indent=2)
            
            report_file = os.path.join(DATA_EXPORTS_DIR, f'ai_tracker_report_reviewed_{timestamp}.txt')
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            print("\n" + t('regenerate_done'))
            print("   " + t('regenerate_data', file=os.path.basename(data_file)))
            print("   " + t('regenerate_report', file=os.path.basename(report_file)))
            print("   " + t('regenerate_web', file=web_file))
            
            # è¯¢é—®æ˜¯å¦æ‰“å¼€
            import webbrowser
            open_prompt = "\nOpen updated web page in browser? (Y/N): " if get_language() == 'en' else "\næ˜¯å¦åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ›´æ–°åçš„Webé¡µé¢? (Y/N): "
            choice = input(open_prompt).strip().lower()
            if choice == 'y':
                webbrowser.open(f'file://{os.path.abspath(web_file)}')
                log.success(t('regenerate_opened'))
        
        except Exception as e:
            print("\n" + t('regenerate_failed', error=str(e)))
    
    def _learning_feedback(self):
        """å­¦ä¹ åé¦ˆåˆ†æ"""
        print("\n" + "="*60)
        print(t('learning_title'))
        print("="*60)
        
        # æŸ¥æ‰¾å®¡æ ¸å†å²æ–‡ä»¶å’Œå®¡æ ¸åæ•°æ®æ–‡ä»¶ï¼ˆéƒ½åœ¨ data/exports ç›®å½•ï¼‰
        review_pattern = os.path.join(DATA_EXPORTS_DIR, 'review_history_*.json')
        review_files = sorted(glob.glob(review_pattern), reverse=True)
        # ä» exports ç›®å½•æŸ¥æ‰¾å®¡æ ¸åçš„æ•°æ®æ–‡ä»¶
        data_pattern = os.path.join(DATA_EXPORTS_DIR, 'ai_tracker_data_reviewed_*.json')
        data_files = sorted(glob.glob(data_pattern), reverse=True)
        
        if not review_files:
            print("\n" + t('learning_no_history'))
            log.info(t('learning_do_review'), emoji="ğŸ’¡")
            return
        
        if not data_files:
            print("\n" + t('learning_no_data'))
            log.info(t('learning_do_save'), emoji="ğŸ’¡")
            return
        
        print(f"\n" + t('learning_found'))
        print("   " + t('learning_history_count', count=len(review_files)))
        print("   " + t('learning_data_count', count=len(data_files)))
        
        # æ˜¾ç¤ºæœ€è¿‘çš„æ–‡ä»¶
        print(f"\n" + t('learning_recent'))
        for i, (review_file, data_file) in enumerate(zip(review_files[:3], data_files[:3]), 1):
            print(f"   {i}. {review_file}")
        
        print("\n" + t('learning_options'))
        print("   " + t('learning_opt_1'))
        print("   " + t('learning_opt_2'))
        print("   " + t('learning_opt_0'))
        
        choice_prompt = "\nPlease select (0-2): " if get_language() == 'en' else "\nè¯·é€‰æ‹© (0-2): "
        choice = input(choice_prompt).strip()
        
        if choice == '1':
            # åˆ†ææœ€è¿‘ä¸€æ¬¡
            review_file = review_files[0]
            data_file = data_files[0]
            
            print(f"\n" + t('learning_analyzing', file=review_file))
            
            try:
                report_file = create_feedback_loop(
                    review_file,
                    data_file,
                    self.classifier
                )
                
                print(f"\n" + t('learning_done'))
                log.file(t('learning_report', file=report_file))
                
                # è¯¢é—®æ˜¯å¦æŸ¥çœ‹å»ºè®®
                view_prompt = "\nView improvement suggestions? (Y/N): " if get_language() == 'en' else "\næ˜¯å¦æŸ¥çœ‹æ”¹è¿›å»ºè®®? (Y/N): "
                view = input(view_prompt).strip().lower()
                if view == 'y':
                    self._show_improvement_suggestions(report_file)
                
            except Exception as e:
                print(f"\n" + t('learning_failed', error=str(e)))
        
        elif choice == '2':
            # é€‰æ‹©ç‰¹å®šæ–‡ä»¶
            print("\n" + t('learning_files'))
            for i, file in enumerate(review_files, 1):
                print(f"   {i}. {file}")
            
            try:
                file_prompt = "\nSelect file number: " if get_language() == 'en' else "\né€‰æ‹©æ–‡ä»¶ç¼–å·: "
                idx = int(input(file_prompt).strip()) - 1
                if 0 <= idx < len(review_files):
                    review_file = review_files[idx]
                    data_file = data_files[idx] if idx < len(data_files) else data_files[0]
                    
                    report_file = create_feedback_loop(
                        review_file,
                        data_file,
                        self.classifier
                    )
                    
                    print(f"\n" + t('learning_done') + " " + t('learning_report', file=report_file))
                else:
                    log.warning(t('invalid_choice'))
            except (ValueError, IndexError) as e:
                log.error(t('review_input_error') + f": {e}")
        
        elif choice == '0':
            return
        else:
            log.warning(t('invalid_choice'))
    
    def _show_improvement_suggestions(self, report_file: str):
        """æ˜¾ç¤ºæ”¹è¿›å»ºè®®"""
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            suggestions = report.get('improvement_suggestions', [])
            
            if not suggestions:
                print("\n" + t('learning_good'))
                return
            
            print("\n" + "="*70)
            log.info(t('learning_suggestions'), emoji="ğŸ’¡")
            print("="*70)
            
            for i, sug in enumerate(suggestions, 1):
                print(f"\n" + t('learning_sug_num', i=i))
                print("   " + t('learning_sug_type', type=sug.get('type')))
                
                if sug.get('category'):
                    print("   " + t('learning_sug_cat', cat=sug.get('category')))
                
                if sug.get('issue'):
                    print("   " + t('learning_sug_issue', issue=sug.get('issue')))
                
                if sug.get('suggestion'):
                    print("   " + t('learning_sug_suggestion', suggestion=sug.get('suggestion')))
                
                if sug.get('keywords'):
                    keywords_str = ', '.join(sug['keywords'])
                    print("   " + t('learning_sug_keywords', keywords=keywords_str))
                
                if sug.get('severity'):
                    print("   " + t('learning_sug_severity', severity=sug.get('severity')))
            
            print("\n" + "="*70)
            log.info(t('learning_note'), emoji="ğŸ“")
            print("   " + t('learning_note_1'))
            print("   " + t('learning_note_2'))
            print("="*70)
            
        except Exception as e:
            log.error(t('learning_read_error', error=str(e)))
    
    def _save_results(self, report: str, web_file: Optional[str] = None, timing_stats: Optional[Dict] = None):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æ„å»ºmetadata
        metadata = {
            'timestamp': timestamp,
            'total_items': len(self.data),
            'classification_mode': self.classification_mode
        }
        
        # æ·»åŠ è€—æ—¶ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
        if timing_stats:
            metadata['timing'] = timing_stats
        
        # å¦‚æœæ˜¯LLMæ¨¡å¼ï¼Œè®°å½•æ¨¡å‹ä¿¡æ¯
        if self.classification_mode == 'llm':
            metadata['llm_provider'] = self.llm_provider
            metadata['llm_model'] = self.llm_model
        
        # ä¿å­˜JSONæ•°æ®åˆ° exports ç›®å½•
        data_file = os.path.join(DATA_EXPORTS_DIR, f'ai_tracker_data_{timestamp}.json')
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': metadata,
                'data': self.data,
                'trends': self.trends
            }, f, ensure_ascii=False, indent=2)
        
        log.data(t('data_saved_to', file=os.path.basename(data_file)))
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Šåˆ° exports ç›®å½•
        report_file = os.path.join(DATA_EXPORTS_DIR, f'ai_tracker_report_{timestamp}.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        log.file(t('report_saved_to', file=os.path.basename(report_file)))
        
        if web_file:
            log.web(t('web_saved_to', file=web_file))


def main():
    """ä¸»å‡½æ•°"""
    tracker = None
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‡ªåŠ¨æ¨¡å¼
        auto_mode = '--auto' in sys.argv
        
        # è¯­è¨€è®¾ç½®ï¼šè‡ªåŠ¨æ¨¡å¼å¼ºåˆ¶è‹±æ–‡ï¼Œäº¤äº’æ¨¡å¼è®©ç”¨æˆ·é€‰æ‹©
        if auto_mode:
            set_language('en')
        else:
            select_language_interactive()
        
        tracker = AIWorldTracker(auto_mode=auto_mode)
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if '--auto' in sys.argv:
            # è‡ªåŠ¨è¿è¡Œå®Œæ•´æµç¨‹
            tracker.run_full_pipeline()
        elif '--help' in sys.argv:
            print(f"\n{t('app_title')} - {t('help_usage')}")
            print(f"\n{t('help_params')}")
            print(f"  --auto    {t('help_auto')}")
            print(f"  --help    {t('help_info')}")
            print(f"\n{t('help_no_params')}\n")
        else:
            # äº¤äº’å¼èœå•
            tracker.show_menu()
    except KeyboardInterrupt:
        # ç”¨æˆ·æŒ‰ Ctrl+C ä¸­æ–­
        print("\n")
        try:
            log.warning(t('user_interrupted'))
        except:
            print("âš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\n" + t('program_error', error=str(e)))
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        # ç¡®ä¿èµ„æºè¢«æ¸…ç†
        if tracker is not None:
            tracker.cleanup()


if __name__ == "__main__":
    main()

"""æµ‹è¯• LLM åˆ†ç±»å™¨çš„ç¼“å­˜åŠŸèƒ½"""
from llm_classifier import LLMClassifier
import time

# å¯ç”¨ç¼“å­˜
classifier = LLMClassifier(
    provider='ollama',
    model='qwen3:8b',
    enable_cache=True
)

test_item = {
    'title': 'OpenAI officially launches GPT-4o with new features',
    'summary': 'OpenAI announces the general availability of GPT-4o model',
    'source': 'TechCrunch'
}

print('=' * 50)
print('ğŸ”„ æµ‹è¯•ç¼“å­˜åŠŸèƒ½')
print('=' * 50)

# ç¬¬ä¸€æ¬¡è°ƒç”¨
print('\nğŸ“ ç¬¬ä¸€æ¬¡è°ƒç”¨ (æ— ç¼“å­˜)...')
start = time.time()
result1 = classifier.classify_item(test_item)
t1 = time.time() - start
print(f'   ç»“æœ: {result1.get("content_type")}')
print(f'   è€—æ—¶: {t1:.1f}s')
print(f'   ç¼“å­˜å‘½ä¸­: {result1.get("from_cache", False)}')

# ç¬¬äºŒæ¬¡è°ƒç”¨ (åº”è¯¥å‘½ä¸­ç¼“å­˜)
print('\nğŸ“ ç¬¬äºŒæ¬¡è°ƒç”¨ (åº”å‘½ä¸­ç¼“å­˜)...')
start = time.time()
result2 = classifier.classify_item(test_item)
t2 = time.time() - start
print(f'   ç»“æœ: {result2.get("content_type")}')
print(f'   è€—æ—¶: {t2:.4f}s')
print(f'   ç¼“å­˜å‘½ä¸­: {result2.get("from_cache", False)}')

# ç¬¬ä¸‰æ¬¡è°ƒç”¨ (éªŒè¯ç¼“å­˜)
print('\nğŸ“ ç¬¬ä¸‰æ¬¡è°ƒç”¨ (éªŒè¯ç¼“å­˜)...')
start = time.time()
result3 = classifier.classify_item(test_item)
t3 = time.time() - start
print(f'   ç»“æœ: {result3.get("content_type")}')
print(f'   è€—æ—¶: {t3:.4f}s')
print(f'   ç¼“å­˜å‘½ä¸­: {result3.get("from_cache", False)}')

# ç»Ÿè®¡
print('\n' + '=' * 50)
print('ğŸ“Š ç»Ÿè®¡')
print('=' * 50)
stats = classifier.get_stats()
print(f'   LLMè°ƒç”¨: {stats["llm_calls"]}')
print(f'   ç¼“å­˜å‘½ä¸­: {stats["cache_hits"]}')
print(f'   é€Ÿåº¦æå‡: {t1/t2:.0f}x (ç¬¬ä¸€æ¬¡ vs ç¬¬äºŒæ¬¡)')

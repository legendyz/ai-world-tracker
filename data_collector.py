"""
AIä¸–ç•Œè¿½è¸ªå™¨ - æ•°æ®é‡‡é›†æ¨¡å—
ä¸“æ³¨äºæ”¶é›†æœ€æ–°AIç ”ç©¶ã€äº§å“ã€å¼€å‘è€…ç¤¾åŒºå’Œè¡Œä¸šä¿¡æ¯

ä½¿ç”¨çº¯å¼‚æ­¥æ¨¡å¼ (asyncio + aiohttp) è¿›è¡Œé«˜æ€§èƒ½é‡‡é›†

ä½¿ç”¨æ–¹å¼:
    collector = DataCollector()
    data = collector.collect_all()
"""

import feedparser
import arxiv
import json
import os
import yaml
import random
import asyncio
import aiohttp
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
import time
import difflib
import hashlib
from urllib.parse import urlparse
from warnings import filterwarnings
from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning
from config import config
from logger import get_log_helper

# å¯¼å…¥å›½é™…åŒ–æ¨¡å—
try:
    from i18n import t, get_language
except ImportError:
    def t(key, **kwargs): return key
    def get_language(): return 'zh'

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('data_collector')

# åŠ è½½ç¼“å­˜ç›®å½•é…ç½®
def _get_cache_dir():
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    cache_dir = 'data/cache'
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                cache_dir = cfg.get('data', {}).get('cache_dir', cache_dir)
    except (OSError, yaml.YAMLError, KeyError) as e:
        # é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        pass
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir

DATA_CACHE_DIR = _get_cache_dir()

# ============== å¼‚æ­¥é‡‡é›†å™¨é…ç½® ==============

@dataclass
class AsyncCollectorConfig:
    """å¼‚æ­¥é‡‡é›†å™¨é…ç½®"""
    # å¹¶å‘æ§åˆ¶
    max_concurrent_requests: int = 20      # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    max_concurrent_per_host: int = 3       # æ¯ä¸ªä¸»æœºæœ€å¤§å¹¶å‘æ•°
    
    # è¶…æ—¶è®¾ç½®ï¼ˆç§’ï¼‰
    request_timeout: int = 15              # å•ä¸ªè¯·æ±‚è¶…æ—¶
    total_timeout: int = 120               # æ€»é‡‡é›†è¶…æ—¶
    
    # é‡è¯•è®¾ç½®
    max_retries: int = 2                   # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0               # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    # é€Ÿç‡é™åˆ¶
    rate_limit_delay: float = 0.2          # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    
    # æ•°æ®ç›®å½•
    cache_dir: str = 'data/cache'
    
    # ç¼“å­˜å¤§å°é™åˆ¶
    max_cache_size: int = 5000              # å†å²ç¼“å­˜æœ€å¤§æ¡ç›®æ•°

def _load_async_config() -> AsyncCollectorConfig:
    """ä» config.yaml åŠ è½½å¼‚æ­¥é‡‡é›†é…ç½®"""
    cfg = AsyncCollectorConfig()
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                yaml_cfg = yaml.safe_load(f)
                async_cfg = yaml_cfg.get('async_collector', {})
                
                cfg.max_concurrent_requests = async_cfg.get('max_concurrent_requests', cfg.max_concurrent_requests)
                cfg.max_concurrent_per_host = async_cfg.get('max_concurrent_per_host', cfg.max_concurrent_per_host)
                cfg.request_timeout = async_cfg.get('request_timeout', cfg.request_timeout)
                cfg.total_timeout = async_cfg.get('total_timeout', cfg.total_timeout)
                cfg.max_retries = async_cfg.get('max_retries', cfg.max_retries)
                cfg.cache_dir = yaml_cfg.get('data', {}).get('cache_dir', cfg.cache_dir)
    except (OSError, yaml.YAMLError, KeyError) as e:
        # é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        pass
    
    os.makedirs(cfg.cache_dir, exist_ok=True)
    return cfg

# RSSæºé…ç½® - ç»Ÿä¸€é…ç½®
RSS_FEEDS = {
    'research': [
        'http://export.arxiv.org/rss/cs.AI',
        'http://export.arxiv.org/rss/cs.CL',
        'http://export.arxiv.org/rss/cs.CV',
        'http://export.arxiv.org/rss/cs.LG',
    ],
    'news': [
        'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml',
        'https://techcrunch.com/category/artificial-intelligence/feed/',
        'https://www.wired.com/feed/tag/ai/latest/rss',
        'https://spectrum.ieee.org/rss/topic/artificial-intelligence',
        'https://www.technologyreview.com/feed/',
        'https://artificialintelligence-news.com/feed/',
        'https://syncedreview.com/feed/',
        'https://www.36kr.com/feed',
        'https://www.ithome.com/rss/',
        'https://www.jiqizhixin.com/rss',
        'https://www.qbitai.com/feed',
        'https://www.infoq.cn/feed/topic/18',
    ],
    'developer': [
        'https://github.blog/feed/',
        'https://huggingface.co/blog/feed.xml',
        'https://openai.com/blog/rss.xml',
        'https://blog.google/technology/ai/rss/',
    ],
    'product_news': [
        # ç¾å›½ç§‘æŠ€å·¨å¤´
        'https://openai.com/blog/rss.xml',
        'https://blog.google/technology/ai/rss/',
        'https://blogs.microsoft.com/ai/feed/',
        'https://ai.meta.com/blog/rss/',
        'https://www.anthropic.com/news/rss',
        # ä¸­å›½ç§‘æŠ€å…¬å¸ (via 36kr/æœºå™¨ä¹‹å¿ƒç­‰)
        'https://www.jiqizhixin.com/rss',
        'https://www.qbitai.com/feed',
    ],
    'community': [
        'https://www.producthunt.com/feed?category=artificial-intelligence',
    ],
    'leader_blogs': [
        {'url': 'http://blog.samaltman.com/posts.atom', 'author': 'Sam Altman', 'title': 'OpenAI CEO'},
        {'url': 'https://karpathy.github.io/feed.xml', 'author': 'Andrej Karpathy', 'title': 'AI Researcher'},
        {'url': 'https://lexfridman.com/feed/podcast/', 'author': 'Lex Fridman', 'title': 'Podcast Host', 'type': 'podcast'},
    ]
}

class AIDataCollector:
    """AIæ•°æ®é‡‡é›†å™¨ - æ”¶é›†çœŸå®æœ€æ–°çš„AIä¿¡æ¯
    
    ä½¿ç”¨çº¯å¼‚æ­¥æ¨¡å¼ (asyncio + aiohttp) è¿›è¡Œé«˜æ€§èƒ½é‡‡é›†
    
    æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨ç”¨æ³•:
        async with AIDataCollector() as collector:
            data = await collector._collect_all_async()
    """
    
    def __init__(self):
        # å¼‚æ­¥é…ç½®
        self.async_config = _load_async_config()
        log.config("ğŸ“¡ Collector mode: Async (aiohttp)")
        
        # æ•°æ®é‡‡é›†æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰- ä»é…ç½®è¯»å–
        self.data_retention_days = config.collector.data_retention_days
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ä½¿ç”¨ç»Ÿä¸€çš„RSSæºé…ç½®
        self.rss_feeds = RSS_FEEDS
        
        # é‡‡é›†å†å²ç¼“å­˜
        self.history_cache_file = os.path.join(DATA_CACHE_DIR, 'collection_history_cache.json')
        self.history_cache = self._load_history_cache()
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåŒæ­¥å’Œå¼‚æ­¥æ¨¡å¼ï¼‰
        self.stats = {
            'requests_made': 0,
            'requests_failed': 0,
            'items_collected': 0,
            'start_time': None,
            'end_time': None,
            'failed_sources': []  # å¤±è´¥çš„æ•°æ®æºåˆ—è¡¨: [{'source': 'xxx', 'category': 'xxx', 'error': 'xxx'}]
        }
        
        # å¼‚æ­¥sessionï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._session: Optional[aiohttp.ClientSession] = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        await self._close_session()
        return False
    
    async def _ensure_session(self):
        """ç¡®ä¿sessionå·²åˆ›å»º"""
        if self._session is None or self._session.closed:
            connector = aiohttp.TCPConnector(
                limit=self.async_config.max_concurrent_requests,
                limit_per_host=self.async_config.max_concurrent_per_host,
                ttl_dns_cache=300
            )
            timeout = aiohttp.ClientTimeout(
                total=self.async_config.total_timeout,
                connect=self.async_config.request_timeout,
                sock_read=self.async_config.request_timeout
            )
            self._session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers=self.headers
            )
    
    async def _close_session(self):
        """å…³é—­å¼‚æ­¥session"""
        if self._session and not self._session.closed:
            await self._session.close()
            self._session = None
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        if self._session and not self._session.closed:
            # åœ¨åŒæ­¥ææ„ä¸­æ— æ³•è°ƒç”¨å¼‚æ­¥closeï¼Œè®°å½•è­¦å‘Š
            log.warning("AIDataCollector session not properly closed")
    
    def _reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'requests_made': 0,
            'requests_failed': 0,
            'items_collected': 0,
            'start_time': None,
            'end_time': None,
            'failed_sources': []
        }
    
    def _record_failure(self, source: str, category: str, error: str):
        """è®°å½•é‡‡é›†å¤±è´¥çš„æ•°æ®æº
        
        Args:
            source: æ•°æ®æºåç§°æˆ–URL
            category: æ•°æ®ç±»åˆ« (research/developer/product/news/leader/community)
            error: é”™è¯¯ä¿¡æ¯
        """
        self.stats['requests_failed'] += 1
        self.stats['failed_sources'].append({
            'source': source[:80] if len(source) > 80 else source,  # æˆªæ–­è¿‡é•¿URL
            'category': category,
            'error': str(error)[:100]  # æˆªæ–­è¿‡é•¿é”™è¯¯ä¿¡æ¯
        })
    
    def _print_failed_sources_summary(self):
        """æ‰“å°å¤±è´¥æ•°æ®æºæ±‡æ€»"""
        failed = self.stats.get('failed_sources', [])
        if not failed:
            return
        
        # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
        by_category = {}
        for f in failed:
            cat = f['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(f)
        
        # åŒè¾“å‡ºæ¨¡å¼æ˜¾ç¤ºå¤±è´¥æ±‡æ€»
        log.dual_warning(t('dc_failed_sources_title', count=len(failed)))
        
        for cat, failures in by_category.items():
            log.dual_info(f"  [{cat}] {len(failures)} å¤±è´¥:", emoji="")
            for f in failures[:3]:  # æ¯ç±»åˆ«æœ€å¤šæ˜¾ç¤º3ä¸ª
                source_short = f['source'][:50] + '...' if len(f['source']) > 50 else f['source']
                log.dual_info(f"    â€¢ {source_short}", emoji="")
            if len(failures) > 3:
                log.dual_info(f"    ... åŠå…¶ä»– {len(failures) - 3} ä¸ª", emoji="")
    
    def _load_history_cache(self) -> Dict:
        """åŠ è½½é‡‡é›†å†å²ç¼“å­˜ï¼ˆæ”¯æŒURLã€æ ‡é¢˜ã€è§„èŒƒåŒ–æ ‡é¢˜ï¼‰"""
        try:
            if os.path.exists(self.history_cache_file):
                with open(self.history_cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                    # éªŒè¯ç¼“å­˜æ ¼å¼
                    if isinstance(cache, dict) and 'urls' in cache and 'titles' in cache:
                        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡7å¤©ï¼‰
                        last_updated = cache.get('last_updated', '')
                        if last_updated:
                            try:
                                last_time = datetime.fromisoformat(last_updated)
                                if (datetime.now() - last_time).days > 7:
                                    log.warning(t('dc_cache_expired'))
                                    return {'urls': set(), 'titles': set(), 'normalized_titles': set(), 'last_updated': ''}
                            except (ValueError, TypeError):
                                pass
                        # è½¬æ¢ä¸º set ä»¥åŠ é€ŸæŸ¥æ‰¾ï¼ŒåŒæ—¶è§„èŒƒåŒ–URL
                        cache['urls'] = set(self._normalize_url(url) for url in cache['urls'])
                        cache['titles'] = set(cache['titles'])
                        # åŠ è½½è§„èŒƒåŒ–æ ‡é¢˜ï¼ˆæ–°å­—æ®µï¼Œå…¼å®¹æ—§ç¼“å­˜ï¼‰
                        cache['normalized_titles'] = set(cache.get('normalized_titles', []))
                        
                        # å¦‚æœæ˜¯æ—§ç¼“å­˜ï¼ˆæ²¡æœ‰normalized_titlesï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆ
                        if not cache['normalized_titles'] and cache['titles']:
                            cache['normalized_titles'] = set(
                                self._normalize_title_for_cache(t) for t in cache['titles'] if t
                            )
                            log.file_only(f"è‡ªåŠ¨ç”Ÿæˆè§„èŒƒåŒ–æ ‡é¢˜ç¼“å­˜: {len(cache['normalized_titles'])} æ¡")
                        
                        log.data(t('dc_cache_loaded', url_count=len(cache['urls']), title_count=len(cache['titles'])))
                        return cache
        except Exception as e:
            log.error(t('dc_cache_load_failed', error=str(e)))
        return {'urls': set(), 'titles': set(), 'normalized_titles': set(), 'last_updated': ''}
    
    def _save_history_cache(self):
        """ä¿å­˜é‡‡é›†å†å²ç¼“å­˜"""
        try:
            # è½¬æ¢ set ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
            cache_to_save = {
                'urls': list(self.history_cache['urls']),
                'titles': list(self.history_cache['titles']),
                'normalized_titles': list(self.history_cache.get('normalized_titles', set())),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.history_cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_to_save, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(t('dc_cache_save_failed', error=str(e)))
    
    def _is_in_history(self, item: Dict) -> bool:
        """
        æ£€æŸ¥é¡¹ç›®æ˜¯å¦åœ¨å†å²ç¼“å­˜ä¸­
        
        åŒ¹é…ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1. URLè§„èŒƒåŒ–åŒ¹é…ï¼ˆå¤„ç†å°¾éƒ¨æ–œæ ã€è·Ÿè¸ªå‚æ•°ç­‰ï¼‰
        2. æ ‡é¢˜ç²¾ç¡®åŒ¹é…
        3. è§„èŒƒåŒ–æ ‡é¢˜åŒ¹é…ï¼ˆç”¨äºå¤„ç†æ ‡é¢˜å¾®å°å˜åŒ–ï¼‰
        
        å¯¹äºä¸ç¨³å®šURLæºï¼ˆå¦‚Google Newsï¼‰ï¼Œä¸»è¦ä¾èµ–æ ‡é¢˜åŒ¹é…
        """
        url = item.get('url', '')
        title = item.get('title', '')
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºä¸ç¨³å®šURLæºï¼ˆè¿™äº›æºçš„URLå¯èƒ½æ¯æ¬¡éƒ½ä¸åŒï¼‰
        unstable_url_sources = [
            'news.google.com/rss/articles/',  # Google Newsé‡å®šå‘URL
            'feedburner.com',
            '/redirect/',
        ]
        is_unstable_url = url and any(s in url for s in unstable_url_sources)
        
        # ç­–ç•¥1: URLè§„èŒƒåŒ–åŒ¹é…ï¼ˆå¯¹äºç¨³å®šURLæºä¼˜å…ˆä½¿ç”¨ï¼‰
        if url and not is_unstable_url:
            normalized_url = self._normalize_url(url)
            if normalized_url in self.history_cache['urls']:
                return True
        
        # ç­–ç•¥2: æ ‡é¢˜ç²¾ç¡®åŒ¹é…
        if title and title in self.history_cache['titles']:
            return True
        
        # ç­–ç•¥3: è§„èŒƒåŒ–æ ‡é¢˜åŒ¹é…ï¼ˆå¤„ç†æ ‡é¢˜å¾®å°å˜åŒ–ï¼‰
        if title:
            normalized_title = self._normalize_title_for_cache(title)
            if normalized_title and normalized_title in self.history_cache.get('normalized_titles', set()):
                return True
        
        return False
    
    def _normalize_title_for_cache(self, title: str) -> str:
        """
        ä¸ºç¼“å­˜ç›®çš„è§„èŒƒåŒ–æ ‡é¢˜
        
        å¤„ç†è§„åˆ™ï¼š
        1. å°å†™åŒ–
        2. ç§»é™¤æ¥æºåç¼€ï¼ˆå¦‚ " - TechCrunch"ï¼‰
        3. ç§»é™¤æ ‡ç‚¹ç¬¦å·
        4. ç§»é™¤å¤šä½™ç©ºæ ¼
        5. åªä¿ç•™å‰60ä¸ªå­—ç¬¦ï¼ˆé¿å…æ ‡é¢˜æˆªæ–­å¯¼è‡´çš„å·®å¼‚ï¼‰
        
        Args:
            title: åŸå§‹æ ‡é¢˜
            
        Returns:
            è§„èŒƒåŒ–åçš„æ ‡é¢˜
        """
        import re
        if not title:
            return ''
        
        # å°å†™åŒ–
        normalized = title.lower()
        
        # ç§»é™¤æ¥æºåç¼€ (- Source, | Source, â€” Source)
        normalized = re.sub(r'\s*[-|â€”]\s*[a-z][a-z\s&.\']+$', '', normalized)
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ï¼‰
        normalized = re.sub(r'[^\w\s]', ' ', normalized)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        normalized = ' '.join(normalized.split())
        
        # æˆªå–å‰60å­—ç¬¦ï¼ˆé¿å…æ ‡é¢˜æœ«å°¾å·®å¼‚ï¼‰
        normalized = normalized[:60].strip()
        
        return normalized
    
    def _add_to_history(self, item: Dict):
        """
        å°†é¡¹ç›®æ·»åŠ åˆ°å†å²ç¼“å­˜ï¼ˆå¸¦å¤§å°é™åˆ¶ï¼‰
        
        ç¼“å­˜å†…å®¹ï¼š
        1. è§„èŒƒåŒ–URL
        2. åŸå§‹æ ‡é¢˜
        3. è§„èŒƒåŒ–æ ‡é¢˜ï¼ˆç”¨äºæ¨¡ç³ŠåŒ¹é…ï¼‰
        """
        url = item.get('url', '')
        title = item.get('title', '')
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œè¶…å‡ºé™åˆ¶æ—¶æ¸…ç†æ—§æ¡ç›®
        max_size = self.async_config.max_cache_size
        
        # æ·»åŠ è§„èŒƒåŒ–URL
        if url:
            normalized_url = self._normalize_url(url)
            if len(self.history_cache['urls']) >= max_size:
                urls_list = list(self.history_cache['urls'])
                remove_count = max_size // 5  # ç§»é™¤20%
                self.history_cache['urls'] = set(urls_list[remove_count:])
                log.file_only(f"ç¼“å­˜æ¸…ç†: URLs {len(urls_list)} â†’ {len(self.history_cache['urls'])}")
            self.history_cache['urls'].add(normalized_url)
        
        # æ·»åŠ åŸå§‹æ ‡é¢˜
        if title:
            if len(self.history_cache['titles']) >= max_size:
                titles_list = list(self.history_cache['titles'])
                remove_count = max_size // 5
                self.history_cache['titles'] = set(titles_list[remove_count:])
                log.file_only(f"ç¼“å­˜æ¸…ç†: Titles {len(titles_list)} â†’ {len(self.history_cache['titles'])}")
            self.history_cache['titles'].add(title)
            
            # æ·»åŠ è§„èŒƒåŒ–æ ‡é¢˜ï¼ˆæ–°å¢ï¼‰
            normalized_title = self._normalize_title_for_cache(title)
            if normalized_title:
                if 'normalized_titles' not in self.history_cache:
                    self.history_cache['normalized_titles'] = set()
                if len(self.history_cache['normalized_titles']) >= max_size:
                    nt_list = list(self.history_cache['normalized_titles'])
                    remove_count = max_size // 5
                    self.history_cache['normalized_titles'] = set(nt_list[remove_count:])
                self.history_cache['normalized_titles'].add(normalized_title)
    
    def _filter_by_history(self, all_data: Dict[str, List[Dict]], 
                           filter_enabled: bool = True) -> Tuple[Dict[str, List[Dict]], Dict[str, int], Dict[str, int]]:
        """
        å†å²ç¼“å­˜æœ€ç»ˆè¿‡æ»¤ä¸ç¼“å­˜æ›´æ–°
        
        èŒè´£è¯´æ˜ï¼š
        1. äºŒæ¬¡è¿‡æ»¤ï¼šé‡‡é›†é˜¶æ®µçš„URLé¢„è¿‡æ»¤å¯èƒ½æœ‰é—æ¼ï¼ˆå¦‚è·¨ç±»åˆ«é‡å¤ï¼‰ï¼Œæ­¤å¤„åšæœ€ç»ˆæ¸…ç†
        2. ç¼“å­˜æ›´æ–°ï¼šå°†æ–°é‡‡é›†çš„é¡¹ç›®æ·»åŠ åˆ°å†å²ç¼“å­˜ï¼Œä¾›ä¸‹æ¬¡é‡‡é›†æ—¶é¢„è¿‡æ»¤ä½¿ç”¨
        3. ç»Ÿè®¡è¾“å‡ºï¼šç»Ÿè®¡å„ç±»åˆ«çš„æ–°å†…å®¹ä¸ç¼“å­˜å‘½ä¸­æ•°é‡
        
        ä¸é¢„è¿‡æ»¤çš„åŒºåˆ«ï¼š
        - é¢„è¿‡æ»¤ï¼ˆé‡‡é›†é˜¶æ®µï¼‰ï¼šåœ¨ç½‘ç»œè¯·æ±‚å‰å¿«é€Ÿè·³è¿‡å·²çŸ¥URLï¼Œå‡å°‘æ— æ•ˆè¯·æ±‚
        - æœ¬æ–¹æ³•ï¼ˆé‡‡é›†åï¼‰ï¼šç¡®ä¿æœ€ç»ˆæ•°æ®æ— é‡å¤ï¼Œå¹¶æ›´æ–°æŒä¹…åŒ–ç¼“å­˜
        
        Args:
            all_data: æŒ‰ç±»åˆ«åˆ†ç»„çš„æ•°æ®å­—å…¸
            filter_enabled: æ˜¯å¦å¯ç”¨è¿‡æ»¤ï¼ˆFalseåˆ™åªç»Ÿè®¡ä¸è¿‡æ»¤ï¼‰
            
        Returns:
            Tuple[filtered_data, new_stats, cached_stats]
            - filtered_data: è¿‡æ»¤åçš„æ•°æ®ï¼ˆæˆ–åŸæ•°æ®ï¼Œå–å†³äºfilter_enabledï¼‰
            - new_stats: æ¯ä¸ªç±»åˆ«çš„æ–°å†…å®¹æ•°é‡
            - cached_stats: æ¯ä¸ªç±»åˆ«çš„ç¼“å­˜å‘½ä¸­æ•°é‡
        """
        new_stats = {}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„æ–°å†…å®¹æ•°é‡
        cached_stats = {}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç¼“å­˜å‘½ä¸­æ•°é‡
        new_items_for_cache = []  # è®°å½•æ–°é‡‡é›†çš„é¡¹ç›®ï¼ˆå¾…åŠ å…¥ç¼“å­˜ï¼‰
        
        if filter_enabled:
            # è¿‡æ»¤æ¨¡å¼ï¼šç§»é™¤å†å²ä¸­å·²æœ‰çš„é¡¹ç›®
            filtered_data = {}
            for cat in all_data:
                new_items = []
                cached_count = 0
                for item in all_data[cat]:
                    if self._is_in_history(item):
                        cached_count += 1
                    else:
                        new_items.append(item)
                        new_items_for_cache.append(item)
                filtered_data[cat] = new_items
                new_stats[cat] = len(new_items)
                cached_stats[cat] = cached_count
        else:
            # ç»Ÿè®¡æ¨¡å¼ï¼šåªç»Ÿè®¡ï¼Œä¸è¿‡æ»¤
            filtered_data = all_data
            for cat in all_data:
                new_count = 0
                cached_count = 0
                for item in all_data[cat]:
                    if self._is_in_history(item):
                        cached_count += 1
                    else:
                        new_count += 1
                        new_items_for_cache.append(item)
                new_stats[cat] = new_count
                cached_stats[cat] = cached_count
        
        # å°†æ–°é‡‡é›†çš„é¡¹ç›®æ·»åŠ åˆ°å†å²ç¼“å­˜
        for item in new_items_for_cache:
            self._add_to_history(item)
        
        # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
        if new_items_for_cache:
            self._save_history_cache()
        
        return filtered_data, new_stats, cached_stats
    
    def clear_history_cache(self):
        """æ¸…é™¤é‡‡é›†å†å²ç¼“å­˜"""
        self.history_cache = {'urls': set(), 'titles': set(), 'normalized_titles': set(), 'last_updated': ''}
        if os.path.exists(self.history_cache_file):
            os.remove(self.history_cache_file)
        log.success(t('dc_cache_cleared'))

    def collect_all(self) -> Dict[str, List[Dict]]:
        """
        é‡‡é›†æ‰€æœ‰ç±»å‹çš„æ•°æ®ï¼ˆçº¯å¼‚æ­¥æ¨¡å¼ï¼‰
        
        Returns:
            åˆ†ç±»çš„æ•°æ®å­—å…¸
        """
        try:
            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥é‡‡é›†
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self._collect_all_async())
            finally:
                loop.close()
        except Exception as e:
            log.error(f"Async collection failed: {e}")
            raise
    
    # ============== è¯­ä¹‰å»é‡ç›¸å…³æ–¹æ³• ==============
    
    # è‹±æ–‡åœç”¨è¯ï¼ˆç”¨äºå…³é”®è¯æå–ï¼‰
    _STOPWORDS = frozenset({
        'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'but',
        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
        'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
        'that', 'this', 'these', 'those', 'with', 'from', 'by', 'as', 'into',
        'through', 'during', 'before', 'after', 'above', 'below', 'between',
        'says', 'said', 'predicts', 'predicted', 'tells', 'told', 'according',
        'thanks', 'about', 'over', 'under', 'again', 'further', 'then', 'once',
        'here', 'there', 'when', 'where', 'why', 'how', 'all', 'each', 'every',
        'both', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'not',
        'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just', 'also',
        'now', 'new', 'first', 'last', 'long', 'great', 'little', 'own', 'make',
        'can', 'like', 'back', 'even', 'well', 'way', 'our', 'out', 'its', 'it',
        'up', 'go', 'going', 'get', 'getting', 'come', 'coming', 'become', 'becoming'
    })
    
    def _normalize_url(self, url: str) -> str:
        """
        å½’ä¸€åŒ–URLï¼šç»Ÿä¸€æ ¼å¼ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡
        
        å¤„ç†è§„åˆ™:
        1. ç§»é™¤å°¾éƒ¨æ–œæ 
        2. è½¬æ¢ä¸ºå°å†™ï¼ˆschemeå’Œhostéƒ¨åˆ†ï¼‰
        3. ç§»é™¤å¸¸è§è·Ÿè¸ªå‚æ•°
        4. ç»Ÿä¸€åè®®ï¼ˆå¯é€‰ï¼‰
        
        Args:
            url: åŸå§‹URL
            
        Returns:
            å½’ä¸€åŒ–åçš„URL
        """
        if not url:
            return ''
        
        from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
        
        try:
            # è§£æURL
            parsed = urlparse(url)
            
            # å°å†™åŒ–schemeå’Œnetloc
            scheme = parsed.scheme.lower()
            netloc = parsed.netloc.lower()
            
            # ç§»é™¤å°¾éƒ¨æ–œæ ï¼ˆè·¯å¾„éƒ¨åˆ†ï¼‰
            path = parsed.path.rstrip('/')
            
            # ç§»é™¤å¸¸è§è·Ÿè¸ªå‚æ•°
            tracking_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_content', 
                               'utm_term', 'ref', 'source', 'fbclid', 'gclid', 'ocid'}
            if parsed.query:
                params = parse_qs(parsed.query, keep_blank_values=True)
                # è¿‡æ»¤æ‰è·Ÿè¸ªå‚æ•°
                filtered_params = {k: v for k, v in params.items() 
                                   if k.lower() not in tracking_params}
                query = urlencode(filtered_params, doseq=True) if filtered_params else ''
            else:
                query = ''
            
            # é‡å»ºURL
            normalized = urlunparse((scheme, netloc, path, parsed.params, query, ''))
            return normalized
            
        except Exception:
            # è§£æå¤±è´¥æ—¶è¿”å›å»é™¤å°¾éƒ¨æ–œæ çš„åŸå§‹URL
            return url.rstrip('/')
    
    def _normalize_title(self, title: str) -> str:
        """
        å½’ä¸€åŒ–æ ‡é¢˜ï¼šå»é™¤æ¥æºåç¼€ã€æ ‡ç‚¹ã€å¤šä½™è¯æ±‡
        
        Args:
            title: åŸå§‹æ ‡é¢˜
            
        Returns:
            å½’ä¸€åŒ–åçš„æ ‡é¢˜
        """
        import re
        if not title:
            return ''
        
        # å»é™¤æ¥æºåç¼€ (- Source Name, | Source, â€” Source)
        # åŒ¹é…æ¨¡å¼: " - Fox Business", " | Reuters", " â€” The Guardian"
        title = re.sub(r'\s*[-|â€”]\s*[A-Z][a-zA-Z\s&.\']+$', '', title)
        
        # å°å†™
        title = title.lower()
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ï¼‰
        title = re.sub(r'[^\w\s]', ' ', title)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        title = ' '.join(title.split())
        
        return title
    
    def _extract_keywords(self, title: str) -> set:
        """
        æå–æ ‡é¢˜å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
        
        Args:
            title: åŸå§‹æ ‡é¢˜
            
        Returns:
            å…³é”®è¯é›†åˆ
        """
        normalized = self._normalize_title(title)
        words = normalized.split()
        # è¿‡æ»¤åœç”¨è¯å’Œè¿‡çŸ­çš„è¯ï¼ˆ<3å­—ç¬¦ï¼‰
        return {w for w in words if len(w) >= 3 and w not in self._STOPWORDS}
    
    def _semantic_similarity(self, title1: str, title2: str) -> tuple:
        """
        è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        
        é‡‡ç”¨åŒé‡ç­–ç•¥:
        1. å…³é”®è¯Jaccardç›¸ä¼¼åº¦ï¼ˆè¯­ä¹‰å±‚é¢ï¼‰
        2. å½’ä¸€åŒ–å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆå­—é¢å±‚é¢ï¼‰
        
        Args:
            title1: ç¬¬ä¸€ä¸ªæ ‡é¢˜
            title2: ç¬¬äºŒä¸ªæ ‡é¢˜
            
        Returns:
            (jaccard_sim, string_sim, common_keywords)
        """
        # æå–å…³é”®è¯
        kw1 = self._extract_keywords(title1)
        kw2 = self._extract_keywords(title2)
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(kw1 & kw2)
        union = len(kw1 | kw2)
        jaccard_sim = intersection / union if union > 0 else 0
        
        # å½’ä¸€åŒ–å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
        norm1 = self._normalize_title(title1)
        norm2 = self._normalize_title(title2)
        string_sim = difflib.SequenceMatcher(None, norm1, norm2).ratio()
        
        return (jaccard_sim, string_sim, kw1 & kw2)
    
    def _is_semantic_duplicate(self, title1: str, title2: str, 
                                jaccard_threshold: float = 0.35,
                                string_threshold: float = 0.50,
                                min_common_keywords: int = 3) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦ä¸ºè¯­ä¹‰é‡å¤
        
        åˆ¤å®šè§„åˆ™ï¼ˆæ»¡è¶³ä»»ä¸€æ¡ä»¶å³ä¸ºé‡å¤ï¼‰:
        1. å…³é”®è¯Jaccard >= 0.35 ä¸” å…±åŒå…³é”®è¯ >= 3
        2. å½’ä¸€åŒ–å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ >= 0.50
        3. å…³é”®è¯Jaccard >= 0.50ï¼ˆå³ä½¿å…±åŒè¯å°‘äº3ä¸ªï¼‰
        
        Args:
            title1: ç¬¬ä¸€ä¸ªæ ‡é¢˜
            title2: ç¬¬äºŒä¸ªæ ‡é¢˜
            jaccard_threshold: Jaccardç›¸ä¼¼åº¦é˜ˆå€¼
            string_threshold: å­—ç¬¦ä¸²ç›¸ä¼¼åº¦é˜ˆå€¼
            min_common_keywords: æœ€å°å…±åŒå…³é”®è¯æ•°
            
        Returns:
            æ˜¯å¦ä¸ºé‡å¤å†…å®¹
        """
        jaccard_sim, string_sim, common_kw = self._semantic_similarity(title1, title2)
        
        # è§„åˆ™1: Jaccard >= 0.35 ä¸” å…±åŒå…³é”®è¯ >= 3
        if jaccard_sim >= jaccard_threshold and len(common_kw) >= min_common_keywords:
            return True
        
        # è§„åˆ™2: å½’ä¸€åŒ–å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ >= 0.50
        if string_sim >= string_threshold:
            return True
        
        # è§„åˆ™3: é«˜Jaccardï¼ˆ>= 0.50ï¼‰å³ä½¿å…±åŒè¯å°‘
        if jaccard_sim >= 0.50:
            return True
        
        return False
    
    def _generate_item_fingerprint(self, item: Dict) -> str:
        """
        ç”Ÿæˆå†…å®¹æŒ‡çº¹ç”¨äºå¿«é€Ÿå»é‡
        
        åŸºäº URL + æ ‡é¢˜å‰50å­—ç¬¦ ç”Ÿæˆ MD5 å“ˆå¸Œ
        
        Args:
            item: æ•°æ®é¡¹å­—å…¸
            
        Returns:
            MD5 å“ˆå¸Œå­—ç¬¦ä¸²
        """
        url = item.get('url', '')
        title = item.get('title', '')[:50]  # å–æ ‡é¢˜å‰50å­—ç¬¦
        key = f"{url}|{title}".lower()
        return hashlib.md5(key.encode('utf-8')).hexdigest()
    
    def _deduplicate_by_fingerprint(self, items: List[Dict]) -> List[Dict]:
        """
        åŸºäºæŒ‡çº¹çš„å¿«é€Ÿå»é‡ (O(n) å¤æ‚åº¦)
        
        Args:
            items: æ•°æ®é¡¹åˆ—è¡¨
            
        Returns:
            å»é‡åçš„åˆ—è¡¨
        """
        if not items:
            return []
        
        seen_fingerprints = set()
        unique_items = []
        
        for item in items:
            fp = self._generate_item_fingerprint(item)
            if fp not in seen_fingerprints:
                seen_fingerprints.add(fp)
                unique_items.append(item)
        
        return unique_items
    
    def _deduplicate_items(self, items: List[Dict], threshold: float = 0.6) -> List[Dict]:
        """
        å¯¹å†…å®¹åˆ—è¡¨è¿›è¡Œå»é‡ï¼ˆä¸‰é˜¶æ®µç­–ç•¥ï¼‰
        
        é˜¶æ®µ1: åŸºäºæŒ‡çº¹å¿«é€Ÿå»é‡ (O(n)) - å®Œå…¨ç›¸åŒçš„URL+æ ‡é¢˜
        é˜¶æ®µ2: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å»é‡ - å¤„ç†åŒä¸€äº‹ä»¶ä¸åŒæ¥æºçš„æŠ¥é“
        é˜¶æ®µ3: åŸºäºä¼ ç»Ÿå­—ç¬¦ä¸²ç›¸ä¼¼åº¦å»é‡ - å…œåº•
        
        Args:
            items: æ•°æ®é¡¹åˆ—è¡¨
            threshold: ä¼ ç»Ÿå­—ç¬¦ä¸²ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå…œåº•ç”¨ï¼‰
            
        Returns:
            å»é‡åçš„åˆ—è¡¨
        """
        if not items:
            return []
        
        # é˜¶æ®µ1: æŒ‡çº¹å¿«é€Ÿå»é‡
        items = self._deduplicate_by_fingerprint(items)
        
        # é˜¶æ®µ2+3: è¯­ä¹‰ç›¸ä¼¼åº¦ç²¾ç»†å»é‡
        unique_items = []
        removed_as_duplicate = []  # è®°å½•è¢«å»é‡çš„æ ‡é¢˜ï¼ˆè°ƒè¯•ç”¨ï¼‰
        
        for item in items:
            is_duplicate = False
            item_title = item.get('title', '')
            
            for existing in unique_items:
                existing_title = existing.get('title', '')
                
                # ä½¿ç”¨è¯­ä¹‰å»é‡åˆ¤æ–­
                if self._is_semantic_duplicate(item_title, existing_title):
                    is_duplicate = True
                    removed_as_duplicate.append((item_title[:50], existing_title[:50]))
                    break
            
            if not is_duplicate:
                unique_items.append(item)
        
        # è®°å½•è¯­ä¹‰å»é‡ç»“æœï¼ˆä»…å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰
        if removed_as_duplicate and len(removed_as_duplicate) > 0:
            log.file_only(f"è¯­ä¹‰å»é‡ç§»é™¤ {len(removed_as_duplicate)} æ¡ç›¸ä¼¼å†…å®¹")
            for new_t, old_t in removed_as_duplicate[:3]:  # åªæ˜¾ç¤ºå‰3æ¡
                log.file_only(f"  - '{new_t}...' ä¸ '{old_t}...' ç›¸ä¼¼")
                
        return unique_items
    
    def _apply_deduplication(self, all_data: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ç»Ÿä¸€å»é‡å…¥å£ - å¯¹æ‰€æœ‰ç±»åˆ«çš„æ•°æ®åº”ç”¨å»é‡å¤„ç†
        
        å¤„ç†æµç¨‹:
        1. å¯¹æ¯ä¸ªç±»åˆ«å†…éƒ¨è¿›è¡Œå»é‡
        2. è·¨ç±»åˆ«å»é‡ï¼ˆåŒä¸€URLå¯èƒ½å‡ºç°åœ¨å¤šä¸ªç±»åˆ«ä¸­ï¼‰
        
        Args:
            all_data: æŒ‰ç±»åˆ«åˆ†ç»„çš„æ•°æ®å­—å…¸
            
        Returns:
            å»é‡åçš„æ•°æ®å­—å…¸
        """
        # ç»Ÿè®¡å»é‡å‰æ•°é‡
        before_count = sum(len(items) for items in all_data.values())
        
        # é˜¶æ®µ1: ç±»åˆ«å†…å»é‡
        for cat in all_data:
            all_data[cat] = self._deduplicate_items(all_data[cat])
        
        # é˜¶æ®µ2: è·¨ç±»åˆ«å»é‡ï¼ˆåŸºäºURLï¼‰
        seen_urls = set()
        for cat in all_data:
            unique_items = []
            for item in all_data[cat]:
                url = item.get('url', '')
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    unique_items.append(item)
                elif not url:
                    # æ²¡æœ‰URLçš„é¡¹ç›®ä¿ç•™
                    unique_items.append(item)
            all_data[cat] = unique_items
        
        # ç»Ÿè®¡å»é‡åæ•°é‡
        after_count = sum(len(items) for items in all_data.values())
        removed_count = before_count - after_count
        
        if removed_count > 0:
            log.dual_info(f"ğŸ”„ å»é‡å®Œæˆ: {before_count} â†’ {after_count} (ç§»é™¤ {removed_count} æ¡é‡å¤)", emoji="")
        
        return all_data

    def _is_ai_related(self, item: Dict) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸AIç›¸å…³"""
        ai_keywords = [
            'ai', 'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'llm', 'gpt', 'transformer', 'chatgpt',
            'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ'
        ]
        
        text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
        return any(keyword in text for keyword in ai_keywords)
    
    def _is_product_related(self, item: Dict) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸äº§å“å‘å¸ƒç›¸å…³"""
        product_keywords = [
            'launch', 'release', 'announce', 'unveil', 'introduce', 'debut',
            'new product', 'new version', 'update', 'upgrade', 'available',
            'official', 'beta', 'preview', 'api', 'service', 'platform',
            'å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿', 'æ­£å¼', 'æ–°ç‰ˆæœ¬', 'æ–°åŠŸèƒ½',
            'äº§å“', 'æœåŠ¡', 'å¹³å°', 'å…¬æµ‹', 'å†…æµ‹'
        ]
        
        text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
        return any(keyword in text for keyword in product_keywords)
    
    def _is_valid_item(self, item: Dict) -> bool:
        """éªŒè¯æ•°æ®é¡¹æœ‰æ•ˆæ€§"""
        return (item.get('title') and 
                item.get('url') and 
                len(item.get('title', '')) > 10)
    
    def _clean_html(self, text: str, max_length: int = 300) -> str:
        """
        æ¸…ç†æ–‡æœ¬ä¸­çš„ HTML æ ‡ç­¾
        
        Args:
            text: åŸå§‹æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å« HTMLï¼‰
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æ¸…ç†åçš„çº¯æ–‡æœ¬
        """
        if not text:
            return ''
        
        try:
            # ä½¿ç”¨ BeautifulSoup æ¸…ç† HTML æ ‡ç­¾
            # æ³¨æ„ï¼šä½¿ç”¨ features å‚æ•°å¹¶å°† text åŒ…è£…ç¡®ä¿ BS4 ä¸ä¼šè¯¯åˆ¤ä¸ºæ–‡ä»¶å
            from warnings import filterwarnings
            filterwarnings('ignore', category=MarkupResemblesLocatorWarning)
            soup = BeautifulSoup(text, features='html.parser')
            clean_text = soup.get_text(separator=' ', strip=True)
            
            # æ¸…ç†å¤šä½™ç©ºç™½
            clean_text = ' '.join(clean_text.split())
            
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            if len(clean_text) > max_length:
                clean_text = clean_text[:max_length] + '...'
            
            return clean_text
        except (AttributeError, TypeError, ValueError) as e:
            # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬çš„æˆªæ–­ç‰ˆæœ¬
            return text[:max_length] + '...' if len(text) > max_length else text
    
    def _is_recent(self, date_val) -> bool:
        """æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘Nå¤©å†…ï¼ˆç”±data_retention_daysé…ç½®å†³å®šï¼‰"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.data_retention_days)
            
            if isinstance(date_val, datetime):
                # å¤„ç†æ—¶åŒºæ„ŸçŸ¥çš„æ—¶é—´
                if date_val.tzinfo is not None:
                    # å¦‚æœcutoff_dateæ²¡æœ‰æ—¶åŒºï¼Œæ·»åŠ å½“å‰æ—¶åŒºæˆ–UTC
                    if cutoff_date.tzinfo is None:
                        from dateutil import tz
                        cutoff_date = cutoff_date.replace(tzinfo=tz.tzlocal())
                    
                    # å†æ¬¡æ£€æŸ¥ï¼Œå¦‚æœè¿˜æ˜¯ä¸åŒ¹é…ï¼Œå°è¯•è½¬æ¢
                    if date_val.tzinfo != cutoff_date.tzinfo:
                         # ç®€å•æ¯”è¾ƒæ—¶é—´æˆ³
                         return date_val.timestamp() >= cutoff_date.timestamp()
                return date_val >= cutoff_date
                
            if isinstance(date_val, str):
                # å°è¯•è§£æå­—ç¬¦ä¸²æ—¥æœŸ
                try:
                    dt = date_parser.parse(date_val)
                    # æ¯”è¾ƒæ—¶é—´æˆ³ä»¥é¿å…æ—¶åŒºé—®é¢˜
                    return dt.timestamp() >= cutoff_date.timestamp()
                except (ValueError, TypeError, AttributeError):
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•æ ¼å¼
                    if len(date_val) >= 10:
                        dt = datetime.strptime(date_val[:10], '%Y-%m-%d')
                        return dt >= cutoff_date
            
            # å¦‚æœæ˜¯struct_time (feedparser)
            if isinstance(date_val, time.struct_time):
                dt = datetime.fromtimestamp(time.mktime(date_val))
                return dt >= cutoff_date
                
            return True # æ— æ³•è§£ææ—¶é»˜è®¤ä¿ç•™
        except (ValueError, TypeError, AttributeError, OverflowError) as e:
            # æ—¥æœŸè§£æå¤±è´¥ï¼Œé»˜è®¤ä¿ç•™é¡¹ç›®
            return True
    
    def _get_backup_leaders_data(self) -> List[Dict]:
        """å¤‡ç”¨é¢†è¢–è¨€è®ºæ•°æ®"""
        return [
            {
                'title': 'Sam Altman: AIå‘å±•çš„é€Ÿåº¦å°†è¶…å‡ºæ‰€æœ‰äººçš„é¢„æœŸ',
                'summary': 'OpenAI CEO Sam Altmanåœ¨æœ€è¿‘çš„é‡‡è®¿ä¸­è¡¨ç¤ºï¼ŒAGIçš„åˆ°æ¥å¯èƒ½æ¯”é¢„æœŸçš„è¦å¿«ï¼Œç¤¾ä¼šéœ€è¦ä¸ºæ­¤åšå¥½å‡†å¤‡ã€‚',
                'url': 'https://openai.com/blog',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Interview',
                'author': 'Sam Altman',
                'author_title': 'OpenAI CEO'
},
            {
                'title': 'Elon Musk: AIå®‰å…¨æ˜¯æœªæ¥çš„é¦–è¦ä»»åŠ¡',
                'summary': 'Elon Muskå†æ¬¡å¼ºè°ƒAIå®‰å…¨çš„é‡è¦æ€§ï¼Œå¹¶è¡¨ç¤ºxAIçš„ç›®æ ‡æ˜¯ç†è§£å®‡å®™çš„æœ¬è´¨ï¼Œæ„å»ºæœ€å¤§é™åº¦è¿½æ±‚çœŸç†çš„AIã€‚',
                'url': 'https://x.ai',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'X (Twitter)',
                'author': 'Elon Musk',
                'author_title': 'xAI Founder'
},
            {
                'title': 'Jensen Huang: ç”Ÿæˆå¼AIæ˜¯è®¡ç®—é¢†åŸŸçš„è½¬æŠ˜ç‚¹',
                'summary': 'NVIDIA CEOé»„ä»å‹‹è¡¨ç¤ºï¼Œç”Ÿæˆå¼AIæ­£åœ¨é‡å¡‘æ¯ä¸€ä¸ªè¡Œä¸šï¼Œè®¡ç®—æ–¹å¼æ­£åœ¨å‘ç”Ÿæ ¹æœ¬æ€§çš„è½¬å˜ã€‚',
                'url': 'https://nvidianews.nvidia.com/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Keynote',
                'author': 'Jensen Huang',
                'author_title': 'NVIDIA CEO'
},
            {
                'title': 'Yann LeCun: ç°åœ¨çš„LLMè¿˜ä¸æ˜¯çœŸæ­£çš„æ™ºèƒ½',
                'summary': 'Metaé¦–å¸­AIç§‘å­¦å®¶Yann LeCunè®¤ä¸ºï¼Œç›®å‰çš„å¤§è¯­è¨€æ¨¡å‹ç¼ºä¹å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ï¼Œè·ç¦»çœŸæ­£çš„é€šç”¨äººå·¥æ™ºèƒ½è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚',
                'url': 'https://ai.meta.com/blog/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Interview',
                'author': 'Yann LeCun',
                'author_title': 'Meta Chief AI Scientist'
},
            {
                'title': 'æå¼€å¤: AI 2.0æ—¶ä»£å·²ç»åˆ°æ¥',
                'summary': 'é›¶ä¸€ä¸‡ç‰©CEOæå¼€å¤è¡¨ç¤ºï¼ŒAI 2.0æ—¶ä»£å°†å¸¦æ¥æ¯”ç§»åŠ¨äº’è”ç½‘å¤§åå€çš„æœºä¼šï¼Œä¸­å›½åœ¨åº”ç”¨å±‚æœ‰å·¨å¤§ä¼˜åŠ¿ã€‚',
                'url': 'https://www.01.ai/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Speech',
                'author': 'Kai-Fu Lee',
                'author_title': '01.AI CEO'
}
        ]

    def _get_backup_research_data(self) -> List[Dict]:
        """å¤‡ç”¨ç ”ç©¶æ•°æ®"""
        return [
            {
                'title': 'Attention Is All You Need: Transformeræ¶æ„æ·±åº¦åˆ†æ',
                'summary': 'æ·±å…¥åˆ†æTransformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é©å‘½æ€§ä½œç”¨ï¼Œæ¢è®¨æ³¨æ„åŠ›æœºåˆ¶çš„åŸç†å’Œåº”ç”¨ã€‚',
                'authors': ['AI Research Team'],
                'url': 'https://arxiv.org/abs/1706.03762',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'categories': ['cs.CL', 'cs.AI'],
                'source': 'arXiv'
}
        ]
    
    def _get_backup_github_data(self) -> List[Dict]:
        """å¤‡ç”¨GitHubæ•°æ®"""
        return [
            {
                'title': 'transformers',
                'summary': 'ğŸ¤— Transformers: State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX.',
                'url': 'https://github.com/huggingface/transformers',
                'stars': 132000,
                'language': 'Python',
                'updated': datetime.now().strftime('%Y-%m-%d'),
                'source': 'GitHub'
}
        ]
    
    def _get_backup_hf_data(self) -> List[Dict]:
        """å¤‡ç”¨Hugging Faceæ•°æ®"""
        return [
            {
                'title': 'HF Model: microsoft/DialoGPT-medium',
                'summary': 'æœ€æ–°AIæ¨¡å‹å‘å¸ƒ: microsoft/DialoGPT-mediumï¼Œä¸‹è½½é‡: 1500000',
                'url': 'https://huggingface.co/microsoft/DialoGPT-medium',
                'downloads': 1500000,
                'updated': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Hugging Face'
}
        ]
    
    def _get_backup_blog_data(self) -> List[Dict]:
        """å¤‡ç”¨åšå®¢æ•°æ®"""
        return [
            {
                'title': 'GitHub Copilotæœ€æ–°åŠŸèƒ½æ›´æ–°',
                'summary': 'GitHub Copilotæ¨å‡ºæ–°åŠŸèƒ½ï¼Œæ”¯æŒæ›´å¤šç¼–ç¨‹è¯­è¨€å’Œæ›´æ™ºèƒ½çš„ä»£ç å»ºè®®ï¼Œæå‡å¼€å‘æ•ˆç‡ã€‚',
                'url': 'https://github.blog',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'GitHub Blog'
}
        ]
    
    # ============== å¼‚æ­¥é‡‡é›†æ–¹æ³• ==============
    
    async def _fetch_url_async(self, session: aiohttp.ClientSession, url: str,
                                semaphore: asyncio.Semaphore,
                                category: str = 'unknown') -> Optional[str]:
        """å¼‚æ­¥è·å–URLå†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰"""
        last_error = None
        async with semaphore:
            for attempt in range(self.async_config.max_retries + 1):
                try:
                    self.stats['requests_made'] += 1
                    await asyncio.sleep(self.async_config.rate_limit_delay)
                    
                    timeout = aiohttp.ClientTimeout(total=self.async_config.request_timeout)
                    async with session.get(url, headers=self.headers, timeout=timeout) as response:
                        if response.status == 200:
                            return await response.text()
                        elif response.status == 429:
                            last_error = f'Rate limited (429)'
                            wait_time = self.async_config.retry_delay * (2 ** attempt)
                            await asyncio.sleep(wait_time)
                        else:
                            last_error = f'HTTP {response.status}'
                            return None
                except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                    last_error = str(e)[:50] or 'Timeout/Connection error'
                    if attempt < self.async_config.max_retries:
                        await asyncio.sleep(self.async_config.retry_delay * (attempt + 1))
                except Exception as e:
                    last_error = str(e)[:50] or 'Unknown error'
            
            # è®°å½•å¤±è´¥è¯¦æƒ…
            self._record_failure(url, category, last_error or 'Max retries exceeded')
            return None
    
    async def _fetch_json_async(self, session: aiohttp.ClientSession, url: str,
                                 semaphore: asyncio.Semaphore, params: Optional[Dict] = None,
                                 category: str = 'unknown') -> Optional[Any]:
        """å¼‚æ­¥è·å–JSONå†…å®¹"""
        last_error = None
        async with semaphore:
            for attempt in range(self.async_config.max_retries + 1):
                try:
                    self.stats['requests_made'] += 1
                    await asyncio.sleep(self.async_config.rate_limit_delay)
                    
                    timeout = aiohttp.ClientTimeout(total=self.async_config.request_timeout)
                    async with session.get(url, headers=self.headers, timeout=timeout, params=params) as response:
                        if response.status == 200:
                            return await response.json()
                        elif response.status == 429:
                            last_error = f'Rate limited (429)'
                            wait_time = self.async_config.retry_delay * (2 ** attempt)
                            await asyncio.sleep(wait_time)
                        else:
                            last_error = f'HTTP {response.status}'
                except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                    last_error = str(e)[:50] or 'Timeout/Connection error'
                    if attempt < self.async_config.max_retries:
                        await asyncio.sleep(self.async_config.retry_delay * (attempt + 1))
                except Exception as e:
                    last_error = str(e)[:50] or 'Unknown error'
            
            # è®°å½•å¤±è´¥è¯¦æƒ…
            self._record_failure(url, category, last_error or 'Max retries exceeded')
            return None
    
    async def _parse_rss_feed_async(self, session: aiohttp.ClientSession,
                                     feed_url: str, category: str,
                                     semaphore: asyncio.Semaphore,
                                     enable_url_filter: bool = True,
                                     items_per_feed: int = 10) -> List[Dict]:
        """å¼‚æ­¥è§£æRSSæºï¼ˆæ”¯æŒURLé¢„è¿‡æ»¤å’Œæ•°é‡é™åˆ¶ï¼‰
        
        Args:
            enable_url_filter: æ˜¯å¦å¯ç”¨URLé¢„è¿‡æ»¤ï¼ˆé»˜è®¤Trueï¼‰
            items_per_feed: æ¯ä¸ªæºæœ€å¤šé‡‡é›†çš„æ¡æ•°ï¼ˆé»˜è®¤10ï¼‰
        """
        items = []
        try:
            content = await self._fetch_url_async(session, feed_url, semaphore, category)
            if not content:
                return items
            
            loop = asyncio.get_event_loop()
            feed = await loop.run_in_executor(None, feedparser.parse, content)
            
            # å…ˆæå–æ‰€æœ‰URLå¹¶è¿›è¡Œé¢„è¿‡æ»¤ï¼ˆé™åˆ¶æ¡æ•°ï¼‰
            max_entries = min(items_per_feed, 10)  # æœ€å¤š10æ¡
            entries_to_process = []
            if enable_url_filter:
                for entry in feed.entries[:max_entries * 2]:  # å¤šæ£€æŸ¥ä¸€äº›ä»¥åº”å¯¹è¿‡æ»¤
                    if len(entries_to_process) >= max_entries:
                        break
                    url = entry.get('link', '')
                    # ä½¿ç”¨è§„èŒƒåŒ–URLè¿›è¡Œç¼“å­˜åŒ¹é…ï¼Œç¡®ä¿ä¸€è‡´æ€§
                    normalized_url = self._normalize_url(url) if url else ''
                    if normalized_url and normalized_url not in self.history_cache['urls']:
                        entries_to_process.append(entry)
            else:
                entries_to_process = feed.entries[:max_entries]
            
            # åªå¤„ç†æ–°URLçš„å†…å®¹
            for entry in entries_to_process:
                if len(items) >= items_per_feed:
                    break
                date_val = entry.get('published_parsed') or entry.get('published')
                if date_val and not self._is_recent(date_val):
                    continue
                
                # æ¸…ç† summary ä¸­çš„ HTML æ ‡ç­¾
                raw_summary = entry.get('summary', entry.get('description', ''))
                clean_summary = self._clean_html(raw_summary, max_length=300)
                
                item = {
                    'title': entry.get('title', ''),
                    'summary': clean_summary,
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'source': feed.feed.get('title', feed_url)[:50],
                    '_source_type': category  # å†…éƒ¨åˆ†ç»„ç”¨ï¼Œä¸ç”¨äºåˆ†ç±»
}
                
                if self._is_valid_item(item):
                    items.append(item)
        except (AttributeError, KeyError, ValueError) as e:
            # RSSè§£æå¤±è´¥ï¼Œè®°å½•é”™è¯¯
            log.debug(f"RSS parsing error: {e}")
        
        return items
    
    def _collect_research_papers_sync(self, max_results: int = 10) -> List[Dict]:
        """åŒæ­¥é‡‡é›†ç ”ç©¶è®ºæ–‡ï¼ˆä¾›å¼‚æ­¥åŒ…è£…å™¨è°ƒç”¨ï¼‰"""
        papers = []
        
        try:
            # ä½¿ç”¨arXiv APIè·å–æœ€æ–°è®ºæ–‡
            client = arxiv.Client()
            
            # æ„å»ºæŸ¥è¯¢ - æœ€æ–°çš„AIç›¸å…³è®ºæ–‡
            search_query = arxiv.Search(
                query="cat:cs.AI OR cat:cs.LG OR cat:cs.CV OR cat:cs.CL",
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            for result in client.results(search_query):
                # è¿‡æ»¤è¶…å‡ºé‡‡é›†çª—å£çš„è®ºæ–‡ï¼ˆç”±data_retention_daysé…ç½®ï¼‰
                if not self._is_recent(result.published):
                    continue
                    
                paper = {
                    'title': result.title,
                    'summary': self._clean_html(result.summary),
                    'authors': [str(author) for author in result.authors],
                    'url': result.entry_id,
                    'published': result.published.strftime('%Y-%m-%d'),
                    'categories': [str(cat) for cat in result.categories],
                    'source': 'arXiv'
                }
                papers.append(paper)
                
        except Exception as e:
            log.error(t('dc_arxiv_failed', error=str(e)))
            # æä¾›å¤‡ç”¨æ•°æ®
            papers = self._get_backup_research_data()
        
        return papers
    
    async def _collect_research_papers_async(self, max_results: int = 10) -> List[Dict]:
        """å¼‚æ­¥é‡‡é›†ç ”ç©¶è®ºæ–‡ (arxivåº“ä¸æ”¯æŒå¼‚æ­¥ï¼Œä½¿ç”¨executor)"""
        loop = asyncio.get_event_loop()
        papers = await loop.run_in_executor(None, self._collect_research_papers_sync, max_results)
        # æ·»åŠ  _source_type ç”¨äºå†…éƒ¨åˆ†ç»„
        for paper in papers:
            paper['_source_type'] = 'research'
        return papers
    
    async def _collect_github_trending_async(self, session: aiohttp.ClientSession, 
                                            semaphore: asyncio.Semaphore,
                                            enable_url_filter: bool = True,
                                            max_items: int = 10) -> List[Dict]:
        """å¼‚æ­¥é‡‡é›†GitHubçƒ­é—¨é¡¹ç›®ï¼ˆæ”¯æŒURLé¢„è¿‡æ»¤å’Œæ•°é‡é™åˆ¶ï¼‰"""
        projects = []
        try:
            cutoff_date = (datetime.now() - timedelta(days=self.data_retention_days)).strftime('%Y-%m-%d')
            url = "https://api.github.com/search/repositories"
            query = f'(machine-learning OR artificial-intelligence OR deep-learning OR llm) created:>{cutoff_date}'
            
            params = {
                'q': query,
                'sort': 'stars',
                'order': 'desc',
                'per_page': min(max_items + 5, 15)  # å¤šè¯·æ±‚å‡ ä¸ªä»¥åº”å¯¹è¿‡æ»¤
            }
            
            data = await self._fetch_json_async(session, url, semaphore, params, 'developer')
            if data:
                # å…ˆè¿‡æ»¤æ‰å·²ç¼“å­˜çš„URLï¼ˆä½¿ç”¨è§„èŒƒåŒ–URLï¼‰
                repos_to_process = []
                for repo in data.get('items', [])[:max_items + 5]:
                    repo_url = repo.get('html_url', '')
                    normalized_url = self._normalize_url(repo_url) if repo_url else ''
                    if enable_url_filter:
                        if normalized_url and normalized_url not in self.history_cache['urls']:
                            repos_to_process.append(repo)
                    else:
                        repos_to_process.append(repo)
                
                # åªå¤„ç†æ–°repoï¼ˆé™åˆ¶æ•°é‡ï¼‰
                for repo in repos_to_process:
                    if len(projects) >= max_items:
                        break
                    if not self._is_recent(repo.get('updated_at', '')):
                        continue
                    
                    project = {
                        'title': repo['full_name'],
                        'summary': repo.get('description') or 'No description',
                        'url': repo['html_url'],
                        'stars': repo.get('stargazers_count', 0),
                        'language': repo.get('language', 'Unknown'),
                        'updated': repo['updated_at'][:10],
                        'published': repo['updated_at'][:10],
                        'source': 'GitHub',
                        '_source_type': 'developer'  # å†…éƒ¨åˆ†ç»„ç”¨
                    }
                    projects.append(project)
        except Exception as e:
            self._record_failure('GitHub API (async)', 'developer', str(e))
            log.warning(f"GitHub trending async failed: {e}")
        
        return projects
    
    async def _collect_huggingface_async(self, session: aiohttp.ClientSession,
                                        semaphore: asyncio.Semaphore,
                                        enable_url_filter: bool = True,
                                        max_items: int = 10) -> List[Dict]:
        """å¼‚æ­¥é‡‡é›†Hugging Faceæ›´æ–°ï¼ˆæ”¯æŒURLé¢„è¿‡æ»¤å’Œæ•°é‡é™åˆ¶ï¼‰"""
        updates = []
        try:
            url = "https://huggingface.co/api/models"
            params = {'limit': min(max_items + 5, 15), 'sort': 'lastModified', 'direction': -1}
            
            data = await self._fetch_json_async(session, url, semaphore, params, 'developer')
            if data:
                # å…ˆè¿‡æ»¤æ‰å·²ç¼“å­˜çš„URLï¼ˆä½¿ç”¨è§„èŒƒåŒ–URLï¼‰
                models_to_process = []
                for model in data[:max_items + 5]:
                    model_url = f"https://huggingface.co/{model['id']}"
                    normalized_url = self._normalize_url(model_url)
                    if enable_url_filter:
                        if normalized_url and normalized_url not in self.history_cache['urls']:
                            models_to_process.append(model)
                    else:
                        models_to_process.append(model)
                
                # åªå¤„ç†æ–°æ¨¡å‹ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                for model in models_to_process:
                    if len(updates) >= max_items:
                        break
                    if not self._is_recent(model.get('lastModified', '')):
                        continue
                    
                    update = {
                        'title': f"HF Model: {model['id']}",
                        'summary': f"Latest AI model: {model['id']}, downloads: {model.get('downloads', 0)}",
                        'url': f"https://huggingface.co/{model['id']}",
                        'downloads': model.get('downloads', 0),
                        'updated': model.get('lastModified', '')[:10],
                        'published': model.get('lastModified', '')[:10],
                        'source': 'Hugging Face',
                        '_source_type': 'developer'  # å†…éƒ¨åˆ†ç»„ç”¨
                    }
                    updates.append(update)
        except Exception as e:
            self._record_failure('Hugging Face API (async)', 'developer', str(e))
            log.warning(f"Hugging Face async failed: {e}")
        
        return updates
    
    async def _collect_hacker_news_async(self, session: aiohttp.ClientSession,
                                        semaphore: asyncio.Semaphore,
                                        max_items: int = 10,
                                        enable_url_filter: bool = True) -> List[Dict]:
        """å¼‚æ­¥é‡‡é›†Hacker Newsï¼ˆæ”¯æŒURLé¢„è¿‡æ»¤ï¼‰"""
        items = []
        try:
            # è·å–top stories
            top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            story_ids = await self._fetch_json_async(session, top_url, semaphore, None, 'community')
            
            if not story_ids:
                return items
            
            # å¹¶å‘è·å–storyè¯¦æƒ…
            ai_keywords = ['ai', 'llm', 'gpt', 'machine learning', 'deep learning', 
                          'neural', 'openai', 'anthropic', 'chatgpt']
            
            # ä¸ºæ¯ä¸ªstory IDæ„å»ºURLï¼Œç”¨äºé¢„è¿‡æ»¤
            story_tasks = []
            for story_id in story_ids[:50]:  # æ£€æŸ¥å‰50ä¸ª
                story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                # æ³¨æ„ï¼šHNçš„URLå®é™…æ˜¯storyæœ¬èº«çš„urlå­—æ®µï¼Œè¿™é‡Œæˆ‘ä»¬ä»éœ€è¦è·å–è¯¦æƒ…æ¥åˆ¤æ–­
                # ä½†å¯ä»¥å…ˆæ£€æŸ¥story IDæ˜¯å¦å·²å¤„ç†è¿‡ï¼ˆé€šè¿‡æ„é€ æ ‡å‡†URLï¼‰
                story_tasks.append(self._fetch_json_async(session, story_url, semaphore, None, 'community'))
            
            stories = await asyncio.gather(*story_tasks, return_exceptions=True)
            
            for story in stories:
                if isinstance(story, dict) and story.get('title'):
                    title_lower = story['title'].lower()
                    if any(kw in title_lower for kw in ai_keywords):
                        # æ„å»ºURLç”¨äºè¿‡æ»¤æ£€æŸ¥
                        story_url = story.get('url', f"https://news.ycombinator.com/item?id={story['id']}")
                        normalized_url = self._normalize_url(story_url)
                        
                        # URLé¢„è¿‡æ»¤ï¼šè·³è¿‡å·²ç¼“å­˜çš„URLï¼ˆä½¿ç”¨è§„èŒƒåŒ–URLï¼‰
                        if enable_url_filter and normalized_url in self.history_cache['urls']:
                            continue
                        
                        # æ£€æŸ¥æ—¶é—´
                        if story.get('time'):
                            published = datetime.fromtimestamp(story['time'])
                            if not self._is_recent(published):
                                continue
                            published_str = published.strftime('%Y-%m-%d')
                        else:
                            published_str = datetime.now().strftime('%Y-%m-%d')
                        
                        item = {
                            'title': story['title'],
                            'summary': self._clean_html(story.get('text', story['title'])),
                            'url': story_url,
                            'published': published_str,
                            'source': 'Hacker News',
                            'score': story.get('score', 0),
                            '_source_type': 'community'  # å†…éƒ¨åˆ†ç»„ç”¨
                        }
                        items.append(item)
                        
                        if len(items) >= max_items:
                            break
        except Exception as e:
            self._record_failure('Hacker News API (async)', 'community', str(e))
            log.warning(f"Hacker News async failed: {e}")
        
        return items
    
    async def _collect_product_releases_async(self, session: aiohttp.ClientSession,
                                             semaphore: asyncio.Semaphore,
                                             max_results: int = 10) -> List[Dict]:
        """å¼‚æ­¥é‡‡é›†äº§å“å‘å¸ƒï¼ˆé€šè¿‡RSSæº + å…¬å¸ä¸“å±æ¥æºï¼‰"""
        products = []
        
        # å…¬å¸æ¥æºæ˜ å°„ï¼ˆç”¨äºæ ‡è®°æ¥æºå…¬å¸ï¼‰
        company_source_map = {
            'openai.com': 'OpenAI',
            'blog.google': 'Google',
            'blogs.microsoft.com': 'Microsoft',
            'ai.meta.com': 'Meta',
            'anthropic.com': 'Anthropic',
            'jiqizhixin.com': 'China_Tech',
            'qbitai.com': 'China_Tech',
        }
        
        # ä½¿ç”¨äº§å“ç›¸å…³çš„RSSæº
        product_feeds = RSS_FEEDS.get('product_news', [])
        
        tasks = []
        for feed_url in product_feeds:
            tasks.append(self._parse_rss_feed_async(session, feed_url, 'product', semaphore))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœï¼Œæ ‡è®°å…¬å¸æ¥æº
        for i, result in enumerate(results):
            if isinstance(result, list):
                feed_url = product_feeds[i] if i < len(product_feeds) else ''
                # è¯†åˆ«æ¥æºå…¬å¸
                company = None
                for domain, comp_name in company_source_map.items():
                    if domain in feed_url:
                        company = comp_name
                        break
                
                for item in result:
                    if self._is_product_related(item):
                        # æ ‡è®°æ¥æºå…¬å¸ï¼ˆå¦‚æœè¯†åˆ«åˆ°ï¼‰
                        if company and not item.get('company'):
                            item['company'] = company
                        products.append(item)
        
        # æŒ‰äº§å“ä¼˜å…ˆçº§æ’åºï¼šå®˜æ–¹å…¬å¸æ¥æºä¼˜å…ˆï¼Œå†æŒ‰æ—¶é—´æ’åº
        def product_sort_key(item):
            # ä¼˜å…ˆçº§ï¼šæœ‰å…¬å¸æ ‡è®°çš„æ’å‰é¢
            has_company = 1 if item.get('company') else 0
            # æ—¶é—´æ’åºï¼ˆé™åºï¼‰
            published = item.get('published', '1970-01-01')
            return (-has_company, published)
        
        products.sort(key=product_sort_key, reverse=True)
        return products[:max_results]
    
    async def _collect_leaders_quotes_async(self, session: aiohttp.ClientSession,
                                           semaphore: asyncio.Semaphore,
                                           max_results: int = 15) -> List[Dict]:
        """å¼‚æ­¥é‡‡é›†AIé¢†è¢–è¨€è®º"""
        quotes = []
        
        leaders = {
            "Sam Altman": "OpenAI CEO",
            "Elon Musk": "xAI Founder",
            "Jensen Huang": "NVIDIA CEO",
            "Demis Hassabis": "Google DeepMind CEO",
            "Yann LeCun": "Meta Chief AI Scientist",
            "Geoffrey Hinton": "AI Pioneer",
            "Andrew Ng": "AI Fund Managing General Partner",
            "Kai-Fu Lee": "01.AI CEO",
            "Robin Li": "Baidu CEO"
        }
        
        # ä½¿ç”¨Google News RSSæœç´¢æ¯ä¸ªé¢†è¢–
        tasks = []
        for leader_name in leaders.keys():
            query_name = leader_name.replace(' ', '+')
            feed_url = f"https://news.google.com/rss/search?q={query_name}+AI+when:7d&hl=en-US&gl=US&ceid=US:en"
            tasks.append(self._parse_rss_feed_async(session, feed_url, 'leader', semaphore))
        
        # åŒæ—¶é‡‡é›†ä¸ªäººåšå®¢
        for source in self.rss_feeds.get('leader_blogs', []):
            tasks.append(self._parse_rss_feed_async(session, source['url'], 'leader', semaphore))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, list):
                for item in result:
                    # å¦‚æœæ˜¯æ–°é—»æœç´¢ç»“æœï¼Œæ·»åŠ é¢†è¢–ä¿¡æ¯
                    if i < len(leaders):
                        leader_name = list(leaders.keys())[i]
                        item['author'] = leader_name
                        item['author_title'] = leaders[leader_name]
                    
                    quotes.append(item)
        
        # å¦‚æœæ•°é‡ä¸è¶³ï¼Œæ·»åŠ å¤‡ç”¨æ•°æ®
        if len(quotes) < 5:
            backup_data = self._get_backup_leaders_data()
            for item in backup_data:
                item['_source_type'] = 'leader'  # ä¸ºå¤‡ç”¨æ•°æ®æ·»åŠ åˆ†ç»„æ ‡è®°
            quotes.extend(backup_data)
        
        # å»é‡
        quotes = self._deduplicate_items(quotes)
        return quotes[:max_results]
    
    async def _collect_community_async(self, session: aiohttp.ClientSession,
                                      semaphore: asyncio.Semaphore,
                                      max_results: int = 15) -> List[Dict]:
        """å¼‚æ­¥é‡‡é›†ç¤¾åŒºçƒ­ç‚¹"""
        trends = []
        
        # Hacker News (ä½¿ç”¨API)
        hn_items = await self._collect_hacker_news_async(session, semaphore, max_items=10)
        trends.extend(hn_items)
        
        # å…¶ä»–ç¤¾åŒºRSSæº
        community_feeds = [f for f in self.rss_feeds.get('community', []) if 'hnrss' not in f]
        
        tasks = []
        for feed_url in community_feeds:
            tasks.append(self._parse_rss_feed_async(session, feed_url, 'community', semaphore))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                for item in result:
                    trends.append(item)
        
        # å»é‡å¹¶æ’åº
        trends = self._deduplicate_items(trends)
        trends.sort(key=lambda x: x.get('published', ''), reverse=True)
        
        return trends[:max_results]
    
    async def _collect_all_async(self) -> Dict[str, List[Dict]]:
        """
        å¼‚æ­¥é‡‡é›†æ‰€æœ‰ç±»å‹çš„æ•°æ®ï¼ˆå¸¦URLé¢„è¿‡æ»¤ä¼˜åŒ–ï¼‰
        
        Returns:
            åˆ†ç±»çš„æ•°æ®å­—å…¸
        """
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self._reset_stats()
        self.stats['start_time'] = time.time()
        log.dual_start(t('dc_start_collection'))
        log.dual_separator("=", 50)
        log.dual_info("ğŸš€ å¼‚æ­¥é‡‡é›†æ¨¡å¼ + URLé¢„è¿‡æ»¤ä¼˜åŒ– (Async Mode with URL Pre-filtering)", emoji="")
        
        all_data = {
            'research': [],
            'developer': [],
            'product': [],
            'news': [],
            'leader': [],
            'community': []
        }
        
        # ä»é…ç½®è¯»å–é‡‡é›†æ•°é‡
        product_count = config.get('collector.product_count', 15)
        community_count = config.get('collector.community_count', 10)
        leader_count = config.get('collector.leader_count', 15)
        research_count = config.get('collector.research_count', 15)
        developer_count = config.get('collector.developer_count', 20)
        news_count = config.get('collector.news_count', 25)
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.async_config.max_concurrent_requests)
        
        # åˆ›å»ºå…±äº«çš„aiohttpä¼šè¯
        connector = aiohttp.TCPConnector(
            limit=self.async_config.max_concurrent_requests,
            limit_per_host=self.async_config.max_concurrent_per_host
        )
        timeout = aiohttp.ClientTimeout(total=self.async_config.total_timeout)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # å¹¶å‘é‡‡é›†æ‰€æœ‰æ•°æ®æº
            log.dual_info("ğŸ“¡ å¯åŠ¨å¹¶å‘é‡‡é›†ä»»åŠ¡...", emoji="")
            
            # åˆ›å»ºå¸¦åç§°çš„ä»»åŠ¡åˆ—è¡¨: [(name, coroutine), ...]
            named_tasks = []
            
            # 1. æ–°é—»RSSæºï¼ˆé™åˆ¶æºæ•°é‡ï¼Œä¼˜å…ˆé‡‡é›†é‡è¦æºï¼‰
            news_feeds = RSS_FEEDS['news'] + RSS_FEEDS.get('product_news', [])
            # è®¡ç®—æ¯æºé…é¢ï¼šnews_count / æºæ•°é‡ï¼Œè‡³å°‘2æ¡
            items_per_news_feed = max(2, news_count // max(len(news_feeds), 1))
            # åªé‡‡é›†å‰å‡ ä¸ªé‡è¦æºï¼Œé¿å…è¿‡å¤šè¯·æ±‚
            max_news_feeds = min(len(news_feeds), max(6, news_count // 3))
            for i, feed_url in enumerate(news_feeds[:max_news_feeds]):
                # ä»URLæå–ç®€çŸ­åç§°
                domain = urlparse(feed_url).netloc.replace('www.', '')[:20]
                named_tasks.append((
                    f"RSS/{domain}",
                    self._parse_rss_feed_async(session, feed_url, 'news', semaphore, 
                                               items_per_feed=items_per_news_feed)
                ))
            
            # 2. å¼€å‘è€…å†…å®¹ (GitHub + Hugging Face + åšå®¢RSS)
            dev_github_limit = min(5, developer_count // 3)
            dev_hf_limit = min(5, developer_count // 3)
            dev_rss_limit = max(2, (developer_count - dev_github_limit - dev_hf_limit) // max(len(RSS_FEEDS['developer']), 1))
            named_tasks.append(("GitHub Trending", self._collect_github_trending_async(session, semaphore, max_items=dev_github_limit)))
            named_tasks.append(("Hugging Face", self._collect_huggingface_async(session, semaphore, max_items=dev_hf_limit)))
            for feed_url in RSS_FEEDS['developer']:
                domain = urlparse(feed_url).netloc.replace('www.', '')[:20]
                named_tasks.append((
                    f"Dev/{domain}",
                    self._parse_rss_feed_async(session, feed_url, 'developer', semaphore,
                                               items_per_feed=dev_rss_limit)
                ))
            
            # 3. äº§å“å‘å¸ƒ
            named_tasks.append(("Product Releases", self._collect_product_releases_async(session, semaphore, product_count)))
            
            # 4. AIé¢†è¢–è¨€è®º
            named_tasks.append(("AI Leaders", self._collect_leaders_quotes_async(session, semaphore, leader_count)))
            
            # 5. ç¤¾åŒºçƒ­ç‚¹
            named_tasks.append(("Community/HN", self._collect_community_async(session, semaphore, community_count)))
            
            # 6. ç ”ç©¶è®ºæ–‡ (åœ¨executorä¸­è¿è¡Œ)
            named_tasks.append(("arXiv Papers", self._collect_research_papers_async(research_count)))
            
            # åˆ›å»ºä»»åŠ¡
            total_tasks = len(named_tasks)
            tasks = [asyncio.create_task(coro) for name, coro in named_tasks]
            
            log.dual_info(f"âš¡ å¹¶å‘æ‰§è¡Œ {total_tasks} ä¸ªé‡‡é›†ä»»åŠ¡", emoji="")
            
            # ä½¿ç”¨ as_completed å®æ—¶æ˜¾ç¤ºè¿›åº¦
            all_results = []
            completed = 0
            total_items = 0
            for future in asyncio.as_completed(tasks):
                try:
                    result = await future
                    completed += 1
                    item_count = len(result) if isinstance(result, list) else 0
                    total_items += item_count
                    all_results.append(result)
                    
                    # æ˜¾ç¤ºè¿›åº¦æ¡
                    progress_pct = int(completed / total_tasks * 100)
                    bar_filled = int(completed / total_tasks * 20)
                    bar = "â–ˆ" * bar_filled + "â–‘" * (20 - bar_filled)
                    log.dual_info(f"  [{bar}] {completed}/{total_tasks} ({progress_pct}%) +{item_count} items", emoji="")
                    
                except Exception as e:
                    completed += 1
                    all_results.append(e)
                    progress_pct = int(completed / total_tasks * 100)
                    bar_filled = int(completed / total_tasks * 20)
                    bar = "â–ˆ" * bar_filled + "â–‘" * (20 - bar_filled)
                    log.dual_warning(f"  [{bar}] {completed}/{total_tasks} ({progress_pct}%) âœ— å¤±è´¥")
            
            # åˆ†ç±»æ”¶é›†ç»“æœï¼ˆå¸¦é…é¢é™åˆ¶ï¼‰
            category_limits = {
                'news': news_count,
                'developer': developer_count,
                'product': product_count,
                'leader': leader_count,
                'community': community_count,
                'research': research_count
            }
            
            for result in all_results:
                if isinstance(result, list):
                    for item in result:
                        # ä½¿ç”¨ _source_type è¿›è¡Œå†…éƒ¨åˆ†ç»„ï¼ˆä¸æ˜¯åˆ†ç±»æ ‡ç­¾ï¼‰
                        source_type = item.pop('_source_type', 'news')  # ç§»é™¤å¹¶è·å–ï¼Œé»˜è®¤ä¸º news
                        if source_type in all_data:
                            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé…é¢
                            if len(all_data[source_type]) < category_limits.get(source_type, 100):
                                all_data[source_type].append(item)
                elif isinstance(result, Exception):
                    self._record_failure('Async Task', 'unknown', str(result))
                    log.warning(f"Task failed: {result}")
        
        # ç»Ÿä¸€å»é‡å¤„ç†
        all_data = self._apply_deduplication(all_data)
        
        # ç»Ÿä¸€å†å²ç¼“å­˜è¿‡æ»¤ï¼ˆå¯ç”¨è¿‡æ»¤ï¼Œç§»é™¤å·²é‡‡é›†è¿‡çš„å†…å®¹ï¼‰
        # filter_enabled=True: å®é™…è¿‡æ»¤æ‰å†å²ä¸­å·²æœ‰çš„é¡¹ç›®ï¼Œå‡å°‘åç»­å¤„ç†é‡
        all_data, new_stats, cached_stats = self._filter_by_history(all_data, filter_enabled=True)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆè¿‡æ»¤åçš„æ•°æ®ï¼‰
        self.stats['end_time'] = time.time()
        total_new = sum(len(items) for items in all_data.values())
        total_cached = sum(cached_stats.values())
        self.stats['items_collected'] = total_new
        
        # æ‰“å°ç»Ÿè®¡
        elapsed = self.stats['end_time'] - self.stats['start_time']
        
        log.dual_separator("=", 50)
        log.dual_done(f"é‡‡é›†å®Œæˆ: {total_new + total_cached} items ({total_new} new, {total_cached} cached)")
        log.dual_info(f"â±ï¸ è€—æ—¶: {elapsed:.1f}s | è¯·æ±‚: {self.stats['requests_made']} | å¤±è´¥: {self.stats['requests_failed']}", emoji="")
        
        for category, items in all_data.items():
            new_count = new_stats.get(category, 0)
            cached_count = cached_stats.get(category, 0)
            log.dual_data(f"  {category}: {new_count + cached_count} ({new_count} new, {cached_count} cached)")
        
        # æ˜¾ç¤ºå¤±è´¥æ•°æ®æºæ±‡æ€»
        self._print_failed_sources_summary()
        
        return all_data

# ç”¨äºå‘åå…¼å®¹
DataCollector = AIDataCollector

"""
AIä¸–ç•Œè¿½è¸ªå™¨ - æ•°æ®é‡‡é›†æ¨¡å—
ä¸“æ³¨äºæ”¶é›†æœ€æ–°AIç ”ç©¶ã€äº§å“ã€å¼€å‘è€…ç¤¾åŒºå’Œè¡Œä¸šä¿¡æ¯
"""

import requests
import feedparser
import arxiv
import json
import os
import yaml
from datetime import datetime, timedelta
from dateutil import parser
from typing import List, Dict, Optional
import time
import random
import difflib
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from config import config
from logger import get_log_helper

# å¯¼å…¥å›½é™…åŒ–æ¨¡å—
try:
    from i18n import t, get_language
except ImportError:
    def t(key, **kwargs): return key
    def get_language(): return 'zh'

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('data_collector')

# åŠ è½½ç¼“å­˜ç›®å½•é…ç½®
def _get_cache_dir():
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    cache_dir = 'data/cache'
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                cache_dir = cfg.get('data', {}).get('cache_dir', cache_dir)
    except Exception:
        pass
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir

DATA_CACHE_DIR = _get_cache_dir()


class AIDataCollector:
    """AIæ•°æ®é‡‡é›†å™¨ - æ”¶é›†çœŸå®æœ€æ–°çš„AIä¿¡æ¯"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # RSSæºé…ç½® - çœŸå®å¯ç”¨çš„AIæ–°é—»æº
        self.rss_feeds = {
            'research': [
                'http://export.arxiv.org/rss/cs.AI',  # ArXiv AI
                'http://export.arxiv.org/rss/cs.CL',  # è®¡ç®—è¯­è¨€å­¦  
                'http://export.arxiv.org/rss/cs.CV',  # è®¡ç®—æœºè§†è§‰
                'http://export.arxiv.org/rss/cs.LG',  # æœºå™¨å­¦ä¹ 
            ],
            'news': [
                # å›½é™…AIæ–°é—»æº
                'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml',  # The Verge AI
                'https://techcrunch.com/category/artificial-intelligence/feed/',  # TechCrunch AI
                'https://www.wired.com/feed/tag/ai/latest/rss',  # Wired AI
                'https://spectrum.ieee.org/rss/topic/artificial-intelligence',  # IEEE Spectrum AI
                'https://www.technologyreview.com/feed/',  # MIT Technology Review
                'https://artificialintelligence-news.com/feed/',  # AI News
                'https://syncedreview.com/feed/',  # Synced Review (AIä¸“ä¸š)
                # ä¸­å›½AIæ–°é—»æº
                'https://www.36kr.com/feed',  # 36æ°ª (ç§‘æŠ€åˆ›ä¸š)
                'https://www.ithome.com/rss/',  # ITä¹‹å®¶
                'https://www.jiqizhixin.com/rss',  # æœºå™¨ä¹‹å¿ƒ
                'https://www.qbitai.com/feed',  # é‡å­ä½
                'https://www.infoq.cn/feed/topic/18',  # InfoQ AI
            ],
            'developer': [
                'https://github.blog/feed/',  # GitHub Blog
                'https://huggingface.co/blog/feed.xml',  # Hugging Face
                'https://openai.com/blog/rss.xml',  # OpenAI Blog
                'https://blog.google/technology/ai/rss/',  # Google AI Blog
            ],
            'product_news': [  # æ–°å¢ä¸“é—¨çš„äº§å“å‘å¸ƒæ–°é—»æº
                # å…¬å¸å®˜æ–¹åšå®¢
                'https://openai.com/blog/rss.xml',  # OpenAIå®˜æ–¹åšå®¢
                'https://blog.google/technology/ai/rss/',  # Google AIåšå®¢
                'https://blogs.microsoft.com/ai/feed/',  # Microsoft AIåšå®¢
                # Metaå’ŒAnthropicå› RSSä¸ç¨³å®šï¼Œä¸»è¦ä¾èµ–å¤‡ç”¨æ•°æ®æˆ–æ–°é—»èšåˆ
                
                # ä¸­å›½å…¬å¸
                # è…¾è®¯äº‘RSSå­˜åœ¨è§£æé”™è¯¯ï¼Œå·²ç§»é™¤
            ],
            'community': [  # æ–°å¢ç¤¾åŒºçƒ­ç‚¹æº
                'https://www.producthunt.com/feed?category=artificial-intelligence', # Product Hunt AI
                'https://hnrss.org/newest?q=AI+LLM', # Hacker News AI (ç®€åŒ–æŸ¥è¯¢ä»¥æé«˜å‘½ä¸­ç‡)
            ],
            'leader_blogs': [  # æ–°å¢é¢†è¢–ä¸ªäººæ¸ é“
                {'url': 'http://blog.samaltman.com/posts.atom', 'author': 'Sam Altman', 'title': 'OpenAI CEO'},
                {'url': 'https://karpathy.github.io/feed.xml', 'author': 'Andrej Karpathy', 'title': 'AI Researcher'},
                {'url': 'https://lexfridman.com/feed/podcast/', 'author': 'Lex Fridman', 'title': 'Podcast Host', 'type': 'podcast'},
            ]
        }
        
        # é‡‡é›†å†å²ç¼“å­˜
        self.history_cache_file = os.path.join(DATA_CACHE_DIR, 'collection_history_cache.json')
        self.history_cache = self._load_history_cache()
    
    def _load_history_cache(self) -> Dict:
        """åŠ è½½é‡‡é›†å†å²ç¼“å­˜"""
        try:
            if os.path.exists(self.history_cache_file):
                with open(self.history_cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                    # éªŒè¯ç¼“å­˜æ ¼å¼
                    if isinstance(cache, dict) and 'urls' in cache and 'titles' in cache:
                        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡7å¤©ï¼‰
                        last_updated = cache.get('last_updated', '')
                        if last_updated:
                            try:
                                last_time = datetime.fromisoformat(last_updated)
                                if (datetime.now() - last_time).days > 7:
                                    log.warning(t('dc_cache_expired'))
                                    return {'urls': set(), 'titles': set(), 'last_updated': ''}
                            except (ValueError, TypeError):
                                pass
                        # è½¬æ¢ä¸º set ä»¥åŠ é€ŸæŸ¥æ‰¾
                        cache['urls'] = set(cache['urls'])
                        cache['titles'] = set(cache['titles'])
                        log.data(t('dc_cache_loaded', url_count=len(cache['urls']), title_count=len(cache['titles'])))
                        return cache
        except Exception as e:
            log.error(t('dc_cache_load_failed', error=str(e)))
        return {'urls': set(), 'titles': set(), 'last_updated': ''}
    
    def _save_history_cache(self):
        """ä¿å­˜é‡‡é›†å†å²ç¼“å­˜"""
        try:
            # è½¬æ¢ set ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
            cache_to_save = {
                'urls': list(self.history_cache['urls']),
                'titles': list(self.history_cache['titles']),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.history_cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_to_save, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(t('dc_cache_save_failed', error=str(e)))
    
    def _is_in_history(self, item: Dict) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦åœ¨å†å²ç¼“å­˜ä¸­ï¼ˆä¸¥æ ¼åŒ¹é… URL æˆ–æ ‡é¢˜ï¼‰"""
        url = item.get('url', '')
        title = item.get('title', '')
        
        # ä¸¥æ ¼åŒ¹é…ï¼šURL å®Œå…¨ç›¸åŒ æˆ– æ ‡é¢˜å®Œå…¨ç›¸åŒ
        if url and url in self.history_cache['urls']:
            return True
        if title and title in self.history_cache['titles']:
            return True
        return False
    
    def _add_to_history(self, item: Dict):
        """å°†é¡¹ç›®æ·»åŠ åˆ°å†å²ç¼“å­˜"""
        url = item.get('url', '')
        title = item.get('title', '')
        if url:
            self.history_cache['urls'].add(url)
        if title:
            self.history_cache['titles'].add(title)
    
    def clear_history_cache(self):
        """æ¸…é™¤é‡‡é›†å†å²ç¼“å­˜"""
        import os
        self.history_cache = {'urls': set(), 'titles': set(), 'last_updated': ''}
        if os.path.exists(self.history_cache_file):
            os.remove(self.history_cache_file)
        log.success(t('dc_cache_cleared'))
    
    def collect_research_papers(self, max_results: int = 10) -> List[Dict]:
        """
        é‡‡é›†æœ€æ–°AIç ”ç©¶è®ºæ–‡
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            ç ”ç©¶è®ºæ–‡åˆ—è¡¨
        """
        log.start(t('dc_collect_research'))
        papers = []
        
        try:
            # ä½¿ç”¨arXiv APIè·å–æœ€æ–°è®ºæ–‡
            client = arxiv.Client()
            
            # æ„å»ºæŸ¥è¯¢ - æœ€æ–°çš„AIç›¸å…³è®ºæ–‡
            search_query = arxiv.Search(
                query="cat:cs.AI OR cat:cs.LG OR cat:cs.CV OR cat:cs.CL",
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            for result in client.results(search_query):
                # è¿‡æ»¤éæœ€è¿‘30å¤©çš„è®ºæ–‡
                if not self._is_recent(result.published):
                    continue
                    
                paper = {
                    'title': result.title,
                    'summary': result.summary[:300] + "..." if len(result.summary) > 300 else result.summary,
                    'authors': [str(author) for author in result.authors],
                    'url': result.entry_id,
                    'published': result.published.strftime('%Y-%m-%d'),
                    'categories': [str(cat) for cat in result.categories],
                    'source': 'arXiv',
                    'category': 'research',
                    'importance': self._calculate_importance(result.title, result.summary)
                }
                papers.append(paper)
                
            log.success(t('dc_got_papers', count=len(papers)))
            
        except Exception as e:
            log.error(t('dc_arxiv_failed', error=str(e)))
            # æä¾›å¤‡ç”¨æ•°æ®
            papers = self._get_backup_research_data()
        
        return papers
    
    def collect_developer_content(self, max_results: int = 15) -> List[Dict]:
        """
        é‡‡é›†å¼€å‘è€…ç¤¾åŒºå†…å®¹
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            å¼€å‘è€…å†…å®¹åˆ—è¡¨
        """
        log.start(t('dc_collect_developer'))
        content = []
        
        # 1. GitHub Trending AIé¡¹ç›®
        github_projects = self._collect_github_trending()
        content.extend(github_projects[:max_results//3])
        
        # 2. Hugging Faceæœ€æ–°æ¨¡å‹/æ•°æ®é›†
        hf_content = self._collect_huggingface_updates()
        content.extend(hf_content[:max_results//3])
        
        # 3. å¼€å‘è€…åšå®¢å’Œæ•™ç¨‹
        dev_blogs = self._collect_dev_blogs()
        content.extend(dev_blogs[:max_results//3])
        
        log.success(t('dc_got_developer', count=len(content)))
        return content
    
    def collect_product_releases(self, max_results: int = 10) -> List[Dict]:
        """
        é‡‡é›†AIäº§å“å‘å¸ƒä¿¡æ¯
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            äº§å“å‘å¸ƒåˆ—è¡¨
        """
        log.start(t('dc_collect_products'))
        products = []
        
        # æ”¶é›†ä¸»è¦AIå…¬å¸çš„äº§å“å‘å¸ƒä¿¡æ¯
        company_sources = {
            'OpenAI': self._collect_openai_updates,
            'Google': self._collect_google_ai_updates,
            'Microsoft': self._collect_microsoft_ai_updates,
            'Meta': self._collect_meta_ai_updates,
            'Anthropic': self._collect_anthropic_updates,
            'China_Tech': self._collect_chinese_ai_updates  # æ–°å¢ä¸­å›½ç§‘æŠ€å…¬å¸
        }
        
        for company, collector_func in company_sources.items():
            try:
                company_products = collector_func()
                # è¿‡æ»¤éæœ€è¿‘å‘å¸ƒçš„äº§å“
                company_products = [p for p in company_products if self._is_recent(p.get('published', ''))]
                products.extend(company_products)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                log.warning(t('dc_product_failed', company=company, error=str(e)))
        
        # æŒ‰é‡è¦æ€§æ’åºå¹¶é™åˆ¶æ•°é‡
        products.sort(key=lambda x: x.get('importance', 0), reverse=True)
        products = products[:max_results]
        
        log.success(t('dc_got_products', count=len(products)))
        return products
    
    def collect_ai_leaders_quotes(self, max_results: int = 15) -> List[Dict]:
        """
        é‡‡é›†å…¨çƒAIé¢†è¢–çš„è¿‘æœŸè¨€è®º
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            é¢†è¢–è¨€è®ºåˆ—è¡¨
        """
        log.start(t('dc_collect_leaders'))
        quotes = []
        
        leaders = {
            "Sam Altman": "OpenAI CEO",
            "Elon Musk": "xAI Founder",
            "Jensen Huang": "NVIDIA CEO",
            "Demis Hassabis": "Google DeepMind CEO",
            "Yann LeCun": "Meta Chief AI Scientist",
            "Geoffrey Hinton": "AI Pioneer",
            "Andrew Ng": "AI Fund Managing General Partner",
            "Kai-Fu Lee": "01.AI CEO",
            "Robin Li": "Baidu CEO"
        }
        
        # 1. å°è¯•ä½¿ç”¨æ–°é—»RSSæœç´¢ (ä¼˜å…ˆBing News)
        base_url_google = "https://news.google.com/rss/search?q={}+AI+when:30d&hl=en-US&gl=US&ceid=US:en"
        base_url_bing = "https://www.bing.com/news/search?q={}+AI&format=rss"
        
        for leader_name, title in leaders.items():
            try:
                query_name = leader_name.replace(' ', '+')
                
                # ç­–ç•¥A: ä¼˜å…ˆä½¿ç”¨ Bing News
                feed_url = base_url_bing.format(query_name)
                feed = feedparser.parse(feed_url)
                
                # ç­–ç•¥B: å¦‚æœ Bing News ä¸ºç©ºï¼Œå°è¯• Google News
                if not feed.entries:
                    feed_url = base_url_google.format(query_name)
                    feed = feedparser.parse(feed_url)
                
                count = 0
                for entry in feed.entries:
                    if count >= 2: # æ¯ä¸ªé¢†è¢–æœ€å¤šå–2æ¡
                        break
                        
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€è¿‘30å¤©
                    date_val = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date_val = entry.published_parsed
                    elif entry.get('published'):
                        date_val = entry.get('published')
                    
                    if date_val and not self._is_recent(date_val):
                        continue
                        
                    # ç®€å•çš„å…³é”®è¯è¿‡æ»¤ï¼Œç¡®ä¿æ˜¯è¨€è®ºç›¸å…³çš„
                    text = (entry.title + " " + entry.get('summary', '')).lower()
                    if any(k in text for k in ['said', 'says', 'stated', 'warns', 'believes', 'predicts', 'interview', 'speech', 'tweet', 'post']):
                        quote = {
                            'title': f"{leader_name}: {entry.title}",
                            'summary': entry.get('summary', entry.title)[:300],
                            'url': entry.link,
                            'published': entry.get('published', datetime.now().strftime('%Y-%m-%d')),
                            'source': f"News about {leader_name}",
                            'category': 'leader',
                            'author': leader_name,
                            'author_title': title,
                            'importance': 0.95
                        }
                        quotes.append(quote)
                        count += 1
                
                time.sleep(0.5) # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                log.warning(t('dc_leader_failed', name=leader_name, error=str(e)))
        
        # 2. é‡‡é›†ä¸ªäººåšå®¢å’Œæ’­å®¢
        log.info(t('dc_collect_blogs'), emoji="ğŸ“")
        for source in self.rss_feeds.get('leader_blogs', []):
            try:
                feed = feedparser.parse(source['url'])
                for entry in feed.entries[:3]:
                    # æ£€æŸ¥æ—¶é—´
                    date_val = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date_val = entry.published_parsed
                    elif entry.get('published'):
                        date_val = entry.get('published')
                    
                    if date_val and not self._is_recent(date_val):
                        continue

                    # å¦‚æœæ˜¯æ’­å®¢ï¼Œæ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³æ³¨çš„é¢†è¢–åå­—
                    if source.get('type') == 'podcast':
                        found_leader = False
                        for leader_name in leaders.keys():
                            if leader_name.lower() in entry.title.lower():
                                found_leader = True
                                source['author'] = leader_name # ä¸´æ—¶è¦†ç›–ä¸ºå˜‰å®¾å
                                break
                        if not found_leader:
                            continue

                    quote = {
                        'title': f"[{source['author']}] {entry.title}",
                        'summary': entry.get('summary', entry.get('description', ''))[:300],
                        'url': entry.link,
                        'published': entry.get('published', datetime.now().strftime('%Y-%m-%d')),
                        'source': 'Personal Blog/Podcast',
                        'category': 'leader',
                        'author': source['author'],
                        'author_title': source['title'],
                        'importance': 1.0 # ä¸ªäººåšå®¢å†…å®¹æƒé‡æœ€é«˜
                    }
                    quotes.append(quote)
            except Exception as e:
                log.warning(t('dc_blog_failed', author=source['author'], error=str(e)))

        # 3. å¦‚æœé‡‡é›†æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®
        if len(quotes) < 5:
            log.warning(t('dc_fallback_data'))
            quotes.extend(self._get_backup_leaders_data())
            
        # å»é‡
        unique_quotes = []
        seen_urls = set()
        for q in quotes:
            if q['url'] not in seen_urls:
                unique_quotes.append(q)
                seen_urls.add(q['url'])
        
        # æŒ‰æ—¶é—´æ’åº
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦è§£ææ—¶é—´å­—ç¬¦ä¸²
        
        result = unique_quotes[:max_results]
        log.success(t('dc_got_leaders', count=len(result)))
        return result

    def collect_latest_news(self, max_results: int = 20) -> List[Dict]:
        """
        é‡‡é›†æœ€æ–°AIè¡Œä¸šæ–°é—»
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        log.start(t('dc_collect_news'))
        
        # ä»äº§å“å‘å¸ƒæ–°é—»æºé‡‡é›†
        product_news = []
        for feed_url in self.rss_feeds.get('product_news', []):
            try:
                feed_news = self._parse_rss_feed(feed_url, category='product')
                product_news.extend(feed_news)
                time.sleep(0.3)
            except Exception as e:
                log.warning(t('dc_product_feed_failed', url=feed_url, error=str(e)))
        
        # ä»ä¼ ç»Ÿæ–°é—»æºé‡‡é›†
        general_news = []
        for feed_url in self.rss_feeds['news']:
            try:
                feed_news = self._parse_rss_feed(feed_url, category='news')
                general_news.extend(feed_news)
                time.sleep(0.5)
            except Exception as e:
                log.warning(t('dc_rss_failed', url=feed_url, error=str(e)))
        
        # åˆå¹¶ä¸¤ç±»æ–°é—»
        all_news = product_news + general_news
        
        # è¿‡æ»¤AIç›¸å…³å†…å®¹
        ai_news = [item for item in all_news if self._is_ai_related(item)]
        
        # å…¨å±€å»é‡ - æé«˜ä¿¡å™ªæ¯”
        ai_news = self._deduplicate_items(ai_news)
        
        # æŒ‰æ—¶é—´æ’åº
        ai_news.sort(key=lambda x: x.get('published', ''), reverse=True)
        
        # ä¼˜å…ˆæ˜¾ç¤ºäº§å“å‘å¸ƒæ–°é—»
        product_related = [item for item in ai_news if self._is_product_related(item)]
        other_news = [item for item in ai_news if not self._is_product_related(item)]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åˆ—ï¼šäº§å“å‘å¸ƒ > å…¶ä»–AIæ–°é—»
        prioritized_news = product_related + other_news
        result = prioritized_news[:max_results]
        log.success(t('dc_got_news', count=len(result)))
        return result
    
    def collect_community_trends(self, max_results: int = 15) -> List[Dict]:
        """
        é‡‡é›†ç¤¾åŒºçƒ­ç‚¹ (Product Hunt, Hacker News, Reddit)
        """
        log.start(t('dc_collect_community'))
        trends = []
        
        for feed_url in self.rss_feeds.get('community', []):
            try:
                # Determine source name for better labeling
                source_name = "Community"
                if "producthunt" in feed_url:
                    source_name = "Product Hunt"
                elif "hnrss" in feed_url:
                    source_name = "Hacker News"
                elif "reddit" in feed_url:
                    if "LocalLLaMA" in feed_url:
                        source_name = "Reddit (LocalLLaMA)"
                    else:
                        source_name = "Reddit (Singularity)"
                elif "lmsys" in feed_url:
                    source_name = "LMSYS Arena"

                feed_items = self._parse_rss_feed(feed_url, category='community')
                
                for item in feed_items:
                    item['source'] = source_name
                    # Boost importance for these high-signal sources
                    item['importance'] = item.get('importance', 0.6) + 0.1
                    trends.append(item)
                    time.sleep(0.2)
                    
            except Exception as e:
                log.warning(t('dc_community_failed', url=feed_url, error=str(e)))
        
        # Deduplicate
        trends = self._deduplicate_items(trends)
        
        # Sort by published date
        trends.sort(key=lambda x: x.get('published', ''), reverse=True)
        
        result = trends[:max_results]
        log.success(t('dc_got_community', count=len(result)))
        return result

    def collect_all(self) -> Dict[str, List[Dict]]:
        """
        é‡‡é›†æ‰€æœ‰ç±»å‹çš„æ•°æ®
        
        Returns:
            åˆ†ç±»çš„æ•°æ®å­—å…¸
        """
        log.start(t('dc_start_collection'))
        log.info("=" * 50, emoji="ğŸ“Š")
        
        all_data = {
            'research': [],
            'developer': [],
            'product': [],
            'news': [],
            'leader': [],
            'community': []
        }
        
        # ä»é…ç½®è¯»å–é‡‡é›†æ•°é‡
        product_count = config.get('collector.product_count', 10)
        community_count = config.get('collector.community_count', 10)
        leader_count = config.get('collector.leader_count', 15)
        research_count = config.get('collector.research_count', 15)
        developer_count = config.get('collector.developer_count', 20)
        news_count = config.get('collector.news_count', 25)

        all_data['research'] = self.collect_research_papers(research_count)
        all_data['developer'] = self.collect_developer_content(developer_count)
        all_data['product'] = self.collect_product_releases(product_count)
        all_data['leader'] = self.collect_ai_leaders_quotes(leader_count)
        all_data['community'] = self.collect_community_trends(community_count)
        all_data['news'] = self.collect_latest_news(news_count)
        
        # ä½¿ç”¨ç‹¬ç«‹çš„é‡‡é›†å†å²ç¼“å­˜ç»Ÿè®¡æ–°å†…å®¹ï¼ˆä½†ä¸è¿‡æ»¤ï¼Œæ‰€æœ‰å†…å®¹éƒ½ä¼ é€’ç»™åˆ†ç±»æ¨¡å—ï¼‰
        new_stats = {}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„æ–°å†…å®¹æ•°é‡
        cached_stats = {}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç¼“å­˜å‘½ä¸­æ•°é‡
        new_items_for_cache = []  # è®°å½•æ–°é‡‡é›†çš„é¡¹ç›®ï¼Œç¨åæ·»åŠ åˆ°ç¼“å­˜
        
        for cat in all_data:
            new_count = 0
            cached_count = 0
            for item in all_data[cat]:
                if self._is_in_history(item):
                    cached_count += 1
                else:
                    new_count += 1
                    new_items_for_cache.append(item)
            new_stats[cat] = new_count
            cached_stats[cat] = cached_count
        
        # å°†æ–°é‡‡é›†çš„é¡¹ç›®æ·»åŠ åˆ°å†å²ç¼“å­˜
        for item in new_items_for_cache:
            self._add_to_history(item)
        
        # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
        if new_items_for_cache:
            self._save_history_cache()
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_items = sum(len(items) for items in all_data.values())
        total_new = sum(new_stats.values())
        total_cached = sum(cached_stats.values())
        log.done(t('dc_collection_done_v2', total=total_items, new=total_new, cached=total_cached))
        for category, items in all_data.items():
            new_count = new_stats.get(category, 0)
            cached_count = cached_stats.get(category, 0)
            log.data(t('dc_category_stats_v2', category=category, count=len(items), new=new_count, cached=cached_count))
        
        return all_data
    
    def _collect_github_trending(self) -> List[Dict]:
        """é‡‡é›†GitHub AIçƒ­é—¨é¡¹ç›® (å…³æ³¨è¿‘æœŸçƒ­é—¨)"""
        projects = []
        
        try:
            # è®¡ç®—30å¤©å‰çš„æ—¥æœŸ
            last_month = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            
            # GitHub APIè·å–AIç›¸å…³çƒ­é—¨é¡¹ç›®
            url = "https://api.github.com/search/repositories"
            # ä¼˜åŒ–æŸ¥è¯¢: å…³æ³¨æœ€è¿‘åˆ›å»ºä¸”é«˜æ˜Ÿçš„é¡¹ç›®ï¼Œå‘ç°"æ˜æ—¥ä¹‹æ˜Ÿ"
            query = f'(machine-learning OR artificial-intelligence OR deep-learning OR llm) created:>{last_month}'
            
            params = {
                'q': query,
                'sort': 'stars',
                'order': 'desc',
                'per_page': 15
            }
            
            response = requests.get(url, params=params, headers=self.headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                
                for repo in data.get('items', []):
                    # è¿‡æ»¤éæœ€è¿‘æ›´æ–°çš„é¡¹ç›®
                    if not self._is_recent(repo['updated_at']):
                        continue
                        
                    project = {
                        'title': repo['full_name'],
                        'summary': repo['description'] or 'æ— æè¿°',
                        'url': repo['html_url'],
                        'stars': repo['stargazers_count'],
                        'language': repo['language'],
                        'updated': repo['updated_at'][:10],
                        'source': 'GitHub',
                        'category': 'developer',
                        'importance': min(repo['stargazers_count'] / 1000, 1.0)
                    }
                    projects.append(project)
            
        except Exception as e:
            log.warning(t('dc_github_failed', error=str(e)))
            # ä½¿ç”¨å¤‡ç”¨æ•°æ®
            projects = self._get_backup_github_data()
        
        return projects
    
    def _collect_huggingface_updates(self) -> List[Dict]:
        """é‡‡é›†Hugging Faceæœ€æ–°æ›´æ–°"""
        updates = []
        
        try:
            # Hugging Faceæ¨¡å‹API
            url = "https://huggingface.co/api/models"
            params = {
                'limit': 10,
                'sort': 'lastModified',
                'direction': -1
            }
            
            response = requests.get(url, params=params, headers=self.headers, timeout=10)
            if response.status_code == 200:
                models = response.json()
                
                for model in models:
                    # è¿‡æ»¤éæœ€è¿‘æ›´æ–°çš„æ¨¡å‹
                    if not self._is_recent(model.get('lastModified', '')):
                        continue
                        
                    update = {
                        'title': f"HF Model: {model['id']}",
                        'summary': f"æœ€æ–°AIæ¨¡å‹å‘å¸ƒ: {model['id']}ï¼Œä¸‹è½½é‡: {model.get('downloads', 0)}",
                        'url': f"https://huggingface.co/{model['id']}",
                        'downloads': model.get('downloads', 0),
                        'updated': model.get('lastModified', '')[:10],
                        'source': 'Hugging Face',
                        'category': 'developer',
                        'importance': min(model.get('downloads', 0) / 10000, 1.0)
                    }
                    updates.append(update)
        
        except Exception as e:
            log.warning(t('dc_hf_failed', error=str(e)))
            updates = self._get_backup_hf_data()
        
        return updates
    
    def _collect_dev_blogs(self) -> List[Dict]:
        """é‡‡é›†å¼€å‘è€…åšå®¢å†…å®¹"""
        blogs = []
        
        try:
            # ä»GitHubåšå®¢RSSè·å–
            for feed_url in self.rss_feeds['developer']:
                feed_content = self._parse_rss_feed(feed_url, category='developer')
                blogs.extend(feed_content)
        
        except Exception as e:
            log.warning(t('dc_dev_blog_failed', error=str(e)))
            blogs = self._get_backup_blog_data()
        
        return blogs
    
    def _collect_openai_updates(self) -> List[Dict]:
        """é‡‡é›†OpenAIäº§å“æ›´æ–°"""
        updates = []
        try:
            # å°è¯•ä»RSSè·å–
            rss_url = 'https://openai.com/blog/rss.xml'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'OpenAI'
                item['importance'] = 0.95 # OpenAIæ–°é—»é€šå¸¸å¾ˆé‡è¦
        except Exception:
            pass
            
        if updates:
            return updates
            
        # å¤‡ç”¨æ•°æ®
        return [
            {
                'title': 'OpenAI ChatGPT-4o å‘å¸ƒå…¬å‘Š',
                'summary': 'OpenAIæ­£å¼å‘å¸ƒChatGPT-4oï¼Œå…·å¤‡æ›´å¼ºçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„ç»¼åˆå¤„ç†ï¼Œå“åº”é€Ÿåº¦æ˜¾è‘—æå‡ã€‚',
                'url': 'https://openai.com/index/hello-gpt-4o/',
                'company': 'OpenAI',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'OpenAI',
                'category': 'product',
                'importance': 0.95
            },
            {
                'title': 'OpenAI API å®šä»·æ›´æ–°å…¬å‘Š',
                'summary': 'OpenAIæ›´æ–°APIå®šä»·ç­–ç•¥ï¼Œé™ä½GPT-4ä½¿ç”¨æˆæœ¬ï¼ŒåŒæ—¶æ¨å‡ºæ›´ç»æµçš„GPT-4 Turboé€‰é¡¹ï¼Œä¸ºå¼€å‘è€…æä¾›æ›´çµæ´»çš„é€‰æ‹©ã€‚',
                'url': 'https://openai.com/api/pricing/',
                'company': 'OpenAI',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'OpenAI',
                'category': 'product',
                'importance': 0.88
            }
        ]
    
    def _collect_google_ai_updates(self) -> List[Dict]:
        """é‡‡é›†Google AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://blog.google/technology/ai/rss/'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Google'
                item['importance'] = 0.92
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Google Gemini äº§å“ä»‹ç»é¡µé¢',
                'summary': 'Google Geminiæ˜¯ä¸‹ä¸€ä»£AIæ¨¡å‹ï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ–‡æœ¬ã€ä»£ç ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„ç»¼åˆå¤„ç†ã€‚',
                'url': 'https://gemini.google.com/',
                'company': 'Google',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Google AI',
                'category': 'product',
                'importance': 0.92
            },
            {
                'title': 'Google AI Studio äº§å“å‘å¸ƒ',
                'summary': 'Google AI Studioä¸ºå¼€å‘è€…æä¾›å¿«é€ŸåŸå‹è®¾è®¡å’Œæµ‹è¯•ç”Ÿæˆå¼AIæƒ³æ³•çš„å¹³å°ï¼Œæ”¯æŒGeminiæ¨¡å‹çš„å¿«é€Ÿé›†æˆå’Œéƒ¨ç½²ã€‚',
                'url': 'https://aistudio.google.com/',
                'company': 'Google',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Google AI',
                'category': 'product',
                'importance': 0.85
            }
        ]
    
    def _collect_microsoft_ai_updates(self) -> List[Dict]:
        """é‡‡é›†Microsoft AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://blogs.microsoft.com/ai/feed/'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Microsoft'
                item['importance'] = 0.90
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Microsoft Copilot äº§å“é¡µé¢',
                'summary': 'Microsoft Copilotæ˜¯AIé©±åŠ¨çš„ç”Ÿäº§åŠ›å·¥å…·ï¼Œé›†æˆåˆ°Microsoft 365ä¸­ï¼Œå¸®åŠ©ç”¨æˆ·æå‡å·¥ä½œæ•ˆç‡ï¼Œæ”¯æŒæ–‡æ¡£ç¼–å†™ã€æ•°æ®åˆ†æç­‰åŠŸèƒ½ã€‚',
                'url': 'https://copilot.microsoft.com/',
                'company': 'Microsoft',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Microsoft',
                'category': 'product',
                'importance': 0.90
            },
            {
                'title': 'Azure AI Services äº§å“ä»‹ç»',
                'summary': 'Azure AI Servicesæä¾›å®Œæ•´çš„AIå’Œæœºå™¨å­¦ä¹ æœåŠ¡å¥—ä»¶ï¼ŒåŒ…æ‹¬è®¤çŸ¥æœåŠ¡ã€æœºå™¨å­¦ä¹ å¹³å°å’ŒOpenAIæœåŠ¡ï¼Œä¸ºä¼ä¸šAIè½¬å‹æä¾›æ”¯æŒã€‚',
                'url': 'https://azure.microsoft.com/en-us/products/ai-services',
                'company': 'Microsoft',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Microsoft Azure',
                'category': 'product',
                'importance': 0.88
            }
        ]
    
    def _collect_meta_ai_updates(self) -> List[Dict]:
        """é‡‡é›†Meta AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://ai.meta.com/blog/rss/'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Meta'
                item['importance'] = 0.90
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Meta Llama 3.3 æ¨¡å‹å‘å¸ƒå…¬å‘Š',
                'summary': 'Metaå‘å¸ƒLlama 3.3ï¼Œè¿™æ˜¯æœ€æ–°çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šè¯­è¨€æ”¯æŒæ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œæ”¯æŒå•†ä¸šä½¿ç”¨ã€‚',
                'url': 'https://llama.meta.com/',
                'company': 'Meta',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Meta AI',
                'category': 'product',
                'importance': 0.90
            },
            {
                'title': 'Meta AI Assistant äº§å“ä»‹ç»',
                'summary': 'Meta AIæ˜¯æ™ºèƒ½åŠ©æ‰‹äº§å“ï¼Œé›†æˆåˆ°Facebookã€Instagramã€WhatsAppç­‰å¹³å°ï¼Œä¸ºç”¨æˆ·æä¾›AIé©±åŠ¨çš„å¯¹è¯ã€åˆ›ä½œå’Œæœç´¢ä½“éªŒã€‚',
                'url': 'https://www.meta.ai/',
                'company': 'Meta',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Meta AI',
                'category': 'product',
                'importance': 0.85
            }
        ]
    
    def _collect_anthropic_updates(self) -> List[Dict]:
        """é‡‡é›†Anthropic AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://www.anthropic.com/news/rss'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Anthropic'
                item['importance'] = 0.88
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Anthropic Claude 3.5 Sonnet äº§å“é¡µé¢',
                'summary': 'Claude 3.5 Sonnetæ˜¯Anthropicæœ€æ–°çš„AIæ¨¡å‹ï¼Œåœ¨æ¨ç†ã€åˆ†æã€ç¼–ç ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒå¤§å®¹é‡ä¸Šä¸‹æ–‡å¤„ç†ï¼Œå…·å¤‡å¼ºå¤§çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚',
                'url': 'https://www.anthropic.com/claude',
                'company': 'Anthropic',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Anthropic',
                'category': 'product',
                'importance': 0.88
            },
            {
                'title': 'Anthropic Claude API æ–‡æ¡£',
                'summary': 'Anthropicæä¾›Claude APIæœåŠ¡ï¼Œä¸ºå¼€å‘è€…æä¾›é«˜è´¨é‡çš„å¯¹è¯AIèƒ½åŠ›ï¼Œæ”¯æŒå¤šç§ä½¿ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬å†…å®¹åˆ›ä½œã€åˆ†æå’Œç¼–ç¨‹è¾…åŠ©ç­‰ã€‚',
                'url': 'https://docs.anthropic.com/',
                'company': 'Anthropic',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Anthropic',
                'category': 'product',
                'importance': 0.82
            }
        ]
    
    def _collect_chinese_ai_updates(self) -> List[Dict]:
        """é‡‡é›†ä¸­å›½AIå…¬å¸äº§å“æ›´æ–°"""
        updates = []
        
        # 1. å°è¯•ä»RSSè·å–
        chinese_feeds = [
            'https://www.jiqizhixin.com/rss',
            'https://www.qbitai.com/feed',
            'https://www.infoq.cn/feed/topic/18',
            'https://www.baidu.com/rss/news.xml',
            'https://cloud.tencent.com/developer/rss/articles',
            'https://www.alibabacloud.com/blog/rss.xml'
        ]
        
        for feed_url in chinese_feeds:
            try:
                feed_updates = self._parse_rss_feed(feed_url, category='product')
                # è¿‡æ»¤å‡ºå¤§å…¬å¸çš„äº§å“æ–°é—»
                for item in feed_updates:
                    if any(c in item['title'] for c in ['ç™¾åº¦', 'é˜¿é‡Œ', 'è…¾è®¯', 'åä¸º', 'å­—èŠ‚', 'æ–‡å¿ƒä¸€è¨€', 'é€šä¹‰åƒé—®', 'æ··å…ƒ', 'ç›˜å¤', 'Kimi', 'æ™ºè°±', 'DeepSeek']):
                        item['company'] = 'China Tech'
                        item['importance'] = 0.9
                        updates.append(item)
            except Exception:
                continue
                
        if updates:
            return updates

        # 2. å¤‡ç”¨æ•°æ® (å¦‚æœRSSå¤±è´¥)
        return [
            {
                'title': 'ç™¾åº¦æ–‡å¿ƒä¸€è¨€ 4.0 å‘å¸ƒ',
                'summary': 'ç™¾åº¦å‘å¸ƒæ–‡å¿ƒä¸€è¨€4.0ç‰ˆæœ¬ï¼Œåœ¨ç†è§£ã€ç”Ÿæˆã€é€»è¾‘å’Œè®°å¿†å››å¤§èƒ½åŠ›ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼Œç»¼åˆæ°´å¹³ä¸GPT-4ç›¸æ¯”æ¯«ä¸é€Šè‰²ã€‚',
                'url': 'https://yiyan.baidu.com/',
                'company': 'Baidu',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Baidu AI',
                'category': 'product',
                'importance': 0.92
            },
            {
                'title': 'é˜¿é‡Œé€šä¹‰åƒé—® 2.5 å‘å¸ƒ',
                'summary': 'é˜¿é‡Œäº‘å‘å¸ƒé€šä¹‰åƒé—®2.5ï¼Œæ¨¡å‹æ€§èƒ½å…¨é¢å‡çº§ï¼Œåœ¨ä¸­æ–‡è¯­å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¼€æºå¤šæ¬¾å°ºå¯¸æ¨¡å‹ä¾›å¼€å‘è€…ä½¿ç”¨ã€‚',
                'url': 'https://tongyi.aliyun.com/',
                'company': 'Alibaba',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Aliyun',
                'category': 'product',
                'importance': 0.90
            },
             {
                'title': 'è…¾è®¯æ··å…ƒå¤§æ¨¡å‹å‡çº§',
                'summary': 'è…¾è®¯æ··å…ƒå¤§æ¨¡å‹è¿æ¥é‡è¦å‡çº§ï¼Œæ‰©å±•äº†ä¸Šä¸‹æ–‡çª—å£ï¼Œå¢å¼ºäº†ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå·²æ¥å…¥è…¾è®¯å…¨ç³»äº§å“ã€‚',
                'url': 'https://hunyuan.tencent.com/',
                'company': 'Tencent',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Tencent Cloud',
                'category': 'product',
                'importance': 0.88
            },
            {
                'title': 'DeepSeek V2 å¼€æºå‘å¸ƒ',
                'summary': 'æ·±åº¦æ±‚ç´¢(DeepSeek)å‘å¸ƒDeepSeek-V2ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„å¼€æºMoEå¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ¨ç†æˆæœ¬æä½ã€‚',
                'url': 'https://www.deepseek.com/',
                'company': 'DeepSeek',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'DeepSeek',
                'category': 'product',
                'importance': 0.95
            }
        ]
    
    def _parse_rss_feed(self, feed_url: str, category: str) -> List[Dict]:
        """è§£æRSSæº"""
        items = []
        
        try:
            feed = feedparser.parse(feed_url)
            
            for entry in feed.entries[:10]:  # é™åˆ¶æ¯ä¸ªæºæœ€å¤š10æ¡
                # æ£€æŸ¥æ—¥æœŸ
                date_val = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    date_val = entry.published_parsed
                elif entry.get('published'):
                    date_val = entry.get('published')
                
                if date_val and not self._is_recent(date_val):
                    continue

                item = {
                    'title': entry.get('title', ''),
                    'summary': entry.get('summary', entry.get('description', ''))[:300] + "...",
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'source': feed.feed.get('title', feed_url),
                    'category': category,
                    'importance': 0.6
                }
                
                if self._is_valid_item(item):
                    items.append(item)
        
        except Exception as e:
            log.warning(t('dc_rss_parse_failed', url=feed_url, error=str(e)))
        
        return items
    
    def _deduplicate_items(self, items: List[Dict], threshold: float = 0.6) -> List[Dict]:
        """
        å¯¹å†…å®¹åˆ—è¡¨è¿›è¡Œå»é‡
        åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦
        """
        if not items:
            return []
            
        unique_items = []
        # æŒ‰é‡è¦æ€§æ’åºï¼Œä¿ç•™é‡è¦çš„
        sorted_items = sorted(items, key=lambda x: x.get('importance', 0), reverse=True)
        
        for item in sorted_items:
            is_duplicate = False
            for existing in unique_items:
                # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
                seq = difflib.SequenceMatcher(None, item['title'].lower(), existing['title'].lower())
                if seq.ratio() > threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_items.append(item)
                
        return unique_items

    def _is_ai_related(self, item: Dict) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸AIç›¸å…³"""
        ai_keywords = [
            'ai', 'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'llm', 'gpt', 'transformer', 'chatgpt',
            'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ'
        ]
        
        text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
        return any(keyword in text for keyword in ai_keywords)
    
    def _is_product_related(self, item: Dict) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸äº§å“å‘å¸ƒç›¸å…³"""
        product_keywords = [
            'launch', 'release', 'announce', 'unveil', 'introduce', 'debut',
            'new product', 'new version', 'update', 'upgrade', 'available',
            'official', 'beta', 'preview', 'api', 'service', 'platform',
            'å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿', 'æ­£å¼', 'æ–°ç‰ˆæœ¬', 'æ–°åŠŸèƒ½',
            'äº§å“', 'æœåŠ¡', 'å¹³å°', 'å…¬æµ‹', 'å†…æµ‹'
        ]
        
        text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
        return any(keyword in text for keyword in product_keywords)
    
    def _is_valid_item(self, item: Dict) -> bool:
        """éªŒè¯æ•°æ®é¡¹æœ‰æ•ˆæ€§"""
        return (item.get('title') and 
                item.get('url') and 
                len(item.get('title', '')) > 10)
    
    def _is_recent(self, date_val) -> bool:
        """æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘30å¤©å†…"""
        try:
            cutoff_date = datetime.now() - timedelta(days=30)
            
            if isinstance(date_val, datetime):
                # å¤„ç†æ—¶åŒºæ„ŸçŸ¥çš„æ—¶é—´
                if date_val.tzinfo is not None:
                    # å¦‚æœcutoff_dateæ²¡æœ‰æ—¶åŒºï¼Œæ·»åŠ å½“å‰æ—¶åŒºæˆ–UTC
                    if cutoff_date.tzinfo is None:
                        from dateutil import tz
                        cutoff_date = cutoff_date.replace(tzinfo=tz.tzlocal())
                    
                    # å†æ¬¡æ£€æŸ¥ï¼Œå¦‚æœè¿˜æ˜¯ä¸åŒ¹é…ï¼Œå°è¯•è½¬æ¢
                    if date_val.tzinfo != cutoff_date.tzinfo:
                         # ç®€å•æ¯”è¾ƒæ—¶é—´æˆ³
                         return date_val.timestamp() >= cutoff_date.timestamp()
                return date_val >= cutoff_date
                
            if isinstance(date_val, str):
                # å°è¯•è§£æå­—ç¬¦ä¸²æ—¥æœŸ
                try:
                    dt = parser.parse(date_val)
                    # æ¯”è¾ƒæ—¶é—´æˆ³ä»¥é¿å…æ—¶åŒºé—®é¢˜
                    return dt.timestamp() >= cutoff_date.timestamp()
                except (ValueError, TypeError, AttributeError):
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•æ ¼å¼
                    if len(date_val) >= 10:
                        dt = datetime.strptime(date_val[:10], '%Y-%m-%d')
                        return dt >= cutoff_date
            
            # å¦‚æœæ˜¯struct_time (feedparser)
            if isinstance(date_val, time.struct_time):
                dt = datetime.fromtimestamp(time.mktime(date_val))
                return dt >= cutoff_date
                
            return True # æ— æ³•è§£ææ—¶é»˜è®¤ä¿ç•™
        except Exception:
            return True

    def _calculate_importance(self, title: str, summary: str) -> float:
        """è®¡ç®—å†…å®¹é‡è¦æ€§"""
        text = f"{title} {summary}".lower()
        
        high_value_keywords = [
            'breakthrough', 'new', 'launch', 'release', 'breakthrough',
            'çªç ´', 'å‘å¸ƒ', 'æ–°', 'æœ€æ–°'
        ]
        
        score = 0.5  # åŸºç¡€åˆ†æ•°
        for keyword in high_value_keywords:
            if keyword in text:
                score += 0.1
        
        return min(score, 1.0)
    
    def _get_backup_leaders_data(self) -> List[Dict]:
        """å¤‡ç”¨é¢†è¢–è¨€è®ºæ•°æ®"""
        return [
            {
                'title': 'Sam Altman: AIå‘å±•çš„é€Ÿåº¦å°†è¶…å‡ºæ‰€æœ‰äººçš„é¢„æœŸ',
                'summary': 'OpenAI CEO Sam Altmanåœ¨æœ€è¿‘çš„é‡‡è®¿ä¸­è¡¨ç¤ºï¼ŒAGIçš„åˆ°æ¥å¯èƒ½æ¯”é¢„æœŸçš„è¦å¿«ï¼Œç¤¾ä¼šéœ€è¦ä¸ºæ­¤åšå¥½å‡†å¤‡ã€‚',
                'url': 'https://openai.com/blog',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Interview',
                'category': 'leader',
                'author': 'Sam Altman',
                'author_title': 'OpenAI CEO',
                'importance': 0.98
            },
            {
                'title': 'Elon Musk: AIå®‰å…¨æ˜¯æœªæ¥çš„é¦–è¦ä»»åŠ¡',
                'summary': 'Elon Muskå†æ¬¡å¼ºè°ƒAIå®‰å…¨çš„é‡è¦æ€§ï¼Œå¹¶è¡¨ç¤ºxAIçš„ç›®æ ‡æ˜¯ç†è§£å®‡å®™çš„æœ¬è´¨ï¼Œæ„å»ºæœ€å¤§é™åº¦è¿½æ±‚çœŸç†çš„AIã€‚',
                'url': 'https://x.ai',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'X (Twitter)',
                'category': 'leader',
                'author': 'Elon Musk',
                'author_title': 'xAI Founder',
                'importance': 0.95
            },
            {
                'title': 'Jensen Huang: ç”Ÿæˆå¼AIæ˜¯è®¡ç®—é¢†åŸŸçš„è½¬æŠ˜ç‚¹',
                'summary': 'NVIDIA CEOé»„ä»å‹‹è¡¨ç¤ºï¼Œç”Ÿæˆå¼AIæ­£åœ¨é‡å¡‘æ¯ä¸€ä¸ªè¡Œä¸šï¼Œè®¡ç®—æ–¹å¼æ­£åœ¨å‘ç”Ÿæ ¹æœ¬æ€§çš„è½¬å˜ã€‚',
                'url': 'https://nvidianews.nvidia.com/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Keynote',
                'category': 'leader',
                'author': 'Jensen Huang',
                'author_title': 'NVIDIA CEO',
                'importance': 0.92
            },
            {
                'title': 'Yann LeCun: ç°åœ¨çš„LLMè¿˜ä¸æ˜¯çœŸæ­£çš„æ™ºèƒ½',
                'summary': 'Metaé¦–å¸­AIç§‘å­¦å®¶Yann LeCunè®¤ä¸ºï¼Œç›®å‰çš„å¤§è¯­è¨€æ¨¡å‹ç¼ºä¹å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ï¼Œè·ç¦»çœŸæ­£çš„é€šç”¨äººå·¥æ™ºèƒ½è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚',
                'url': 'https://ai.meta.com/blog/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Interview',
                'category': 'leader',
                'author': 'Yann LeCun',
                'author_title': 'Meta Chief AI Scientist',
                'importance': 0.90
            },
            {
                'title': 'æå¼€å¤: AI 2.0æ—¶ä»£å·²ç»åˆ°æ¥',
                'summary': 'é›¶ä¸€ä¸‡ç‰©CEOæå¼€å¤è¡¨ç¤ºï¼ŒAI 2.0æ—¶ä»£å°†å¸¦æ¥æ¯”ç§»åŠ¨äº’è”ç½‘å¤§åå€çš„æœºä¼šï¼Œä¸­å›½åœ¨åº”ç”¨å±‚æœ‰å·¨å¤§ä¼˜åŠ¿ã€‚',
                'url': 'https://www.01.ai/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Speech',
                'category': 'leader',
                'author': 'Kai-Fu Lee',
                'author_title': '01.AI CEO',
                'importance': 0.88
            }
        ]

    def _get_backup_research_data(self) -> List[Dict]:
        """å¤‡ç”¨ç ”ç©¶æ•°æ®"""
        return [
            {
                'title': 'Attention Is All You Need: Transformeræ¶æ„æ·±åº¦åˆ†æ',
                'summary': 'æ·±å…¥åˆ†æTransformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é©å‘½æ€§ä½œç”¨ï¼Œæ¢è®¨æ³¨æ„åŠ›æœºåˆ¶çš„åŸç†å’Œåº”ç”¨ã€‚',
                'authors': ['AI Research Team'],
                'url': 'https://arxiv.org/abs/1706.03762',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'categories': ['cs.CL', 'cs.AI'],
                'source': 'arXiv',
                'category': 'research',
                'importance': 0.95
            }
        ]
    
    def _get_backup_github_data(self) -> List[Dict]:
        """å¤‡ç”¨GitHubæ•°æ®"""
        return [
            {
                'title': 'transformers',
                'summary': 'ğŸ¤— Transformers: State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX.',
                'url': 'https://github.com/huggingface/transformers',
                'stars': 132000,
                'language': 'Python',
                'updated': datetime.now().strftime('%Y-%m-%d'),
                'source': 'GitHub',
                'category': 'developer',
                'importance': 0.98
            }
        ]
    
    def _get_backup_hf_data(self) -> List[Dict]:
        """å¤‡ç”¨Hugging Faceæ•°æ®"""
        return [
            {
                'title': 'HF Model: microsoft/DialoGPT-medium',
                'summary': 'æœ€æ–°AIæ¨¡å‹å‘å¸ƒ: microsoft/DialoGPT-mediumï¼Œä¸‹è½½é‡: 1500000',
                'url': 'https://huggingface.co/microsoft/DialoGPT-medium',
                'downloads': 1500000,
                'updated': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Hugging Face',
                'category': 'developer',
                'importance': 0.85
            }
        ]
    
    def _get_backup_blog_data(self) -> List[Dict]:
        """å¤‡ç”¨åšå®¢æ•°æ®"""
        return [
            {
                'title': 'GitHub Copilotæœ€æ–°åŠŸèƒ½æ›´æ–°',
                'summary': 'GitHub Copilotæ¨å‡ºæ–°åŠŸèƒ½ï¼Œæ”¯æŒæ›´å¤šç¼–ç¨‹è¯­è¨€å’Œæ›´æ™ºèƒ½çš„ä»£ç å»ºè®®ï¼Œæå‡å¼€å‘æ•ˆç‡ã€‚',
                'url': 'https://github.blog',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'GitHub Blog',
                'category': 'developer',
                'importance': 0.80
            }
        ]


# ç”¨äºå‘åå…¼å®¹
DataCollector = AIDataCollector

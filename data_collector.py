"""
AIä¸–ç•Œè¿½è¸ªå™¨ - æ•°æ®é‡‡é›†æ¨¡å—
ä¸“æ³¨äºæ”¶é›†æœ€æ–°AIç ”ç©¶ã€äº§å“ã€å¼€å‘è€…ç¤¾åŒºå’Œè¡Œä¸šä¿¡æ¯

æ”¯æŒä¸¤ç§æ¨¡å¼:
- åŒæ­¥æ¨¡å¼ (ThreadPoolExecutor): å…¼å®¹æ—§ä»£ç 
- å¼‚æ­¥æ¨¡å¼ (asyncio + aiohttp): é«˜æ€§èƒ½é‡‡é›†

ä½¿ç”¨æ–¹å¼:
    # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å¼
    collector = DataCollector()
    data = collector.collect_all()
    
    # å¼ºåˆ¶ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
    collector = DataCollector(async_mode=True)
    data = collector.collect_all()
"""

import requests
import feedparser
import arxiv
import json
import os
import yaml
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from typing import List, Dict, Optional, Callable, Tuple, Any
from dataclasses import dataclass
import time
import random
import difflib
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from config import config
from logger import get_log_helper

# å¯¼å…¥å›½é™…åŒ–æ¨¡å—
try:
    from i18n import t, get_language
except ImportError:
    def t(key, **kwargs): return key
    def get_language(): return 'zh'

# å°è¯•å¯¼å…¥å¼‚æ­¥åº“
try:
    import asyncio
    import aiohttp
    ASYNC_AVAILABLE = True
except ImportError:
    ASYNC_AVAILABLE = False

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('data_collector')

# åŠ è½½ç¼“å­˜ç›®å½•é…ç½®
def _get_cache_dir():
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    cache_dir = 'data/cache'
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                cache_dir = cfg.get('data', {}).get('cache_dir', cache_dir)
    except Exception:
        pass
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir

DATA_CACHE_DIR = _get_cache_dir()


# ============== å¼‚æ­¥é‡‡é›†å™¨é…ç½® ==============

@dataclass
class AsyncCollectorConfig:
    """å¼‚æ­¥é‡‡é›†å™¨é…ç½®"""
    # å¹¶å‘æ§åˆ¶
    max_concurrent_requests: int = 20      # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    max_concurrent_per_host: int = 3       # æ¯ä¸ªä¸»æœºæœ€å¤§å¹¶å‘æ•°
    
    # è¶…æ—¶è®¾ç½®ï¼ˆç§’ï¼‰
    request_timeout: int = 15              # å•ä¸ªè¯·æ±‚è¶…æ—¶
    total_timeout: int = 120               # æ€»é‡‡é›†è¶…æ—¶
    
    # é‡è¯•è®¾ç½®
    max_retries: int = 2                   # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0               # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    # é€Ÿç‡é™åˆ¶
    rate_limit_delay: float = 0.2          # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    
    # æ•°æ®ç›®å½•
    cache_dir: str = 'data/cache'


def _load_async_config() -> AsyncCollectorConfig:
    """ä» config.yaml åŠ è½½å¼‚æ­¥é‡‡é›†é…ç½®"""
    cfg = AsyncCollectorConfig()
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                yaml_cfg = yaml.safe_load(f)
                async_cfg = yaml_cfg.get('async_collector', {})
                
                cfg.max_concurrent_requests = async_cfg.get('max_concurrent_requests', cfg.max_concurrent_requests)
                cfg.max_concurrent_per_host = async_cfg.get('max_concurrent_per_host', cfg.max_concurrent_per_host)
                cfg.request_timeout = async_cfg.get('request_timeout', cfg.request_timeout)
                cfg.total_timeout = async_cfg.get('total_timeout', cfg.total_timeout)
                cfg.max_retries = async_cfg.get('max_retries', cfg.max_retries)
                cfg.cache_dir = yaml_cfg.get('data', {}).get('cache_dir', cfg.cache_dir)
    except Exception:
        pass
    
    os.makedirs(cfg.cache_dir, exist_ok=True)
    return cfg


def _check_async_mode() -> bool:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å¼‚æ­¥æ¨¡å¼"""
    if not ASYNC_AVAILABLE:
        return False
    
    # ä»é…ç½®è¯»å–
    try:
        if os.path.exists('config.yaml'):
            with open('config.yaml', 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
                return cfg.get('collector', {}).get('async_mode', True)
    except Exception:
        pass
    
    return True  # é»˜è®¤ä½¿ç”¨å¼‚æ­¥æ¨¡å¼


# ============== AIç›¸å…³å¸¸é‡å®šä¹‰ ==============

# AIé¢†è¢–åˆ—è¡¨
AI_LEADERS = {
    "Sam Altman": "OpenAI CEO",
    "Elon Musk": "xAI Founder",
    "Jensen Huang": "NVIDIA CEO",
    "Demis Hassabis": "Google DeepMind CEO",
    "Yann LeCun": "Meta Chief AI Scientist",
    "Geoffrey Hinton": "AI Pioneer",
    "Andrew Ng": "AI Fund Managing General Partner",
    "Kai-Fu Lee": "01.AI CEO",
    "Robin Li": "Baidu CEO"
}

# AIç›¸å…³å…³é”®è¯
AI_KEYWORDS = [
    'ai', 'artificial intelligence', 'machine learning', 'deep learning',
    'neural network', 'llm', 'gpt', 'transformer', 'chatgpt', 'claude',
    'gemini', 'llama', 'anthropic', 'openai',
    'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'å¤§æ¨¡å‹'
]

# HNæœç´¢å…³é”®è¯
HN_SEARCH_TERMS = [
    'ai', 'llm', 'gpt', 'chatgpt', 'openai', 'anthropic', 'claude',
    'gemini', 'llama', 'transformer', 'machine learning', 'deep learning',
    'neural', 'diffusion', 'stable diffusion', 'midjourney', 'copilot',
    'langchain', 'rag', 'vector', 'embedding', 'fine-tune', 'rlhf'
]

# RSSæºé…ç½® - ç»Ÿä¸€é…ç½®
RSS_FEEDS = {
    'research': [
        'http://export.arxiv.org/rss/cs.AI',
        'http://export.arxiv.org/rss/cs.CL',
        'http://export.arxiv.org/rss/cs.CV',
        'http://export.arxiv.org/rss/cs.LG',
    ],
    'news': [
        'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml',
        'https://techcrunch.com/category/artificial-intelligence/feed/',
        'https://www.wired.com/feed/tag/ai/latest/rss',
        'https://spectrum.ieee.org/rss/topic/artificial-intelligence',
        'https://www.technologyreview.com/feed/',
        'https://artificialintelligence-news.com/feed/',
        'https://syncedreview.com/feed/',
        'https://www.36kr.com/feed',
        'https://www.ithome.com/rss/',
        'https://www.jiqizhixin.com/rss',
        'https://www.qbitai.com/feed',
        'https://www.infoq.cn/feed/topic/18',
    ],
    'developer': [
        'https://github.blog/feed/',
        'https://huggingface.co/blog/feed.xml',
        'https://openai.com/blog/rss.xml',
        'https://blog.google/technology/ai/rss/',
    ],
    'product_news': [
        'https://openai.com/blog/rss.xml',
        'https://blog.google/technology/ai/rss/',
        'https://blogs.microsoft.com/ai/feed/',
    ],
    'community': [
        'https://www.producthunt.com/feed?category=artificial-intelligence',
    ],
    'leader_blogs': [
        {'url': 'http://blog.samaltman.com/posts.atom', 'author': 'Sam Altman', 'title': 'OpenAI CEO'},
        {'url': 'https://karpathy.github.io/feed.xml', 'author': 'Andrej Karpathy', 'title': 'AI Researcher'},
        {'url': 'https://lexfridman.com/feed/podcast/', 'author': 'Lex Fridman', 'title': 'Podcast Host', 'type': 'podcast'},
    ]
}


class AIDataCollector:
    """AIæ•°æ®é‡‡é›†å™¨ - æ”¶é›†çœŸå®æœ€æ–°çš„AIä¿¡æ¯
    
    æ”¯æŒä¸¤ç§æ¨¡å¼:
    - åŒæ­¥æ¨¡å¼: ä½¿ç”¨ThreadPoolExecutorå¹¶è¡Œé‡‡é›†
    - å¼‚æ­¥æ¨¡å¼: ä½¿ç”¨asyncio+aiohttpé«˜æ€§èƒ½é‡‡é›†ï¼ˆæ¨èï¼‰
    
    Args:
        async_mode: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
    """
    
    def __init__(self, async_mode: Optional[bool] = None):
        # ç¡®å®šé‡‡é›†æ¨¡å¼
        if async_mode is None:
            self._use_async = _check_async_mode()
        else:
            self._use_async = async_mode and ASYNC_AVAILABLE
        
        # å¼‚æ­¥é…ç½®
        if self._use_async:
            self.async_config = _load_async_config()
            log.config("ğŸ“¡ Collector mode: Async (aiohttp)")
        else:
            self.async_config = None
            log.config("ğŸ“¡ Collector mode: Sync (ThreadPool)")
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ä½¿ç”¨ç»Ÿä¸€çš„RSSæºé…ç½®
        self.rss_feeds = RSS_FEEDS
        
        # é‡‡é›†å†å²ç¼“å­˜
        self.history_cache_file = os.path.join(DATA_CACHE_DIR, 'collection_history_cache.json')
        self.history_cache = self._load_history_cache()
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå¼‚æ­¥æ¨¡å¼ï¼‰
        self.stats = {
            'requests_made': 0,
            'requests_failed': 0,
            'items_collected': 0,
            'start_time': None,
            'end_time': None
        }
    
    def _load_history_cache(self) -> Dict:
        """åŠ è½½é‡‡é›†å†å²ç¼“å­˜"""
        try:
            if os.path.exists(self.history_cache_file):
                with open(self.history_cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                    # éªŒè¯ç¼“å­˜æ ¼å¼
                    if isinstance(cache, dict) and 'urls' in cache and 'titles' in cache:
                        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡7å¤©ï¼‰
                        last_updated = cache.get('last_updated', '')
                        if last_updated:
                            try:
                                last_time = datetime.fromisoformat(last_updated)
                                if (datetime.now() - last_time).days > 7:
                                    log.warning(t('dc_cache_expired'))
                                    return {'urls': set(), 'titles': set(), 'last_updated': ''}
                            except (ValueError, TypeError):
                                pass
                        # è½¬æ¢ä¸º set ä»¥åŠ é€ŸæŸ¥æ‰¾
                        cache['urls'] = set(cache['urls'])
                        cache['titles'] = set(cache['titles'])
                        log.data(t('dc_cache_loaded', url_count=len(cache['urls']), title_count=len(cache['titles'])))
                        return cache
        except Exception as e:
            log.error(t('dc_cache_load_failed', error=str(e)))
        return {'urls': set(), 'titles': set(), 'last_updated': ''}
    
    def _save_history_cache(self):
        """ä¿å­˜é‡‡é›†å†å²ç¼“å­˜"""
        try:
            # è½¬æ¢ set ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
            cache_to_save = {
                'urls': list(self.history_cache['urls']),
                'titles': list(self.history_cache['titles']),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.history_cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_to_save, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(t('dc_cache_save_failed', error=str(e)))
    
    def _is_in_history(self, item: Dict) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦åœ¨å†å²ç¼“å­˜ä¸­ï¼ˆä¸¥æ ¼åŒ¹é… URL æˆ–æ ‡é¢˜ï¼‰"""
        url = item.get('url', '')
        title = item.get('title', '')
        
        # ä¸¥æ ¼åŒ¹é…ï¼šURL å®Œå…¨ç›¸åŒ æˆ– æ ‡é¢˜å®Œå…¨ç›¸åŒ
        if url and url in self.history_cache['urls']:
            return True
        if title and title in self.history_cache['titles']:
            return True
        return False
    
    def _add_to_history(self, item: Dict):
        """å°†é¡¹ç›®æ·»åŠ åˆ°å†å²ç¼“å­˜"""
        url = item.get('url', '')
        title = item.get('title', '')
        if url:
            self.history_cache['urls'].add(url)
        if title:
            self.history_cache['titles'].add(title)
    
    def clear_history_cache(self):
        """æ¸…é™¤é‡‡é›†å†å²ç¼“å­˜"""
        import os
        self.history_cache = {'urls': set(), 'titles': set(), 'last_updated': ''}
        if os.path.exists(self.history_cache_file):
            os.remove(self.history_cache_file)
        # å¦‚æœæœ‰å¼‚æ­¥é‡‡é›†å™¨ï¼Œä¹Ÿæ¸…é™¤å…¶ç¼“å­˜
        if self._async_collector:
            self._async_collector.clear_history_cache()
        log.success(t('dc_cache_cleared'))
    
    @property
    def is_async_mode(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼"""
        return self._use_async and self._async_collector is not None
    
    def collect_research_papers(self, max_results: int = 10) -> List[Dict]:
        """
        é‡‡é›†æœ€æ–°AIç ”ç©¶è®ºæ–‡
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            ç ”ç©¶è®ºæ–‡åˆ—è¡¨
        """
        log.dual_start(t('dc_collect_research'))
        papers = []
        
        try:
            # ä½¿ç”¨arXiv APIè·å–æœ€æ–°è®ºæ–‡
            client = arxiv.Client()
            
            # æ„å»ºæŸ¥è¯¢ - æœ€æ–°çš„AIç›¸å…³è®ºæ–‡
            search_query = arxiv.Search(
                query="cat:cs.AI OR cat:cs.LG OR cat:cs.CV OR cat:cs.CL",
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            for result in client.results(search_query):
                # è¿‡æ»¤éæœ€è¿‘30å¤©çš„è®ºæ–‡
                if not self._is_recent(result.published):
                    continue
                    
                paper = {
                    'title': result.title,
                    'summary': result.summary[:300] + "..." if len(result.summary) > 300 else result.summary,
                    'authors': [str(author) for author in result.authors],
                    'url': result.entry_id,
                    'published': result.published.strftime('%Y-%m-%d'),
                    'categories': [str(cat) for cat in result.categories],
                    'source': 'arXiv',
                    'category': 'research',
                    'importance': self._calculate_importance(result.title, result.summary)
                }
                papers.append(paper)
                
            log.dual_success(t('dc_got_papers', count=len(papers)))
            
        except Exception as e:
            log.error(t('dc_arxiv_failed', error=str(e)))
            # æä¾›å¤‡ç”¨æ•°æ®
            papers = self._get_backup_research_data()
        
        return papers
    
    def collect_developer_content(self, max_results: int = 15) -> List[Dict]:
        """
        é‡‡é›†å¼€å‘è€…ç¤¾åŒºå†…å®¹
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            å¼€å‘è€…å†…å®¹åˆ—è¡¨
        """
        log.dual_start(t('dc_collect_developer'))
        content = []
        
        # 1. GitHub Trending AIé¡¹ç›®
        github_projects = self._collect_github_trending()
        content.extend(github_projects[:max_results//3])
        
        # 2. Hugging Faceæœ€æ–°æ¨¡å‹/æ•°æ®é›†
        hf_content = self._collect_huggingface_updates()
        content.extend(hf_content[:max_results//3])
        
        # 3. å¼€å‘è€…åšå®¢å’Œæ•™ç¨‹
        dev_blogs = self._collect_dev_blogs()
        content.extend(dev_blogs[:max_results//3])
        
        log.dual_success(t('dc_got_developer', count=len(content)))
        return content
    
    def collect_product_releases(self, max_results: int = 10) -> List[Dict]:
        """
        é‡‡é›†AIäº§å“å‘å¸ƒä¿¡æ¯
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            äº§å“å‘å¸ƒåˆ—è¡¨
        """
        log.dual_start(t('dc_collect_products'))
        products = []
        
        # æ”¶é›†ä¸»è¦AIå…¬å¸çš„äº§å“å‘å¸ƒä¿¡æ¯
        company_sources = {
            'OpenAI': self._collect_openai_updates,
            'Google': self._collect_google_ai_updates,
            'Microsoft': self._collect_microsoft_ai_updates,
            'Meta': self._collect_meta_ai_updates,
            'Anthropic': self._collect_anthropic_updates,
            'China_Tech': self._collect_chinese_ai_updates  # æ–°å¢ä¸­å›½ç§‘æŠ€å…¬å¸
        }
        
        for company, collector_func in company_sources.items():
            try:
                company_products = collector_func()
                # è¿‡æ»¤éæœ€è¿‘å‘å¸ƒçš„äº§å“
                company_products = [p for p in company_products if self._is_recent(p.get('published', ''))]
                products.extend(company_products)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                log.warning(t('dc_product_failed', company=company, error=str(e)))
        
        # æŒ‰é‡è¦æ€§æ’åºå¹¶é™åˆ¶æ•°é‡
        products.sort(key=lambda x: x.get('importance', 0), reverse=True)
        products = products[:max_results]
        
        log.dual_success(t('dc_got_products', count=len(products)))
        return products
    
    def collect_ai_leaders_quotes(self, max_results: int = 15) -> List[Dict]:
        """
        é‡‡é›†å…¨çƒAIé¢†è¢–çš„è¿‘æœŸè¨€è®º
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            é¢†è¢–è¨€è®ºåˆ—è¡¨
        """
        log.dual_start(t('dc_collect_leaders'))
        quotes = []
        
        leaders = {
            "Sam Altman": "OpenAI CEO",
            "Elon Musk": "xAI Founder",
            "Jensen Huang": "NVIDIA CEO",
            "Demis Hassabis": "Google DeepMind CEO",
            "Yann LeCun": "Meta Chief AI Scientist",
            "Geoffrey Hinton": "AI Pioneer",
            "Andrew Ng": "AI Fund Managing General Partner",
            "Kai-Fu Lee": "01.AI CEO",
            "Robin Li": "Baidu CEO"
        }
        
        # 1. å°è¯•ä½¿ç”¨æ–°é—»RSSæœç´¢ (ä¼˜å…ˆBing News)
        base_url_google = "https://news.google.com/rss/search?q={}+AI+when:30d&hl=en-US&gl=US&ceid=US:en"
        base_url_bing = "https://www.bing.com/news/search?q={}+AI&format=rss"
        
        for leader_name, title in leaders.items():
            try:
                query_name = leader_name.replace(' ', '+')
                
                # ç­–ç•¥A: ä¼˜å…ˆä½¿ç”¨ Bing News
                feed_url = base_url_bing.format(query_name)
                feed = feedparser.parse(feed_url)
                
                # ç­–ç•¥B: å¦‚æœ Bing News ä¸ºç©ºï¼Œå°è¯• Google News
                if not feed.entries:
                    feed_url = base_url_google.format(query_name)
                    feed = feedparser.parse(feed_url)
                
                count = 0
                for entry in feed.entries:
                    if count >= 2: # æ¯ä¸ªé¢†è¢–æœ€å¤šå–2æ¡
                        break
                        
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€è¿‘30å¤©
                    date_val = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date_val = entry.published_parsed
                    elif entry.get('published'):
                        date_val = entry.get('published')
                    
                    if date_val and not self._is_recent(date_val):
                        continue
                        
                    # ç®€å•çš„å…³é”®è¯è¿‡æ»¤ï¼Œç¡®ä¿æ˜¯è¨€è®ºç›¸å…³çš„
                    text = (entry.title + " " + entry.get('summary', '')).lower()
                    if any(k in text for k in ['said', 'says', 'stated', 'warns', 'believes', 'predicts', 'interview', 'speech', 'tweet', 'post']):
                        quote = {
                            'title': f"{leader_name}: {entry.title}",
                            'summary': entry.get('summary', entry.title)[:300],
                            'url': entry.link,
                            'published': entry.get('published', datetime.now().strftime('%Y-%m-%d')),
                            'source': f"News about {leader_name}",
                            'category': 'leader',
                            'author': leader_name,
                            'author_title': title,
                            'importance': 0.95
                        }
                        quotes.append(quote)
                        count += 1
                
                time.sleep(0.5) # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                log.warning(t('dc_leader_failed', name=leader_name, error=str(e)))
        
        # 2. é‡‡é›†ä¸ªäººåšå®¢å’Œæ’­å®¢
        for source in self.rss_feeds.get('leader_blogs', []):
            try:
                feed = feedparser.parse(source['url'])
                for entry in feed.entries[:3]:
                    # æ£€æŸ¥æ—¶é—´
                    date_val = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        date_val = entry.published_parsed
                    elif entry.get('published'):
                        date_val = entry.get('published')
                    
                    if date_val and not self._is_recent(date_val):
                        continue

                    # å¦‚æœæ˜¯æ’­å®¢ï¼Œæ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³æ³¨çš„é¢†è¢–åå­—
                    if source.get('type') == 'podcast':
                        found_leader = False
                        for leader_name in leaders.keys():
                            if leader_name.lower() in entry.title.lower():
                                found_leader = True
                                source['author'] = leader_name # ä¸´æ—¶è¦†ç›–ä¸ºå˜‰å®¾å
                                break
                        if not found_leader:
                            continue

                    quote = {
                        'title': f"[{source['author']}] {entry.title}",
                        'summary': entry.get('summary', entry.get('description', ''))[:300],
                        'url': entry.link,
                        'published': entry.get('published', datetime.now().strftime('%Y-%m-%d')),
                        'source': 'Personal Blog/Podcast',
                        'category': 'leader',
                        'author': source['author'],
                        'author_title': source['title'],
                        'importance': 1.0 # ä¸ªäººåšå®¢å†…å®¹æƒé‡æœ€é«˜
                    }
                    quotes.append(quote)
            except Exception as e:
                log.warning(t('dc_blog_failed', author=source['author'], error=str(e)))

        # 3. å¦‚æœé‡‡é›†æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®
        if len(quotes) < 5:
            log.warning(t('dc_fallback_data'))
            quotes.extend(self._get_backup_leaders_data())
            
        # å»é‡
        unique_quotes = []
        seen_urls = set()
        for q in quotes:
            if q['url'] not in seen_urls:
                unique_quotes.append(q)
                seen_urls.add(q['url'])
        
        # æŒ‰æ—¶é—´æ’åº
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦è§£ææ—¶é—´å­—ç¬¦ä¸²
        
        result = unique_quotes[:max_results]
        log.dual_success(t('dc_got_leaders', count=len(result)))
        return result

    def collect_latest_news(self, max_results: int = 20) -> List[Dict]:
        """
        é‡‡é›†æœ€æ–°AIè¡Œä¸šæ–°é—»
        
        Args:
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        log.dual_start(t('dc_collect_news'))
        
        # ä»äº§å“å‘å¸ƒæ–°é—»æºé‡‡é›†
        product_news = []
        for feed_url in self.rss_feeds.get('product_news', []):
            try:
                feed_news = self._parse_rss_feed(feed_url, category='product')
                product_news.extend(feed_news)
                time.sleep(0.3)
            except Exception as e:
                log.warning(t('dc_product_feed_failed', url=feed_url, error=str(e)))
        
        # ä»ä¼ ç»Ÿæ–°é—»æºé‡‡é›†
        general_news = []
        for feed_url in self.rss_feeds['news']:
            try:
                feed_news = self._parse_rss_feed(feed_url, category='news')
                general_news.extend(feed_news)
                time.sleep(0.5)
            except Exception as e:
                log.warning(t('dc_rss_failed', url=feed_url, error=str(e)))
        
        # åˆå¹¶ä¸¤ç±»æ–°é—»
        all_news = product_news + general_news
        
        # è¿‡æ»¤AIç›¸å…³å†…å®¹
        ai_news = [item for item in all_news if self._is_ai_related(item)]
        
        # å…¨å±€å»é‡ - æé«˜ä¿¡å™ªæ¯”
        ai_news = self._deduplicate_items(ai_news)
        
        # æŒ‰æ—¶é—´æ’åº
        ai_news.sort(key=lambda x: x.get('published', ''), reverse=True)
        
        # ä¼˜å…ˆæ˜¾ç¤ºäº§å“å‘å¸ƒæ–°é—»
        product_related = [item for item in ai_news if self._is_product_related(item)]
        other_news = [item for item in ai_news if not self._is_product_related(item)]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åˆ—ï¼šäº§å“å‘å¸ƒ > å…¶ä»–AIæ–°é—»
        prioritized_news = product_related + other_news
        result = prioritized_news[:max_results]
        log.dual_success(t('dc_got_news', count=len(result)))
        return result
    
    def collect_community_trends(self, max_results: int = 15) -> List[Dict]:
        """
        é‡‡é›†ç¤¾åŒºçƒ­ç‚¹ (Product Hunt, Hacker News)
        
        Hacker News ä½¿ç”¨å®˜æ–¹ API è·å–æ›´å¥½çš„æ•°æ®è´¨é‡
        """
        log.dual_start(t('dc_collect_community'))
        trends = []
        
        # 1. ä½¿ç”¨ HN å®˜æ–¹ API é‡‡é›†
        try:
            hn_items = self._fetch_hacker_news_api(max_items=10)
            for item in hn_items:
                # æ ¹æ® score è°ƒæ•´é‡è¦æ€§
                score = item.get('score', 0)
                if score > 100:
                    item['importance'] = 0.85
                elif score > 50:
                    item['importance'] = 0.75
                else:
                    item['importance'] = 0.65
                trends.append(item)
        except Exception as e:
            log.dual_warning(t('dc_hn_api_failed', error=str(e)))
        
        # 2. é‡‡é›† Product Hunt ç­‰å…¶ä»– RSS æº
        for feed_url in self.rss_feeds.get('community', []):
            try:
                # è·³è¿‡ HN RSS (å·²ç”¨ API æ›¿ä»£)
                if "hnrss" in feed_url:
                    continue
                    
                # Determine source name for better labeling
                source_name = "Community"
                if "producthunt" in feed_url:
                    source_name = "Product Hunt"
                elif "reddit" in feed_url:
                    if "LocalLLaMA" in feed_url:
                        source_name = "Reddit (LocalLLaMA)"
                    else:
                        source_name = "Reddit (Singularity)"
                elif "lmsys" in feed_url:
                    source_name = "LMSYS Arena"

                feed_items = self._parse_rss_feed(feed_url, category='community')
                
                for item in feed_items:
                    item['source'] = source_name
                    # Boost importance for these high-signal sources
                    item['importance'] = item.get('importance', 0.6) + 0.1
                    trends.append(item)
                    time.sleep(0.2)
                    
            except Exception as e:
                log.warning(t('dc_community_failed', url=feed_url, error=str(e)))
        
        # Deduplicate
        trends = self._deduplicate_items(trends)
        
        # Sort by published date
        trends.sort(key=lambda x: x.get('published', ''), reverse=True)
        
        result = trends[:max_results]
        log.dual_success(t('dc_got_community', count=len(result)))
        return result

    def collect_all(self, parallel: bool = True, max_workers: int = 6) -> Dict[str, List[Dict]]:
        """
        é‡‡é›†æ‰€æœ‰ç±»å‹çš„æ•°æ®
        
        Args:
            parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œé‡‡é›†ï¼ˆåŒæ­¥æ¨¡å¼å‚æ•°ï¼‰
            max_workers: å¹¶è¡Œé‡‡é›†çš„æœ€å¤§çº¿ç¨‹æ•°ï¼ˆåŒæ­¥æ¨¡å¼å‚æ•°ï¼‰
        
        Returns:
            åˆ†ç±»çš„æ•°æ®å­—å…¸
        """
        # å¦‚æœä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼Œå§”æ‰˜ç»™å¼‚æ­¥é‡‡é›†
        if self._use_async and ASYNC_AVAILABLE:
            return self._collect_all_async_wrapper()
        
        # åŒæ­¥æ¨¡å¼ - åŸæœ‰å®ç°
        return self._collect_all_sync(parallel, max_workers)
    
    def _collect_all_async_wrapper(self) -> Dict[str, List[Dict]]:
        """å¼‚æ­¥é‡‡é›†çš„åŒæ­¥åŒ…è£…å™¨"""
        try:
            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥é‡‡é›†
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self._collect_all_async())
            finally:
                loop.close()
        except Exception as e:
            log.error(f"Async collection failed: {e}, falling back to sync mode")
            return self._collect_all_sync(True, 6)
    
    def _collect_all_sync(self, parallel: bool = True, max_workers: int = 6) -> Dict[str, List[Dict]]:
        """
        åŒæ­¥é‡‡é›†æ‰€æœ‰ç±»å‹çš„æ•°æ®ï¼ˆåŸæœ‰å®ç°ï¼‰
        
        Args:
            parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œé‡‡é›†ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
            max_workers: å¹¶è¡Œé‡‡é›†çš„æœ€å¤§çº¿ç¨‹æ•°ï¼ˆé»˜è®¤6ï¼‰
        
        Returns:
            åˆ†ç±»çš„æ•°æ®å­—å…¸
        """
        log.dual_start(t('dc_start_collection'))
        log.dual_separator("=", 50)
        
        all_data = {
            'research': [],
            'developer': [],
            'product': [],
            'news': [],
            'leader': [],
            'community': []
        }
        
        # ä»é…ç½®è¯»å–é‡‡é›†æ•°é‡
        product_count = config.get('collector.product_count', 10)
        community_count = config.get('collector.community_count', 10)
        leader_count = config.get('collector.leader_count', 15)
        research_count = config.get('collector.research_count', 15)
        developer_count = config.get('collector.developer_count', 20)
        news_count = config.get('collector.news_count', 25)
        
        # ä»é…ç½®è¯»å–å¹¶è¡Œè®¾ç½®
        parallel = config.get('collector.parallel_enabled', parallel)
        max_workers = config.get('collector.parallel_workers', max_workers)
        
        # å®šä¹‰é‡‡é›†ä»»åŠ¡
        collect_tasks: List[Tuple[str, Callable, int]] = [
            ('research', self.collect_research_papers, research_count),
            ('developer', self.collect_developer_content, developer_count),
            ('product', self.collect_product_releases, product_count),
            ('leader', self.collect_ai_leaders_quotes, leader_count),
            ('community', self.collect_community_trends, community_count),
            ('news', self.collect_latest_news, news_count),
        ]
        
        if parallel and max_workers > 1:
            # å¹¶è¡Œé‡‡é›†æ¨¡å¼
            log.dual_info(t('dc_parallel_mode', workers=max_workers))
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = {
                    executor.submit(func, count): category 
                    for category, func, count in collect_tasks
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    category = futures[future]
                    try:
                        result = future.result()
                        all_data[category] = result
                        log.dual_success(t('dc_parallel_task_done', category=category, count=len(result)))
                    except Exception as e:
                        log.error(t('dc_parallel_task_failed', category=category, error=str(e)))
                        all_data[category] = []
            
            elapsed = time.time() - start_time
            log.dual_info(t('dc_parallel_complete', time=f"{elapsed:.1f}"))
        else:
            # ä¸²è¡Œé‡‡é›†æ¨¡å¼
            log.dual_info(t('dc_serial_mode'))
            for category, func, count in collect_tasks:
                all_data[category] = func(count)
        
        # ä½¿ç”¨ç‹¬ç«‹çš„é‡‡é›†å†å²ç¼“å­˜ç»Ÿè®¡æ–°å†…å®¹ï¼ˆä½†ä¸è¿‡æ»¤ï¼Œæ‰€æœ‰å†…å®¹éƒ½ä¼ é€’ç»™åˆ†ç±»æ¨¡å—ï¼‰
        new_stats = {}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„æ–°å†…å®¹æ•°é‡
        cached_stats = {}  # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç¼“å­˜å‘½ä¸­æ•°é‡
        new_items_for_cache = []  # è®°å½•æ–°é‡‡é›†çš„é¡¹ç›®ï¼Œç¨åæ·»åŠ åˆ°ç¼“å­˜
        
        for cat in all_data:
            new_count = 0
            cached_count = 0
            for item in all_data[cat]:
                if self._is_in_history(item):
                    cached_count += 1
                else:
                    new_count += 1
                    new_items_for_cache.append(item)
            new_stats[cat] = new_count
            cached_stats[cat] = cached_count
        
        # å°†æ–°é‡‡é›†çš„é¡¹ç›®æ·»åŠ åˆ°å†å²ç¼“å­˜
        for item in new_items_for_cache:
            self._add_to_history(item)
        
        # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
        if new_items_for_cache:
            self._save_history_cache()
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_items = sum(len(items) for items in all_data.values())
        total_new = sum(new_stats.values())
        total_cached = sum(cached_stats.values())
        log.dual_done(t('dc_collection_done_v2', total=total_items, new=total_new, cached=total_cached))
        for category, items in all_data.items():
            new_count = new_stats.get(category, 0)
            cached_count = cached_stats.get(category, 0)
            log.dual_data(t('dc_category_stats_v2', category=category, count=len(items), new=new_count, cached=cached_count))
        
        return all_data
    
    def _collect_github_trending(self) -> List[Dict]:
        """é‡‡é›†GitHub AIçƒ­é—¨é¡¹ç›® (å…³æ³¨è¿‘æœŸçƒ­é—¨)"""
        projects = []
        
        try:
            # è®¡ç®—30å¤©å‰çš„æ—¥æœŸ
            last_month = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            
            # GitHub APIè·å–AIç›¸å…³çƒ­é—¨é¡¹ç›®
            url = "https://api.github.com/search/repositories"
            # ä¼˜åŒ–æŸ¥è¯¢: å…³æ³¨æœ€è¿‘åˆ›å»ºä¸”é«˜æ˜Ÿçš„é¡¹ç›®ï¼Œå‘ç°"æ˜æ—¥ä¹‹æ˜Ÿ"
            query = f'(machine-learning OR artificial-intelligence OR deep-learning OR llm) created:>{last_month}'
            
            params = {
                'q': query,
                'sort': 'stars',
                'order': 'desc',
                'per_page': 15
            }
            
            response = requests.get(url, params=params, headers=self.headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                
                for repo in data.get('items', []):
                    # è¿‡æ»¤éæœ€è¿‘æ›´æ–°çš„é¡¹ç›®
                    if not self._is_recent(repo['updated_at']):
                        continue
                        
                    project = {
                        'title': repo['full_name'],
                        'summary': repo['description'] or 'æ— æè¿°',
                        'url': repo['html_url'],
                        'stars': repo['stargazers_count'],
                        'language': repo['language'],
                        'updated': repo['updated_at'][:10],
                        'source': 'GitHub',
                        'category': 'developer',
                        'importance': min(repo['stargazers_count'] / 1000, 1.0)
                    }
                    projects.append(project)
            
        except Exception as e:
            log.warning(t('dc_github_failed', error=str(e)))
            # ä½¿ç”¨å¤‡ç”¨æ•°æ®
            projects = self._get_backup_github_data()
        
        return projects
    
    def _collect_huggingface_updates(self) -> List[Dict]:
        """é‡‡é›†Hugging Faceæœ€æ–°æ›´æ–°"""
        updates = []
        
        try:
            # Hugging Faceæ¨¡å‹API
            url = "https://huggingface.co/api/models"
            params = {
                'limit': 10,
                'sort': 'lastModified',
                'direction': -1
            }
            
            response = requests.get(url, params=params, headers=self.headers, timeout=10)
            if response.status_code == 200:
                models = response.json()
                
                for model in models:
                    # è¿‡æ»¤éæœ€è¿‘æ›´æ–°çš„æ¨¡å‹
                    if not self._is_recent(model.get('lastModified', '')):
                        continue
                        
                    update = {
                        'title': f"HF Model: {model['id']}",
                        'summary': f"æœ€æ–°AIæ¨¡å‹å‘å¸ƒ: {model['id']}ï¼Œä¸‹è½½é‡: {model.get('downloads', 0)}",
                        'url': f"https://huggingface.co/{model['id']}",
                        'downloads': model.get('downloads', 0),
                        'updated': model.get('lastModified', '')[:10],
                        'source': 'Hugging Face',
                        'category': 'developer',
                        'importance': min(model.get('downloads', 0) / 10000, 1.0)
                    }
                    updates.append(update)
        
        except Exception as e:
            log.warning(t('dc_hf_failed', error=str(e)))
            updates = self._get_backup_hf_data()
        
        return updates
    
    def _collect_dev_blogs(self) -> List[Dict]:
        """é‡‡é›†å¼€å‘è€…åšå®¢å†…å®¹"""
        blogs = []
        
        try:
            # ä»GitHubåšå®¢RSSè·å–
            for feed_url in self.rss_feeds['developer']:
                feed_content = self._parse_rss_feed(feed_url, category='developer')
                blogs.extend(feed_content)
        
        except Exception as e:
            log.warning(t('dc_dev_blog_failed', error=str(e)))
            blogs = self._get_backup_blog_data()
        
        return blogs
    
    def _collect_openai_updates(self) -> List[Dict]:
        """é‡‡é›†OpenAIäº§å“æ›´æ–°"""
        updates = []
        try:
            # å°è¯•ä»RSSè·å–
            rss_url = 'https://openai.com/blog/rss.xml'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'OpenAI'
                item['importance'] = 0.95 # OpenAIæ–°é—»é€šå¸¸å¾ˆé‡è¦
        except Exception:
            pass
            
        if updates:
            return updates
            
        # å¤‡ç”¨æ•°æ®
        return [
            {
                'title': 'OpenAI ChatGPT-4o å‘å¸ƒå…¬å‘Š',
                'summary': 'OpenAIæ­£å¼å‘å¸ƒChatGPT-4oï¼Œå…·å¤‡æ›´å¼ºçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„ç»¼åˆå¤„ç†ï¼Œå“åº”é€Ÿåº¦æ˜¾è‘—æå‡ã€‚',
                'url': 'https://openai.com/index/hello-gpt-4o/',
                'company': 'OpenAI',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'OpenAI',
                'category': 'product',
                'importance': 0.95
            },
            {
                'title': 'OpenAI API å®šä»·æ›´æ–°å…¬å‘Š',
                'summary': 'OpenAIæ›´æ–°APIå®šä»·ç­–ç•¥ï¼Œé™ä½GPT-4ä½¿ç”¨æˆæœ¬ï¼ŒåŒæ—¶æ¨å‡ºæ›´ç»æµçš„GPT-4 Turboé€‰é¡¹ï¼Œä¸ºå¼€å‘è€…æä¾›æ›´çµæ´»çš„é€‰æ‹©ã€‚',
                'url': 'https://openai.com/api/pricing/',
                'company': 'OpenAI',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'OpenAI',
                'category': 'product',
                'importance': 0.88
            }
        ]
    
    def _collect_google_ai_updates(self) -> List[Dict]:
        """é‡‡é›†Google AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://blog.google/technology/ai/rss/'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Google'
                item['importance'] = 0.92
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Google Gemini äº§å“ä»‹ç»é¡µé¢',
                'summary': 'Google Geminiæ˜¯ä¸‹ä¸€ä»£AIæ¨¡å‹ï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒæ–‡æœ¬ã€ä»£ç ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„ç»¼åˆå¤„ç†ã€‚',
                'url': 'https://gemini.google.com/',
                'company': 'Google',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Google AI',
                'category': 'product',
                'importance': 0.92
            },
            {
                'title': 'Google AI Studio äº§å“å‘å¸ƒ',
                'summary': 'Google AI Studioä¸ºå¼€å‘è€…æä¾›å¿«é€ŸåŸå‹è®¾è®¡å’Œæµ‹è¯•ç”Ÿæˆå¼AIæƒ³æ³•çš„å¹³å°ï¼Œæ”¯æŒGeminiæ¨¡å‹çš„å¿«é€Ÿé›†æˆå’Œéƒ¨ç½²ã€‚',
                'url': 'https://aistudio.google.com/',
                'company': 'Google',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Google AI',
                'category': 'product',
                'importance': 0.85
            }
        ]
    
    def _collect_microsoft_ai_updates(self) -> List[Dict]:
        """é‡‡é›†Microsoft AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://blogs.microsoft.com/ai/feed/'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Microsoft'
                item['importance'] = 0.90
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Microsoft Copilot äº§å“é¡µé¢',
                'summary': 'Microsoft Copilotæ˜¯AIé©±åŠ¨çš„ç”Ÿäº§åŠ›å·¥å…·ï¼Œé›†æˆåˆ°Microsoft 365ä¸­ï¼Œå¸®åŠ©ç”¨æˆ·æå‡å·¥ä½œæ•ˆç‡ï¼Œæ”¯æŒæ–‡æ¡£ç¼–å†™ã€æ•°æ®åˆ†æç­‰åŠŸèƒ½ã€‚',
                'url': 'https://copilot.microsoft.com/',
                'company': 'Microsoft',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Microsoft',
                'category': 'product',
                'importance': 0.90
            },
            {
                'title': 'Azure AI Services äº§å“ä»‹ç»',
                'summary': 'Azure AI Servicesæä¾›å®Œæ•´çš„AIå’Œæœºå™¨å­¦ä¹ æœåŠ¡å¥—ä»¶ï¼ŒåŒ…æ‹¬è®¤çŸ¥æœåŠ¡ã€æœºå™¨å­¦ä¹ å¹³å°å’ŒOpenAIæœåŠ¡ï¼Œä¸ºä¼ä¸šAIè½¬å‹æä¾›æ”¯æŒã€‚',
                'url': 'https://azure.microsoft.com/en-us/products/ai-services',
                'company': 'Microsoft',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Microsoft Azure',
                'category': 'product',
                'importance': 0.88
            }
        ]
    
    def _collect_meta_ai_updates(self) -> List[Dict]:
        """é‡‡é›†Meta AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://ai.meta.com/blog/rss/'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Meta'
                item['importance'] = 0.90
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Meta Llama 3.3 æ¨¡å‹å‘å¸ƒå…¬å‘Š',
                'summary': 'Metaå‘å¸ƒLlama 3.3ï¼Œè¿™æ˜¯æœ€æ–°çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ¨ç†ã€ä»£ç ç”Ÿæˆå’Œå¤šè¯­è¨€æ”¯æŒæ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œæ”¯æŒå•†ä¸šä½¿ç”¨ã€‚',
                'url': 'https://llama.meta.com/',
                'company': 'Meta',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Meta AI',
                'category': 'product',
                'importance': 0.90
            },
            {
                'title': 'Meta AI Assistant äº§å“ä»‹ç»',
                'summary': 'Meta AIæ˜¯æ™ºèƒ½åŠ©æ‰‹äº§å“ï¼Œé›†æˆåˆ°Facebookã€Instagramã€WhatsAppç­‰å¹³å°ï¼Œä¸ºç”¨æˆ·æä¾›AIé©±åŠ¨çš„å¯¹è¯ã€åˆ›ä½œå’Œæœç´¢ä½“éªŒã€‚',
                'url': 'https://www.meta.ai/',
                'company': 'Meta',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Meta AI',
                'category': 'product',
                'importance': 0.85
            }
        ]
    
    def _collect_anthropic_updates(self) -> List[Dict]:
        """é‡‡é›†Anthropic AIäº§å“æ›´æ–°"""
        updates = []
        try:
            rss_url = 'https://www.anthropic.com/news/rss'
            updates = self._parse_rss_feed(rss_url, category='product')
            for item in updates:
                item['company'] = 'Anthropic'
                item['importance'] = 0.88
        except Exception:
            pass
            
        if updates:
            return updates

        return [
            {
                'title': 'Anthropic Claude 3.5 Sonnet äº§å“é¡µé¢',
                'summary': 'Claude 3.5 Sonnetæ˜¯Anthropicæœ€æ–°çš„AIæ¨¡å‹ï¼Œåœ¨æ¨ç†ã€åˆ†æã€ç¼–ç ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒå¤§å®¹é‡ä¸Šä¸‹æ–‡å¤„ç†ï¼Œå…·å¤‡å¼ºå¤§çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚',
                'url': 'https://www.anthropic.com/claude',
                'company': 'Anthropic',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Anthropic',
                'category': 'product',
                'importance': 0.88
            },
            {
                'title': 'Anthropic Claude API æ–‡æ¡£',
                'summary': 'Anthropicæä¾›Claude APIæœåŠ¡ï¼Œä¸ºå¼€å‘è€…æä¾›é«˜è´¨é‡çš„å¯¹è¯AIèƒ½åŠ›ï¼Œæ”¯æŒå¤šç§ä½¿ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬å†…å®¹åˆ›ä½œã€åˆ†æå’Œç¼–ç¨‹è¾…åŠ©ç­‰ã€‚',
                'url': 'https://docs.anthropic.com/',
                'company': 'Anthropic',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Anthropic',
                'category': 'product',
                'importance': 0.82
            }
        ]
    
    def _collect_chinese_ai_updates(self) -> List[Dict]:
        """é‡‡é›†ä¸­å›½AIå…¬å¸äº§å“æ›´æ–°"""
        updates = []
        
        # 1. å°è¯•ä»RSSè·å–
        chinese_feeds = [
            'https://www.jiqizhixin.com/rss',
            'https://www.qbitai.com/feed',
            'https://www.infoq.cn/feed/topic/18',
            'https://www.baidu.com/rss/news.xml',
            'https://cloud.tencent.com/developer/rss/articles',
            'https://www.alibabacloud.com/blog/rss.xml'
        ]
        
        for feed_url in chinese_feeds:
            try:
                feed_updates = self._parse_rss_feed(feed_url, category='product')
                # è¿‡æ»¤å‡ºå¤§å…¬å¸çš„äº§å“æ–°é—»
                for item in feed_updates:
                    if any(c in item['title'] for c in ['ç™¾åº¦', 'é˜¿é‡Œ', 'è…¾è®¯', 'åä¸º', 'å­—èŠ‚', 'æ–‡å¿ƒä¸€è¨€', 'é€šä¹‰åƒé—®', 'æ··å…ƒ', 'ç›˜å¤', 'Kimi', 'æ™ºè°±', 'DeepSeek']):
                        item['company'] = 'China Tech'
                        item['importance'] = 0.9
                        updates.append(item)
            except Exception:
                continue
                
        if updates:
            return updates

        # 2. å¤‡ç”¨æ•°æ® (å¦‚æœRSSå¤±è´¥)
        return [
            {
                'title': 'ç™¾åº¦æ–‡å¿ƒä¸€è¨€ 4.0 å‘å¸ƒ',
                'summary': 'ç™¾åº¦å‘å¸ƒæ–‡å¿ƒä¸€è¨€4.0ç‰ˆæœ¬ï¼Œåœ¨ç†è§£ã€ç”Ÿæˆã€é€»è¾‘å’Œè®°å¿†å››å¤§èƒ½åŠ›ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼Œç»¼åˆæ°´å¹³ä¸GPT-4ç›¸æ¯”æ¯«ä¸é€Šè‰²ã€‚',
                'url': 'https://yiyan.baidu.com/',
                'company': 'Baidu',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Baidu AI',
                'category': 'product',
                'importance': 0.92
            },
            {
                'title': 'é˜¿é‡Œé€šä¹‰åƒé—® 2.5 å‘å¸ƒ',
                'summary': 'é˜¿é‡Œäº‘å‘å¸ƒé€šä¹‰åƒé—®2.5ï¼Œæ¨¡å‹æ€§èƒ½å…¨é¢å‡çº§ï¼Œåœ¨ä¸­æ–‡è¯­å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¼€æºå¤šæ¬¾å°ºå¯¸æ¨¡å‹ä¾›å¼€å‘è€…ä½¿ç”¨ã€‚',
                'url': 'https://tongyi.aliyun.com/',
                'company': 'Alibaba',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Aliyun',
                'category': 'product',
                'importance': 0.90
            },
             {
                'title': 'è…¾è®¯æ··å…ƒå¤§æ¨¡å‹å‡çº§',
                'summary': 'è…¾è®¯æ··å…ƒå¤§æ¨¡å‹è¿æ¥é‡è¦å‡çº§ï¼Œæ‰©å±•äº†ä¸Šä¸‹æ–‡çª—å£ï¼Œå¢å¼ºäº†ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå·²æ¥å…¥è…¾è®¯å…¨ç³»äº§å“ã€‚',
                'url': 'https://hunyuan.tencent.com/',
                'company': 'Tencent',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Tencent Cloud',
                'category': 'product',
                'importance': 0.88
            },
            {
                'title': 'DeepSeek V2 å¼€æºå‘å¸ƒ',
                'summary': 'æ·±åº¦æ±‚ç´¢(DeepSeek)å‘å¸ƒDeepSeek-V2ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„å¼€æºMoEå¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ¨ç†æˆæœ¬æä½ã€‚',
                'url': 'https://www.deepseek.com/',
                'company': 'DeepSeek',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'DeepSeek',
                'category': 'product',
                'importance': 0.95
            }
        ]
    
    def _fetch_hacker_news_api(self, max_items: int = 15, search_terms: List[str] = None) -> List[Dict]:
        """
        ä½¿ç”¨ Hacker News å®˜æ–¹ API é‡‡é›† AI ç›¸å…³å†…å®¹
        
        API æ–‡æ¡£: https://github.com/HackerNews/API
        Base URL: https://hacker-news.firebaseio.com/v0/
        
        Args:
            max_items: æœ€å¤§è¿”å›æ¡ç›®æ•°
            search_terms: æœç´¢å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ç›¸å…³å†…å®¹
            
        Returns:
            é‡‡é›†åˆ°çš„æ•°æ®åˆ—è¡¨
        """
        if search_terms is None:
            search_terms = ['ai', 'llm', 'gpt', 'chatgpt', 'openai', 'anthropic', 'claude', 
                          'gemini', 'llama', 'transformer', 'machine learning', 'deep learning',
                          'neural', 'diffusion', 'stable diffusion', 'midjourney', 'copilot',
                          'langchain', 'rag', 'vector', 'embedding', 'fine-tune', 'rlhf']
        
        HN_API_BASE = "https://hacker-news.firebaseio.com/v0"
        items = []
        
        try:
            # è·å–æœ€æ–°æ•…äº‹ ID åˆ—è¡¨
            response = requests.get(f"{HN_API_BASE}/newstories.json", timeout=10)
            if response.status_code != 200:
                log.dual_warning(t('dc_hn_api_failed', error=f"HTTP {response.status_code}"))
                return []
            
            story_ids = response.json()[:100]  # å–æœ€æ–°100æ¡è¿›è¡Œç­›é€‰
            
            ai_stories = []
            for story_id in story_ids:
                if len(ai_stories) >= max_items * 2:  # é‡‡é›†è¶³å¤Ÿå¤šå†ç­›é€‰
                    break
                    
                try:
                    # è·å–æ•…äº‹è¯¦æƒ…
                    item_response = requests.get(f"{HN_API_BASE}/item/{story_id}.json", timeout=5)
                    if item_response.status_code != 200:
                        continue
                    
                    story = item_response.json()
                    if not story or story.get('deleted') or story.get('dead'):
                        continue
                    
                    title = story.get('title', '').lower()
                    text = story.get('text', '').lower() if story.get('text') else ''
                    url = story.get('url', '')
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ AI ç›¸å…³
                    combined_text = f"{title} {text} {url}".lower()
                    if any(term in combined_text for term in search_terms):
                        ai_stories.append(story)
                    
                    time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    continue
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            for story in ai_stories[:max_items]:
                # è½¬æ¢ Unix æ—¶é—´æˆ³ä¸ºæ—¥æœŸå­—ç¬¦ä¸²
                pub_time = datetime.fromtimestamp(story.get('time', 0))
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€è¿‘çš„å†…å®¹
                if not self._is_recent(pub_time):
                    continue
                
                # æ„å»ºæ‘˜è¦ï¼šä¼˜å…ˆä½¿ç”¨ text å­—æ®µï¼Œå¦åˆ™ç”Ÿæˆæè¿°
                text_content = story.get('text', '')
                if text_content:
                    # æ¸…ç† HTML æ ‡ç­¾
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(text_content, 'html.parser')
                    summary = soup.get_text()[:300]
                else:
                    # å¦‚æœæ²¡æœ‰ textï¼Œç”ŸæˆåŸºäºå…ƒæ•°æ®çš„æ‘˜è¦
                    score = story.get('score', 0)
                    comments = story.get('descendants', 0)
                    author = story.get('by', 'unknown')
                    summary = f"Posted by {author} | {score} points | {comments} comments"
                    if story.get('url'):
                        # ä» URL æå–åŸŸåä½œä¸ºæ¥æºä¿¡æ¯
                        from urllib.parse import urlparse
                        domain = urlparse(story.get('url')).netloc
                        summary += f" | Source: {domain}"
                
                item = {
                    'title': story.get('title', ''),
                    'summary': summary,
                    'url': story.get('url') or f"https://news.ycombinator.com/item?id={story.get('id')}",
                    'published': pub_time.strftime('%Y-%m-%d %H:%M:%S'),
                    'source': 'Hacker News',
                    'category': 'community',
                    'importance': 0.7,
                    # HN ç‰¹æœ‰çš„å…ƒæ•°æ®
                    'hn_id': story.get('id'),
                    'score': story.get('score', 0),
                    'comments': story.get('descendants', 0),
                    'author': story.get('by', '')
                }
                
                if self._is_valid_item(item):
                    items.append(item)
            
        except Exception as e:
            log.dual_warning(t('dc_hn_api_failed', error=str(e)))
        
        return items

    def _parse_rss_feed(self, feed_url: str, category: str) -> List[Dict]:
        """è§£æRSSæº"""
        items = []
        
        try:
            feed = feedparser.parse(feed_url)
            
            for entry in feed.entries[:10]:  # é™åˆ¶æ¯ä¸ªæºæœ€å¤š10æ¡
                # æ£€æŸ¥æ—¥æœŸ
                date_val = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    date_val = entry.published_parsed
                elif entry.get('published'):
                    date_val = entry.get('published')
                
                if date_val and not self._is_recent(date_val):
                    continue

                item = {
                    'title': entry.get('title', ''),
                    'summary': entry.get('summary', entry.get('description', ''))[:300] + "...",
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'source': feed.feed.get('title', feed_url),
                    'category': category,
                    'importance': 0.6
                }
                
                if self._is_valid_item(item):
                    items.append(item)
        
        except Exception as e:
            log.warning(t('dc_rss_parse_failed', url=feed_url, error=str(e)))
        
        return items
    
    def _deduplicate_items(self, items: List[Dict], threshold: float = 0.6) -> List[Dict]:
        """
        å¯¹å†…å®¹åˆ—è¡¨è¿›è¡Œå»é‡
        åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦
        """
        if not items:
            return []
            
        unique_items = []
        # æŒ‰é‡è¦æ€§æ’åºï¼Œä¿ç•™é‡è¦çš„
        sorted_items = sorted(items, key=lambda x: x.get('importance', 0), reverse=True)
        
        for item in sorted_items:
            is_duplicate = False
            for existing in unique_items:
                # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
                seq = difflib.SequenceMatcher(None, item['title'].lower(), existing['title'].lower())
                if seq.ratio() > threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_items.append(item)
                
        return unique_items

    def _is_ai_related(self, item: Dict) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸AIç›¸å…³"""
        ai_keywords = [
            'ai', 'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'llm', 'gpt', 'transformer', 'chatgpt',
            'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ'
        ]
        
        text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
        return any(keyword in text for keyword in ai_keywords)
    
    def _is_product_related(self, item: Dict) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸äº§å“å‘å¸ƒç›¸å…³"""
        product_keywords = [
            'launch', 'release', 'announce', 'unveil', 'introduce', 'debut',
            'new product', 'new version', 'update', 'upgrade', 'available',
            'official', 'beta', 'preview', 'api', 'service', 'platform',
            'å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿', 'æ­£å¼', 'æ–°ç‰ˆæœ¬', 'æ–°åŠŸèƒ½',
            'äº§å“', 'æœåŠ¡', 'å¹³å°', 'å…¬æµ‹', 'å†…æµ‹'
        ]
        
        text = f"{item.get('title', '')} {item.get('summary', '')}".lower()
        return any(keyword in text for keyword in product_keywords)
    
    def _is_valid_item(self, item: Dict) -> bool:
        """éªŒè¯æ•°æ®é¡¹æœ‰æ•ˆæ€§"""
        return (item.get('title') and 
                item.get('url') and 
                len(item.get('title', '')) > 10)
    
    def _is_recent(self, date_val) -> bool:
        """æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘30å¤©å†…"""
        try:
            cutoff_date = datetime.now() - timedelta(days=30)
            
            if isinstance(date_val, datetime):
                # å¤„ç†æ—¶åŒºæ„ŸçŸ¥çš„æ—¶é—´
                if date_val.tzinfo is not None:
                    # å¦‚æœcutoff_dateæ²¡æœ‰æ—¶åŒºï¼Œæ·»åŠ å½“å‰æ—¶åŒºæˆ–UTC
                    if cutoff_date.tzinfo is None:
                        from dateutil import tz
                        cutoff_date = cutoff_date.replace(tzinfo=tz.tzlocal())
                    
                    # å†æ¬¡æ£€æŸ¥ï¼Œå¦‚æœè¿˜æ˜¯ä¸åŒ¹é…ï¼Œå°è¯•è½¬æ¢
                    if date_val.tzinfo != cutoff_date.tzinfo:
                         # ç®€å•æ¯”è¾ƒæ—¶é—´æˆ³
                         return date_val.timestamp() >= cutoff_date.timestamp()
                return date_val >= cutoff_date
                
            if isinstance(date_val, str):
                # å°è¯•è§£æå­—ç¬¦ä¸²æ—¥æœŸ
                try:
                    dt = date_parser.parse(date_val)
                    # æ¯”è¾ƒæ—¶é—´æˆ³ä»¥é¿å…æ—¶åŒºé—®é¢˜
                    return dt.timestamp() >= cutoff_date.timestamp()
                except (ValueError, TypeError, AttributeError):
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•æ ¼å¼
                    if len(date_val) >= 10:
                        dt = datetime.strptime(date_val[:10], '%Y-%m-%d')
                        return dt >= cutoff_date
            
            # å¦‚æœæ˜¯struct_time (feedparser)
            if isinstance(date_val, time.struct_time):
                dt = datetime.fromtimestamp(time.mktime(date_val))
                return dt >= cutoff_date
                
            return True # æ— æ³•è§£ææ—¶é»˜è®¤ä¿ç•™
        except Exception:
            return True

    def _calculate_importance(self, title: str, summary: str) -> float:
        """è®¡ç®—å†…å®¹é‡è¦æ€§"""
        text = f"{title} {summary}".lower()
        
        high_value_keywords = [
            'breakthrough', 'new', 'launch', 'release', 'breakthrough',
            'çªç ´', 'å‘å¸ƒ', 'æ–°', 'æœ€æ–°'
        ]
        
        score = 0.5  # åŸºç¡€åˆ†æ•°
        for keyword in high_value_keywords:
            if keyword in text:
                score += 0.1
        
        return min(score, 1.0)
    
    def _get_backup_leaders_data(self) -> List[Dict]:
        """å¤‡ç”¨é¢†è¢–è¨€è®ºæ•°æ®"""
        return [
            {
                'title': 'Sam Altman: AIå‘å±•çš„é€Ÿåº¦å°†è¶…å‡ºæ‰€æœ‰äººçš„é¢„æœŸ',
                'summary': 'OpenAI CEO Sam Altmanåœ¨æœ€è¿‘çš„é‡‡è®¿ä¸­è¡¨ç¤ºï¼ŒAGIçš„åˆ°æ¥å¯èƒ½æ¯”é¢„æœŸçš„è¦å¿«ï¼Œç¤¾ä¼šéœ€è¦ä¸ºæ­¤åšå¥½å‡†å¤‡ã€‚',
                'url': 'https://openai.com/blog',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Interview',
                'category': 'leader',
                'author': 'Sam Altman',
                'author_title': 'OpenAI CEO',
                'importance': 0.98
            },
            {
                'title': 'Elon Musk: AIå®‰å…¨æ˜¯æœªæ¥çš„é¦–è¦ä»»åŠ¡',
                'summary': 'Elon Muskå†æ¬¡å¼ºè°ƒAIå®‰å…¨çš„é‡è¦æ€§ï¼Œå¹¶è¡¨ç¤ºxAIçš„ç›®æ ‡æ˜¯ç†è§£å®‡å®™çš„æœ¬è´¨ï¼Œæ„å»ºæœ€å¤§é™åº¦è¿½æ±‚çœŸç†çš„AIã€‚',
                'url': 'https://x.ai',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'X (Twitter)',
                'category': 'leader',
                'author': 'Elon Musk',
                'author_title': 'xAI Founder',
                'importance': 0.95
            },
            {
                'title': 'Jensen Huang: ç”Ÿæˆå¼AIæ˜¯è®¡ç®—é¢†åŸŸçš„è½¬æŠ˜ç‚¹',
                'summary': 'NVIDIA CEOé»„ä»å‹‹è¡¨ç¤ºï¼Œç”Ÿæˆå¼AIæ­£åœ¨é‡å¡‘æ¯ä¸€ä¸ªè¡Œä¸šï¼Œè®¡ç®—æ–¹å¼æ­£åœ¨å‘ç”Ÿæ ¹æœ¬æ€§çš„è½¬å˜ã€‚',
                'url': 'https://nvidianews.nvidia.com/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Keynote',
                'category': 'leader',
                'author': 'Jensen Huang',
                'author_title': 'NVIDIA CEO',
                'importance': 0.92
            },
            {
                'title': 'Yann LeCun: ç°åœ¨çš„LLMè¿˜ä¸æ˜¯çœŸæ­£çš„æ™ºèƒ½',
                'summary': 'Metaé¦–å¸­AIç§‘å­¦å®¶Yann LeCunè®¤ä¸ºï¼Œç›®å‰çš„å¤§è¯­è¨€æ¨¡å‹ç¼ºä¹å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ï¼Œè·ç¦»çœŸæ­£çš„é€šç”¨äººå·¥æ™ºèƒ½è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚',
                'url': 'https://ai.meta.com/blog/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Interview',
                'category': 'leader',
                'author': 'Yann LeCun',
                'author_title': 'Meta Chief AI Scientist',
                'importance': 0.90
            },
            {
                'title': 'æå¼€å¤: AI 2.0æ—¶ä»£å·²ç»åˆ°æ¥',
                'summary': 'é›¶ä¸€ä¸‡ç‰©CEOæå¼€å¤è¡¨ç¤ºï¼ŒAI 2.0æ—¶ä»£å°†å¸¦æ¥æ¯”ç§»åŠ¨äº’è”ç½‘å¤§åå€çš„æœºä¼šï¼Œä¸­å›½åœ¨åº”ç”¨å±‚æœ‰å·¨å¤§ä¼˜åŠ¿ã€‚',
                'url': 'https://www.01.ai/',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Speech',
                'category': 'leader',
                'author': 'Kai-Fu Lee',
                'author_title': '01.AI CEO',
                'importance': 0.88
            }
        ]

    def _get_backup_research_data(self) -> List[Dict]:
        """å¤‡ç”¨ç ”ç©¶æ•°æ®"""
        return [
            {
                'title': 'Attention Is All You Need: Transformeræ¶æ„æ·±åº¦åˆ†æ',
                'summary': 'æ·±å…¥åˆ†æTransformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é©å‘½æ€§ä½œç”¨ï¼Œæ¢è®¨æ³¨æ„åŠ›æœºåˆ¶çš„åŸç†å’Œåº”ç”¨ã€‚',
                'authors': ['AI Research Team'],
                'url': 'https://arxiv.org/abs/1706.03762',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'categories': ['cs.CL', 'cs.AI'],
                'source': 'arXiv',
                'category': 'research',
                'importance': 0.95
            }
        ]
    
    def _get_backup_github_data(self) -> List[Dict]:
        """å¤‡ç”¨GitHubæ•°æ®"""
        return [
            {
                'title': 'transformers',
                'summary': 'ğŸ¤— Transformers: State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX.',
                'url': 'https://github.com/huggingface/transformers',
                'stars': 132000,
                'language': 'Python',
                'updated': datetime.now().strftime('%Y-%m-%d'),
                'source': 'GitHub',
                'category': 'developer',
                'importance': 0.98
            }
        ]
    
    def _get_backup_hf_data(self) -> List[Dict]:
        """å¤‡ç”¨Hugging Faceæ•°æ®"""
        return [
            {
                'title': 'HF Model: microsoft/DialoGPT-medium',
                'summary': 'æœ€æ–°AIæ¨¡å‹å‘å¸ƒ: microsoft/DialoGPT-mediumï¼Œä¸‹è½½é‡: 1500000',
                'url': 'https://huggingface.co/microsoft/DialoGPT-medium',
                'downloads': 1500000,
                'updated': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Hugging Face',
                'category': 'developer',
                'importance': 0.85
            }
        ]
    
    def _get_backup_blog_data(self) -> List[Dict]:
        """å¤‡ç”¨åšå®¢æ•°æ®"""
        return [
            {
                'title': 'GitHub Copilotæœ€æ–°åŠŸèƒ½æ›´æ–°',
                'summary': 'GitHub Copilotæ¨å‡ºæ–°åŠŸèƒ½ï¼Œæ”¯æŒæ›´å¤šç¼–ç¨‹è¯­è¨€å’Œæ›´æ™ºèƒ½çš„ä»£ç å»ºè®®ï¼Œæå‡å¼€å‘æ•ˆç‡ã€‚',
                'url': 'https://github.blog',
                'published': datetime.now().strftime('%Y-%m-%d'),
                'source': 'GitHub Blog',
                'category': 'developer',
                'importance': 0.80
            }
        ]
    
    # ============== å¼‚æ­¥é‡‡é›†æ–¹æ³• ==============
    
    async def _fetch_url_async(self, session: aiohttp.ClientSession, url: str,
                                semaphore: asyncio.Semaphore) -> Optional[str]:
        """å¼‚æ­¥è·å–URLå†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰"""
        async with semaphore:
            for attempt in range(self.async_config.max_retries + 1):
                try:
                    self.stats['requests_made'] += 1
                    await asyncio.sleep(self.async_config.rate_limit_delay)
                    
                    timeout = aiohttp.ClientTimeout(total=self.async_config.request_timeout)
                    async with session.get(url, headers=self.headers, timeout=timeout) as response:
                        if response.status == 200:
                            return await response.text()
                        elif response.status == 429:
                            wait_time = self.async_config.retry_delay * (2 ** attempt)
                            await asyncio.sleep(wait_time)
                        else:
                            return None
                except (asyncio.TimeoutError, aiohttp.ClientError):
                    if attempt < self.async_config.max_retries:
                        await asyncio.sleep(self.async_config.retry_delay * (attempt + 1))
                except Exception:
                    pass
            
            self.stats['requests_failed'] += 1
            return None
    
    async def _fetch_json_async(self, session: aiohttp.ClientSession, url: str,
                                 semaphore: asyncio.Semaphore, params: Optional[Dict] = None) -> Optional[Dict]:
        """å¼‚æ­¥è·å–JSONå†…å®¹"""
        async with semaphore:
            for attempt in range(self.async_config.max_retries + 1):
                try:
                    self.stats['requests_made'] += 1
                    await asyncio.sleep(self.async_config.rate_limit_delay)
                    
                    timeout = aiohttp.ClientTimeout(total=self.async_config.request_timeout)
                    async with session.get(url, headers=self.headers, timeout=timeout, params=params) as response:
                        if response.status == 200:
                            return await response.json()
                        elif response.status == 429:
                            wait_time = self.async_config.retry_delay * (2 ** attempt)
                            await asyncio.sleep(wait_time)
                except (asyncio.TimeoutError, aiohttp.ClientError):
                    if attempt < self.async_config.max_retries:
                        await asyncio.sleep(self.async_config.retry_delay * (attempt + 1))
                except Exception:
                    pass
            
            self.stats['requests_failed'] += 1
            return None
    
    async def _parse_rss_feed_async(self, session: aiohttp.ClientSession,
                                     feed_url: str, category: str,
                                     semaphore: asyncio.Semaphore) -> List[Dict]:
        """å¼‚æ­¥è§£æRSSæº"""
        items = []
        try:
            content = await self._fetch_url_async(session, feed_url, semaphore)
            if not content:
                return items
            
            loop = asyncio.get_event_loop()
            feed = await loop.run_in_executor(None, feedparser.parse, content)
            
            for entry in feed.entries[:10]:
                date_val = entry.get('published_parsed') or entry.get('published')
                if date_val and not self._is_recent(date_val):
                    continue
                
                item = {
                    'title': entry.get('title', ''),
                    'summary': (entry.get('summary', entry.get('description', ''))[:300] + "...") 
                               if len(entry.get('summary', entry.get('description', ''))) > 300 else 
                               entry.get('summary', entry.get('description', '')),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'source': feed.feed.get('title', feed_url)[:50],
                    'category': category,
                    'importance': 0.6
                }
                
                if self._is_valid_item(item):
                    items.append(item)
        except Exception:
            pass
        
        return items
    
    async def _collect_all_async(self) -> Dict[str, List[Dict]]:
        """
        å¼‚æ­¥é‡‡é›†æ‰€æœ‰ç±»å‹çš„æ•°æ®
        
        Returns:
            åˆ†ç±»çš„æ•°æ®å­—å…¸
        """
        self.stats['start_time'] = time.time()
        log.dual_start(t('dc_start_collection'))
        log.dual_separator("=", 50)
        log.dual_info("ğŸš€ å¼‚æ­¥é‡‡é›†æ¨¡å¼ (Async Mode)", emoji="")
        
        all_data = {
            'research': [],
            'developer': [],
            'product': [],
            'news': [],
            'leader': [],
            'community': []
        }
        
        # ä»é…ç½®è¯»å–é‡‡é›†æ•°é‡
        product_count = config.get('collector.product_count', 15)
        community_count = config.get('collector.community_count', 10)
        leader_count = config.get('collector.leader_count', 15)
        research_count = config.get('collector.research_count', 15)
        developer_count = config.get('collector.developer_count', 20)
        news_count = config.get('collector.news_count', 25)
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.async_config.max_concurrent_requests)
        
        # åˆ›å»ºå…±äº«çš„aiohttpä¼šè¯
        connector = aiohttp.TCPConnector(
            limit=self.async_config.max_concurrent_requests,
            limit_per_host=self.async_config.max_concurrent_per_host
        )
        timeout = aiohttp.ClientTimeout(total=self.async_config.total_timeout)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # å¼‚æ­¥é‡‡é›†å„ç±»æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨RSSé‡‡é›†ï¼‰
            news_feeds = RSS_FEEDS['news'] + RSS_FEEDS.get('product_news', [])
            developer_feeds = RSS_FEEDS['developer']
            
            # å¹¶å‘é‡‡é›†æ‰€æœ‰RSSæº
            news_tasks = [
                self._parse_rss_feed_async(session, feed_url, 'news', semaphore)
                for feed_url in news_feeds
            ]
            
            developer_tasks = [
                self._parse_rss_feed_async(session, feed_url, 'developer', semaphore)
                for feed_url in developer_feeds
            ]
            
            # åŒæ—¶é‡‡é›†
            all_results = await asyncio.gather(
                *news_tasks, *developer_tasks,
                return_exceptions=True
            )
            
            # æ”¶é›†ç»“æœ
            for result in all_results:
                if isinstance(result, list):
                    for item in result:
                        category = item.get('category', 'news')
                        all_data[category].append(item)
            
            # åŒæ­¥é‡‡é›†ç ”ç©¶è®ºæ–‡ï¼ˆarxivåº“ä¸æ”¯æŒå¼‚æ­¥ï¼‰
            all_data['research'] = self.collect_research_papers(research_count)
            all_data['product'] = self.collect_product_releases(product_count)
            all_data['leader'] = self.collect_ai_leaders_quotes(leader_count)
            all_data['community'] = self.collect_community_trends(community_count)
        
        # ç»Ÿè®¡æ–°æ—§å†…å®¹
        new_stats = {}
        cached_stats = {}
        new_items_for_cache = []
        
        for cat in all_data:
            new_count = 0
            cached_count = 0
            for item in all_data[cat]:
                if self._is_in_history(item):
                    cached_count += 1
                else:
                    new_count += 1
                    new_items_for_cache.append(item)
            new_stats[cat] = new_count
            cached_stats[cat] = cached_count
        
        # æ›´æ–°å†å²ç¼“å­˜
        for item in new_items_for_cache:
            self._add_to_history(item)
        
        if new_items_for_cache:
            self._save_history_cache()
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats['end_time'] = time.time()
        self.stats['items_collected'] = sum(len(items) for items in all_data.values())
        
        # æ‰“å°ç»Ÿè®¡
        total_items = self.stats['items_collected']
        total_new = sum(new_stats.values())
        total_cached = sum(cached_stats.values())
        elapsed = self.stats['end_time'] - self.stats['start_time']
        
        log.dual_separator("=", 50)
        log.dual_done(f"é‡‡é›†å®Œæˆ: {total_items} items ({total_new} new, {total_cached} cached)")
        log.dual_info(f"â±ï¸ è€—æ—¶: {elapsed:.1f}s | è¯·æ±‚: {self.stats['requests_made']} | å¤±è´¥: {self.stats['requests_failed']}", emoji="")
        
        for category, items in all_data.items():
            new_count = new_stats.get(category, 0)
            log.dual_data(f"  {category}: {len(items)} ({new_count} new)")
        
        return all_data


# ç”¨äºå‘åå…¼å®¹
DataCollector = AIDataCollector

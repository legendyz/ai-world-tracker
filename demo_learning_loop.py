"""
å®Œæ•´çš„å­¦ä¹ åé¦ˆé—­ç¯æ¼”ç¤º
å±•ç¤ºä»è‡ªåŠ¨åˆ†ç±» â†’ äººå·¥å®¡æ ¸ â†’ å­¦ä¹ åé¦ˆ â†’ æ¨¡å‹ä¼˜åŒ–çš„å…¨æµç¨‹
"""

import json
from content_classifier import ContentClassifier
from manual_reviewer import ManualReviewer
from learning_feedback import LearningFeedback

print("="*80)
print("ğŸ”„ å­¦ä¹ åé¦ˆé—­ç¯ç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º")
print("="*80)

# æ¨¡æ‹Ÿæ•°æ®ï¼šè‡ªåŠ¨åˆ†ç±»ç»“æœï¼ˆåŒ…å«ä¸€äº›é”™è¯¯ï¼‰
test_data = [
    {
        'title': 'Baidu secures $500M in AI funding round',
        'summary': 'Chinese tech giant raises new capital for AI research',
        'source': 'TechNews',
        'content_type': 'product',  # é”™è¯¯ï¼šåº”è¯¥æ˜¯market
        'confidence': 0.45,
        'needs_review': True,
        'tech_categories': ['General AI'],
        'region': 'China'
    },
    {
        'title': 'OpenAI announces GPT-5 official release',
        'summary': 'Company unveils next generation language model available now',
        'source': 'OpenAI Blog',
        'content_type': 'product',  # æ­£ç¡®
        'confidence': 0.92,
        'needs_review': False,
        'tech_categories': ['Generative AI'],
        'region': 'USA'
    },
    {
        'title': 'Research paper on arXiv with GitHub code',
        'summary': 'Academic study about transformer architecture with implementation',
        'source': 'arXiv',
        'content_type': 'research',  # æ­£ç¡®ï¼Œä½†åº”è¯¥æœ‰developeræ¬¡è¦æ ‡ç­¾
        'confidence': 0.88,
        'needs_review': False,
        'tech_categories': ['NLP'],
        'region': 'Global'
    },
    {
        'title': 'DeepMind completes Series C funding',
        'summary': 'Google subsidiary raises capital for AGI development',
        'source': 'VentureBeat',
        'content_type': 'product',  # é”™è¯¯ï¼šåº”è¯¥æ˜¯market
        'confidence': 0.38,
        'needs_review': True,
        'tech_categories': ['General AI'],
        'region': 'Europe'
    },
    {
        'title': 'Spam: Free AI tools download now!!!',
        'summary': 'Click here for amazing offers',
        'source': 'Unknown',
        'content_type': 'market',  # é”™è¯¯ï¼šåº”è¯¥åˆ é™¤
        'confidence': 0.15,
        'needs_review': True,
        'tech_categories': ['General AI'],
        'region': 'Global'
    }
]

print("\n" + "="*80)
print("é˜¶æ®µ 1: è‡ªåŠ¨åˆ†ç±»ç»“æœ")
print("="*80)

classifier = ContentClassifier()
reviewer = ManualReviewer()

print(f"\næ€»æ•°æ®: {len(test_data)} æ¡")
review_needed = [item for item in test_data if item.get('needs_review')]
print(f"éœ€è¦å®¡æ ¸: {len(review_needed)} æ¡ ({len(review_needed)/len(test_data):.0%})")

print("\nè‡ªåŠ¨åˆ†ç±»ç»“æœæ¦‚è§ˆ:")
for i, item in enumerate(test_data, 1):
    status = "âš ï¸" if item.get('needs_review') else "âœ…"
    print(f"   {i}. {status} {item['title'][:45]}...")
    print(f"      åˆ†ç±»: {item['content_type']} | ç½®ä¿¡åº¦: {item['confidence']:.0%}")

print("\n" + "="*80)
print("é˜¶æ®µ 2: æ¨¡æ‹Ÿäººå·¥å®¡æ ¸")
print("="*80)

print("\näººå·¥å®¡æ ¸æ“ä½œï¼ˆæ¨¡æ‹Ÿï¼‰:")

# æ¨¡æ‹Ÿå®¡æ ¸ä¿®æ­£
corrections = [
    (0, 'market', 'èèµ„æ–°é—»åº”è¯¥æ˜¯marketç±»'),
    (3, 'market', 'èèµ„æ–°é—»åº”è¯¥æ˜¯marketç±»'),
    (4, None, 'æ ‡è®°ä¸ºåƒåœ¾å¹¶åˆ é™¤')
]

for idx, new_cat, reason in corrections:
    item = test_data[idx]
    print(f"\n   ä¿®æ­£ {idx+1}: {item['title'][:40]}...")
    print(f"      åŸåˆ†ç±»: {item['content_type']} â†’ ", end='')
    
    if new_cat:
        old_cat = item['content_type']
        item['content_type'] = new_cat
        item['confidence'] = 1.0
        item['manually_reviewed'] = True
        item['original_category'] = old_cat
        item['original_confidence'] = item.get('confidence', 0)
        reviewer._add_to_history(item, f'ä¿®æ”¹åˆ†ç±»: {old_cat} â†’ {new_cat}')
        print(f"{new_cat} âœ“")
    else:
        item['is_spam'] = True
        item['manually_reviewed'] = True
        reviewer._add_to_history(item, 'æ ‡è®°ä¸ºåƒåœ¾')
        print("åˆ é™¤ ğŸ—‘ï¸")
    
    print(f"      ç†ç”±: {reason}")

# ç§»é™¤åƒåœ¾å†…å®¹
test_data = [item for item in test_data if not item.get('is_spam')]

print(f"\nâœ… å®¡æ ¸å®Œæˆï¼å‰©ä½™ {len(test_data)} æ¡æœ‰æ•ˆå†…å®¹")

print("\n" + "="*80)
print("é˜¶æ®µ 3: å­¦ä¹ åé¦ˆåˆ†æ")
print("="*80)

learner = LearningFeedback()

# åˆ†æå®¡æ ¸å†å²
print("\nğŸ“Š åˆ†æå®¡æ ¸æ¨¡å¼...")
analysis = learner.analyze_review_history(reviewer.review_history)

print(f"\nå®¡æ ¸ç»Ÿè®¡:")
print(f"   æ€»å®¡æ ¸: {analysis['total_reviews']} æ¡")
print(f"   ä¿®æ­£: {analysis['corrections']} æ¡")
print(f"   åˆ é™¤åƒåœ¾: {analysis['spam_removed']} æ¡")

if analysis['common_transitions']:
    print(f"\nå¸¸è§è½¬æ¢:")
    for transition, count in analysis['common_transitions']:
        print(f"   {transition}: {count} æ¬¡")

# æå–å…³é”®è¯æ¨¡å¼
print("\nğŸ” æå–å…³é”®è¯æ¨¡å¼...")
patterns = learner.extract_keyword_patterns(test_data)

print(f"\nå‘ç°çš„æ¨¡å¼:")
for category, keywords in patterns.items():
    if keywords:
        print(f"   {category}: {', '.join(keywords[:3])}...")

# ç”Ÿæˆæ”¹è¿›å»ºè®®
print("\nâš™ï¸ ç”Ÿæˆæ”¹è¿›å»ºè®®...")
adjustments = learner.generate_weight_adjustments(analysis)

print(f"\næ”¹è¿›å»ºè®®:")
for category, adj in adjustments.get('category_thresholds', {}).items():
    print(f"\n   åˆ†ç±»: {category}")
    print(f"   é—®é¢˜: {adj['issue']}")
    print(f"   å»ºè®®: {adj['suggestion']}")
    print(f"   é¢‘ç‡: {adj['frequency']} æ¬¡")

print("\n" + "="*80)
print("é˜¶æ®µ 4: åº”ç”¨æ”¹è¿›ï¼ˆç¤ºä¾‹ï¼‰")
print("="*80)

print("\nğŸ’¡ åŸºäºåˆ†æç»“æœï¼Œåº”è¯¥è¿›è¡Œä»¥ä¸‹æ”¹è¿›:")

print("\n1. æ·»åŠ èèµ„æ£€æµ‹è§„åˆ™:")
print("   ```python")
print("   # åœ¨ classify_content_type() ä¸­æ·»åŠ ")
print("   funding_keywords = ['funding', 'raises', 'secures', 'èèµ„']")
print("   has_funding = any(word in text for word in funding_keywords)")
print("   ")
print("   if has_funding:")
print("       scores['market'] *= 2.0")
print("       scores['product'] *= 0.5")
print("   ```")

print("\n2. æ·»åŠ åƒåœ¾å†…å®¹è¿‡æ»¤:")
print("   ```python")
print("   spam_indicators = ['click here', 'download now', '!!!']")
print("   if any(spam in text.lower() for spam in spam_indicators):")
print("       return 'spam', 0.0, []  # ç›´æ¥æ ‡è®°ä¸ºåƒåœ¾")
print("   ```")

print("\n3. å¢å¼ºå¤šæ ‡ç­¾æ”¯æŒ:")
print("   ```python")
print("   # å¯¹äºGitHubä¸Šçš„ç ”ç©¶é¡¹ç›®")
print("   if 'github' in source and 'arxiv' in text:")
print("       secondary_labels.append('research')")
print("   ```")

print("\n" + "="*80)
print("é˜¶æ®µ 5: éªŒè¯æ”¹è¿›æ•ˆæœ")
print("="*80)

print("\né¢„æœŸæ”¹è¿›æ•ˆæœ:")
print("   âœ… èèµ„æ–°é—»ä¸å†è¢«è¯¯åˆ¤ä¸ºäº§å“å‘å¸ƒ")
print("   âœ… åƒåœ¾å†…å®¹è¢«è‡ªåŠ¨è¿‡æ»¤")
print("   âœ… å¹³å‡ç½®ä¿¡åº¦æå‡ 10-15%")
print("   âœ… éœ€è¦äººå·¥å®¡æ ¸çš„æ¯”ä¾‹ä» 60% é™è‡³ 10%")

print("\n" + "="*80)
print("ğŸ‰ å­¦ä¹ åé¦ˆé—­ç¯æ¼”ç¤ºå®Œæˆï¼")
print("="*80)

print("\nğŸ“ æ€»ç»“:")
print("   1. è‡ªåŠ¨åˆ†ç±»å™¨äº§ç”Ÿåˆå§‹ç»“æœ")
print("   2. äººå·¥å®¡æ ¸ä¿®æ­£é”™è¯¯ï¼ˆ60% éœ€è¦å®¡æ ¸ï¼‰")
print("   3. å­¦ä¹ ç³»ç»Ÿåˆ†æå®¡æ ¸æ¨¡å¼")
print("   4. ç”Ÿæˆå…·ä½“çš„æ”¹è¿›å»ºè®®")
print("   5. åº”ç”¨æ”¹è¿›åï¼Œå‡†ç¡®ç‡æå‡")
print("   6. é‡å¤å¾ªç¯ï¼ŒæŒç»­ä¼˜åŒ–")

print("\nğŸ’¡ åœ¨å®é™…ä½¿ç”¨ä¸­:")
print("   python TheWorldOfAI.py")
print("   â†’ é€‰æ‹© 5. äººå·¥å®¡æ ¸")
print("   â†’ å®Œæˆå®¡æ ¸")
print("   â†’ é€‰æ‹© 6. å­¦ä¹ åé¦ˆ")
print("   â†’ æŸ¥çœ‹å¹¶åº”ç”¨æ”¹è¿›å»ºè®®")
print("   â†’ é‡æ–°è¿è¡ŒéªŒè¯æ•ˆæœ")

print("\nğŸ”— ç›¸å…³æ–‡æ¡£:")
print("   - LEARNING_FEEDBACK_GUIDE.md - è¯¦ç»†ä½¿ç”¨æŒ‡å—")
print("   - MANUAL_REVIEW_GUIDE.md - äººå·¥å®¡æ ¸æŒ‡å—")
print("   - CLASSIFIER_IMPROVEMENTS.md - åˆ†ç±»å™¨æ”¹è¿›è¯´æ˜")

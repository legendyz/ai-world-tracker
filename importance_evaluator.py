"""
é‡è¦æ€§è¯„ä¼°å™¨ - Importance Evaluator
ç‹¬ç«‹çš„å¤šç»´åº¦é‡è¦æ€§è¯„ä¼°æ¨¡å—

è¯„ä¼°ç»´åº¦:
1. æ¥æºæƒå¨åº¦ (source_authority) - 25%
2. æ—¶æ•ˆæ€§ (recency) - 25%
3. åˆ†ç±»ç½®ä¿¡åº¦ (confidence) - 20%
4. å†…å®¹ç›¸å…³åº¦ (relevance) - 20%
5. ç¤¾äº¤çƒ­åº¦ (engagement) - 10%

AIç›¸å…³æ€§è°ƒæ•´:
- ai_relevance ä½œä¸ºä¹˜æ•°è°ƒæ•´æœ€ç»ˆå¾—åˆ†
- é«˜ç›¸å…³(>0.8): è½»å¾®åŠ æˆ (1.0-1.05)
- ä¸­ç­‰ç›¸å…³(0.5-0.8): è½»å¾®æƒ©ç½š (0.85-1.0)
- ä½ç›¸å…³(0.3-0.5): ä¸­ç­‰æƒ©ç½š (0.6-0.85)
- æä½ç›¸å…³(<0.3): å¤§å¹…æƒ©ç½š (0.3-0.6)

è¯¥æ¨¡å—ç‹¬ç«‹äºåˆ†ç±»å™¨ï¼Œå¯è¢«è§„åˆ™åˆ†ç±»å™¨å’ŒLLMåˆ†ç±»å™¨å…±åŒä½¿ç”¨ã€‚
"""

from typing import Dict, Tuple, Optional
from datetime import datetime
from dateutil import parser as date_parser
import math
import json
import os
from collections import defaultdict
from logger import get_log_helper

# æ¨¡å—æ—¥å¿—å™¨
log = get_log_helper('importance_evaluator')

# åŠ¨æ€å­¦ä¹ é…ç½®æ–‡ä»¶
LEARNING_CONFIG_FILE = 'data/cache/importance_learning.json'


class ImportanceEvaluator:
    """
    å¤šç»´åº¦é‡è¦æ€§è¯„ä¼°å™¨
    
    è¯„ä¼°ç»´åº¦:
    1. æ¥æºæƒå¨åº¦ (source_authority) - 25%
    2. æ—¶æ•ˆæ€§ (recency) - 25%
    3. åˆ†ç±»ç½®ä¿¡åº¦ (confidence) - 20% (å¯¹ä½ä»·å€¼å†…å®¹è®¾ä¸Šé™)
    4. å†…å®¹ç›¸å…³åº¦ (relevance) - 20%
    5. ç¤¾äº¤çƒ­åº¦ (engagement) - 10%
    
    AIç›¸å…³æ€§è°ƒæ•´ (ai_relevance):
    - ä½œä¸ºä¹˜æ•°å› å­è°ƒæ•´æœ€ç»ˆå¾—åˆ†
    - ä½AIç›¸å…³æ€§å†…å®¹ä¼šè¢«é™æƒ
    """
    
    def __init__(self):
        # ç»´åº¦æƒé‡é…ç½® (æ”¯æŒåŠ¨æ€è°ƒæ•´)
        self.weights = {
            'source_authority': 0.25,
            'recency': 0.25,
            'confidence': 0.20,
            'relevance': 0.20,
            'engagement': 0.10
        }
        
        # åŠ¨æ€å­¦ä¹ æ•°æ®
        self.source_performance = defaultdict(lambda: {'scores': [], 'count': 0, 'avg': 0.5})
        self.user_feedback_count = 0
        
        # åŠ è½½å†å²å­¦ä¹ æ•°æ®
        self._load_learning_data()
        
        # æ¥æºæƒå¨åº¦è¯„åˆ†
        self.source_authority_scores = {
            # å®˜æ–¹ä¸€æ‰‹æ¥æº (0.9-1.0)
            'openai.com': 1.0,
            'openai': 1.0,
            'blog.google': 1.0,
            'google ai': 0.95,
            'ai.meta.com': 1.0,
            'meta ai': 0.95,
            'anthropic.com': 1.0,
            'anthropic': 0.95,
            'microsoft.com': 0.95,
            'blogs.microsoft': 0.95,
            'nvidia': 0.90,
            'arxiv.org': 0.95,
            'arxiv': 0.95,
            'github.com': 0.90,
            'github': 0.90,
            'huggingface.co': 0.90,
            'hugging face': 0.90,
            
            # ä¸­å›½AIå…¬å¸å®˜æ–¹
            'baidu': 0.90,
            'ç™¾åº¦': 0.90,
            'alibaba': 0.90,
            'é˜¿é‡Œ': 0.90,
            'tencent': 0.90,
            'è…¾è®¯': 0.90,
            'deepseek': 0.90,
            'æ™ºè°±': 0.85,
            'æœˆä¹‹æš—é¢': 0.85,
            'kimi': 0.85,
            
            # ä¸“ä¸šåª’ä½“ (0.7-0.85)
            'techcrunch': 0.85,
            'theverge': 0.80,
            'the verge': 0.80,
            'wired': 0.80,
            'technologyreview': 0.85,
            'mit technology review': 0.85,
            'ieee spectrum': 0.85,
            'artificialintelligence-news': 0.80,
            'syncedreview': 0.80,
            'æœºå™¨ä¹‹å¿ƒ': 0.85,
            'jiqizhixin': 0.85,
            'é‡å­ä½': 0.80,
            'qbitai': 0.80,
            'infoq': 0.75,
            '36kr': 0.70,
            '36æ°ª': 0.70,
            'ithome': 0.70,
            'itä¹‹å®¶': 0.70,
            
            # ç¤¾åŒº/èšåˆ (0.5-0.7)
            'reddit': 0.65,
            'producthunt': 0.70,
            'product hunt': 0.70,
            'hacker news': 0.70,
            'hnrss': 0.70,
            
            # é€šç”¨æ–°é—» (0.4-0.6)
            'news.google': 0.50,
            'bing.com/news': 0.50,
            'reuters': 0.75,
            'bloomberg': 0.75,
            
            # ä¸ªäººåšå®¢/æ’­å®¢
            'sam altman': 0.90,
            'karpathy': 0.90,
            'andrej karpathy': 0.90,
            'lex fridman': 0.80,
        }
        
        # é«˜ä»·å€¼å…³é”®è¯ (ç”¨äºç›¸å…³åº¦è®¡ç®—) - åˆ†å±‚æƒé‡ç³»ç»Ÿ
        # ç¬¬ä¸€å±‚ï¼šçªç ´æ€§/é‡Œç¨‹ç¢‘äº‹ä»¶ (0.12-0.18)
        self.breakthrough_keywords = {
            'breakthrough': 0.18, 'sota': 0.15, 'state-of-the-art': 0.15,
            'world record': 0.15, 'revolutionary': 0.14, 'game-changer': 0.14,
            'milestone': 0.12, 'paradigm shift': 0.15, 'first-ever': 0.14,
            # ä¸­æ–‡
            'çªç ´': 0.18, 'é‡Œç¨‹ç¢‘': 0.14, 'é©å‘½æ€§': 0.14, 'é¢ è¦†': 0.12,
            'å²ä¸Šé¦–æ¬¡': 0.15, 'é‡å¤§çªç ´': 0.16,
        }
        
        # ç¬¬äºŒå±‚ï¼šå‘å¸ƒ/å…¬å‘Šäº‹ä»¶ (0.08-0.12)
        self.release_keywords = {
            'release': 0.10, 'launch': 0.10, 'announce': 0.10, 'unveil': 0.12,
            'introduce': 0.08, 'available': 0.08, 'official': 0.10,
            'beta': 0.06, 'preview': 0.06, 'alpha': 0.05,
            'general availability': 0.10, 'ga': 0.08, 'v1.0': 0.08,
            # ä¸­æ–‡
            'å‘å¸ƒ': 0.10, 'æ¨å‡º': 0.10, 'ä¸Šçº¿': 0.10, 'æ­£å¼ç‰ˆ': 0.10,
            'å®˜å®£': 0.10, 'å®˜æ–¹': 0.08, 'å…¬æµ‹': 0.06, 'å†…æµ‹': 0.05,
        }
        
        # ç¬¬ä¸‰å±‚ï¼šæŠ€æœ¯/æ¨¡å‹ç›¸å…³ (0.05-0.10)
        self.tech_keywords = {
            'open source': 0.10, 'open-source': 0.10, 'opensource': 0.10,
            'benchmark': 0.08, 'evaluation': 0.06, 'paper': 0.06,
            'gpt': 0.06, 'llm': 0.06, 'transformer': 0.05, 'diffusion': 0.06,
            'multimodal': 0.08, 'reasoning': 0.08, 'chain-of-thought': 0.08,
            'agent': 0.08, 'agi': 0.10, 'agentic': 0.08,
            'fine-tune': 0.06, 'finetune': 0.06, 'rlhf': 0.07,
            'inference': 0.05, 'training': 0.05, 'dataset': 0.06,
            # ä¸­æ–‡
            'å¼€æº': 0.10, 'æ¨¡å‹': 0.05, 'å¤§æ¨¡å‹': 0.07, 'å¤šæ¨¡æ€': 0.08,
            'æ¨ç†': 0.06, 'è®­ç»ƒ': 0.05, 'å¾®è°ƒ': 0.06, 'æ•°æ®é›†': 0.06,
        }
        
        # ç¬¬å››å±‚ï¼šä¸€èˆ¬æ€§æè¿° (0.02-0.05)
        self.general_keywords = {
            'new': 0.03, 'update': 0.04, 'improve': 0.04, 'enhance': 0.04,
            'feature': 0.03, 'support': 0.03, 'capability': 0.04,
            'performance': 0.05, 'faster': 0.04, 'better': 0.03,
            # ä¸­æ–‡
            'æœ€æ–°': 0.04, 'æ›´æ–°': 0.04, 'å‡çº§': 0.05, 'ä¼˜åŒ–': 0.04,
            'æ–°å¢': 0.04, 'æ”¯æŒ': 0.03, 'åŠŸèƒ½': 0.03,
        }
        
        # è´Ÿé¢/é™æƒå…³é”®è¯
        self.negative_relevance_keywords = {
            'rumor': -0.08, 'speculation': -0.06, 'might': -0.03, 'may': -0.02,
            'could': -0.02, 'possibly': -0.04, 'unconfirmed': -0.08,
            'alleged': -0.06, 'reportedly': -0.04,
            # ä¸­æ–‡
            'ä¼ é—»': -0.08, 'æ®æ‚‰': -0.04, 'æˆ–å°†': -0.04, 'å¯èƒ½': -0.03,
            'ç–‘ä¼¼': -0.06, 'æœªç»è¯å®': -0.08,
        }
        
        # å†…å®¹ç±»å‹ç›¸å…³åº¦ç³»æ•°
        self.type_relevance_multipliers = {
            'research': 1.15,   # ç ”ç©¶ç±»é€šå¸¸ç›¸å…³åº¦é«˜
            'product': 1.12,    # äº§å“å‘å¸ƒé‡è¦
            'leader': 1.08,     # é¢†è¢–è¨€è®º
            'developer': 1.05,  # å¼€å‘è€…å†…å®¹
            'tutorial': 1.0,    # æ•™ç¨‹å†…å®¹
            'news': 0.95,       # æ–°é—»å¯èƒ½æ³›æ³›è€Œè°ˆ
            'market': 0.88,     # å¸‚åœºåˆ†æ
            'community': 0.85,  # ç¤¾åŒºè®¨è®º
            'opinion': 0.80,    # è§‚ç‚¹è¯„è®º
            'other': 0.75,      # å…¶ä»–å†…å®¹
        }
        
        # æ—¶æ•ˆæ€§è¡°å‡å‚æ•°ï¼ˆæ”¯æŒæŒ‰å†…å®¹ç±»å‹è°ƒæ•´ï¼‰
        self.recency_decay_rate = 0.12  # è¡°å‡ç‡ï¼Œå€¼è¶Šå¤§è¡°å‡è¶Šå¿«
        self.recency_min_score = 0.08   # æœ€ä½æ—¶æ•ˆåˆ†æ•°
        
        # ä¸åŒå†…å®¹ç±»å‹çš„æ—¶æ•ˆæ€§è¡°å‡ç‡
        self.type_decay_rates = {
            'product': 0.15,   # äº§å“å‘å¸ƒè¡°å‡å¿«ï¼ˆæ—¶æ•ˆæ€§æ›´é‡è¦ï¼‰
            'news': 0.14,      # æ–°é—»è¡°å‡å¿«
            'market': 0.10,    # å¸‚åœºåˆ†æè¡°å‡æ…¢ä¸€äº›
            'research': 0.08,  # ç ”ç©¶è®ºæ–‡è¡°å‡æœ€æ…¢ï¼ˆæŒä¹…ä»·å€¼ï¼‰
            'tutorial': 0.06,  # æ•™ç¨‹æ›´æŒä¹…
            'leader': 0.10,    # é¢†è¢–è¨€è®º
        }
        
        # ç¤¾äº¤çƒ­åº¦ç»Ÿä¸€é…ç½®
        self.engagement_config = {
            'github_stars': {'threshold_low': 100, 'threshold_high': 50000, 'weight': 1.0},
            'huggingface_downloads': {'threshold_low': 1000, 'threshold_high': 1000000, 'weight': 0.9},
            'reddit_score': {'threshold_low': 50, 'threshold_high': 5000, 'weight': 0.85},
            'hn_points': {'threshold_low': 30, 'threshold_high': 1000, 'weight': 0.85},
            'likes': {'threshold_low': 100, 'threshold_high': 10000, 'weight': 0.7},
            'comments': {'threshold_low': 20, 'threshold_high': 500, 'weight': 0.6},
        }
    
    def calculate_importance(self, item: Dict, 
                            classification_result: Optional[Dict] = None) -> Tuple[float, Dict]:
        """
        è®¡ç®—å¤šç»´åº¦é‡è¦æ€§åˆ†æ•°
        
        Args:
            item: åŸå§‹æ•°æ®é¡¹
            classification_result: åˆ†ç±»ç»“æœï¼ŒåŒ…å« content_type, confidence, ai_relevance ç­‰
            
        Returns:
            (importance_score, score_breakdown)
        """
        if classification_result is None:
            classification_result = {}
        
        breakdown = {}
        
        # è·å–å†…å®¹ç±»å‹ï¼ˆç”¨äºè‡ªé€‚åº”è¯„åˆ†ï¼‰
        content_type = classification_result.get('content_type', 'news')
        
        # 1. æ¥æºæƒå¨åº¦ (0-1)
        source_score = self._calculate_source_authority(item)
        breakdown['source_authority'] = round(source_score, 3)
        
        # 2. æ—¶æ•ˆæ€§ (0-1) - æ ¹æ®å†…å®¹ç±»å‹è‡ªé€‚åº”è¡°å‡
        recency_score = self._calculate_recency(item, content_type)
        breakdown['recency'] = round(recency_score, 3)
        
        # 3. åˆ†ç±»ç½®ä¿¡åº¦ (0-1) - å¯¹ä½ä»·å€¼å†…å®¹è®¾ç½®ä¸Šé™
        confidence = classification_result.get('confidence', 0.5)
        # ä½æ—¶æ•ˆå†…å®¹ï¼ˆ>14å¤©ï¼‰é™åˆ¶ç½®ä¿¡åº¦è´¡çŒ®
        if recency_score <= 0.50:  # 14å¤©ä»¥ä¸Šçš„å†…å®¹
            if source_score < 0.80:  # éå®˜æ–¹é«˜æƒå¨æ¥æº
                confidence = min(confidence, 0.60)  # ç½®ä¿¡åº¦ä¸Šé™60%
            else:
                confidence = min(confidence, 0.75)  # å®˜æ–¹æ¥æºä¸Šé™75%
        elif recency_score <= 0.70:  # 7-14å¤©çš„å†…å®¹
            if source_score < 0.70:
                confidence = min(confidence, 0.75)  # æ™®é€šæ¥æºä¸Šé™75%
        breakdown['confidence'] = round(confidence, 3)
        
        # 4. å†…å®¹ç›¸å…³åº¦ (0-1)
        relevance_score = self._calculate_relevance(item, content_type)
        breakdown['relevance'] = round(relevance_score, 3)
        
        # 5. ç¤¾äº¤çƒ­åº¦ (0-1)
        engagement_score = self._calculate_engagement(item)
        breakdown['engagement'] = round(engagement_score, 3)
        
        # 6. AIç›¸å…³æ€§è°ƒæ•´ (æ–°å¢)
        # ai_relevance ç”¨äºæƒ©ç½šéAIç›¸å…³å†…å®¹
        ai_relevance = classification_result.get('ai_relevance', 0.7)  # é»˜è®¤0.7ï¼ˆå‡è®¾å¤§éƒ¨åˆ†é‡‡é›†å†…å®¹AIç›¸å…³ï¼‰
        breakdown['ai_relevance'] = round(ai_relevance, 3)
        
        # åŠ æƒæ±‚å’Œ
        total_score = (
            source_score * self.weights['source_authority'] +
            recency_score * self.weights['recency'] +
            confidence * self.weights['confidence'] +
            relevance_score * self.weights['relevance'] +
            engagement_score * self.weights['engagement']
        )
        
        # åº”ç”¨AIç›¸å…³æ€§è°ƒæ•´
        # é«˜ç›¸å…³(>0.7): ä¸è°ƒæ•´æˆ–ç•¥å¾®åŠ æˆ
        # ä¸­ç­‰ç›¸å…³(0.5-0.7): è½»å¾®æƒ©ç½š
        # ä½ç›¸å…³(<0.5): è¾ƒå¤§æƒ©ç½š
        # æä½ç›¸å…³(<0.3): å¤§å¹…æƒ©ç½š
        if ai_relevance >= 0.8:
            ai_multiplier = 1.0 + (ai_relevance - 0.8) * 0.25  # 0.8-1.0 è½»å¾®åŠ æˆ (1.0-1.05)
        elif ai_relevance >= 0.5:
            ai_multiplier = 0.85 + (ai_relevance - 0.5) * 0.5  # 0.5-0.8 è½»å¾®æƒ©ç½š (0.85-1.0)
        elif ai_relevance >= 0.3:
            ai_multiplier = 0.6 + (ai_relevance - 0.3) * 1.25  # 0.3-0.5 ä¸­ç­‰æƒ©ç½š (0.6-0.85)
        else:
            ai_multiplier = 0.3 + ai_relevance  # <0.3 å¤§å¹…æƒ©ç½š (0.3-0.6)
        
        total_score *= ai_multiplier
        breakdown['ai_multiplier'] = round(ai_multiplier, 3)
        
        # ç¡®ä¿åœ¨ 0-1 èŒƒå›´å†…
        importance = round(min(max(total_score, 0.0), 1.0), 3)
        
        return importance, breakdown
    
    def _calculate_source_authority(self, item: Dict) -> float:
        """
        è®¡ç®—æ¥æºæƒå¨åº¦ - ç»“åˆé™æ€è¯„åˆ†å’ŒåŠ¨æ€å­¦ä¹ 
        
        Args:
            item: æ•°æ®é¡¹
            
        Returns:
            æƒå¨åº¦åˆ†æ•° 0-1
        """
        source = item.get('source', '').lower()
        url = item.get('url', '').lower()
        author = item.get('author', '').lower()
        
        # åˆå¹¶æ£€æŸ¥æ–‡æœ¬
        check_text = f"{source} {url} {author}"
        
        # é™æ€è¯„åˆ†ï¼šæŸ¥æ‰¾åŒ¹é…çš„æ¥æº
        static_score = 0.40  # é»˜è®¤å€¼
        matched_source = None
        
        for known_source, score in self.source_authority_scores.items():
            if known_source.lower() in check_text:
                if score > static_score:
                    static_score = score
                    matched_source = known_source
        
        # åŠ¨æ€è¯„åˆ†ï¼šä»å†å²è¡¨ç°å­¦ä¹ 
        dynamic_score = None
        if matched_source and matched_source in self.source_performance:
            perf = self.source_performance[matched_source]
            if perf['count'] >= 5:  # è‡³å°‘5ä¸ªæ ·æœ¬æ‰å¯ç”¨åŠ¨æ€è¯„åˆ†
                dynamic_score = perf['avg']
        
        # ç»“åˆé™æ€å’ŒåŠ¨æ€è¯„åˆ†
        if dynamic_score is not None:
            # åŠ¨æ€è¯„åˆ†æƒé‡éšæ ·æœ¬æ•°å¢åŠ ï¼š20% -> 40%
            sample_count = self.source_performance[matched_source]['count']
            dynamic_weight = min(0.20 + sample_count * 0.02, 0.40)
            final_score = static_score * (1 - dynamic_weight) + dynamic_score * dynamic_weight
            return round(final_score, 3)
        
        return static_score
    
    def _calculate_recency(self, item: Dict, content_type: str = 'news') -> float:
        """
        è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•° - è‡ªé€‚åº”æŒ‡æ•°è¡°å‡æ›²çº¿
        
        ä½¿ç”¨æŒ‡æ•°è¡°å‡å…¬å¼: score = max_score * e^(-decay_rate * days) + min_score
        è¡°å‡ç‡æ ¹æ®å†…å®¹ç±»å‹è‡ªé€‚åº”è°ƒæ•´
        
        Args:
            item: æ•°æ®é¡¹
            content_type: å†…å®¹ç±»å‹ï¼ˆå½±å“è¡°å‡ç‡ï¼‰
            
        Returns:
            æ—¶æ•ˆæ€§åˆ†æ•° 0-1
        """
        published = item.get('published', '')
        
        if not published:
            # æ— æ—¥æœŸä¿¡æ¯ï¼Œç»™ä¸­ç­‰åˆ†æ•°
            return 0.5
        
        try:
            # è§£ææ—¥æœŸ
            if isinstance(published, datetime):
                pub_date = published
            elif isinstance(published, str):
                # å°è¯•å¤šç§æ ¼å¼è§£æ
                try:
                    pub_date = date_parser.parse(published)
                except (ValueError, TypeError):
                    # å°è¯•ç®€å•æ ¼å¼
                    if len(published) >= 10:
                        pub_date = datetime.strptime(published[:10], '%Y-%m-%d')
                    else:
                        return 0.5
            else:
                return 0.5
            
            # è®¡ç®—å¤©æ•°å·®
            now = datetime.now()
            # å¤„ç†æ—¶åŒº
            if pub_date.tzinfo is not None and now.tzinfo is None:
                pub_date = pub_date.replace(tzinfo=None)
            
            days_ago = (now - pub_date).days
            
            # æœªæ¥æ—¥æœŸæˆ–ä»Šå¤©
            if days_ago <= 0:
                return 1.0
            
            # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©è¡°å‡ç‡
            decay_rate = self.type_decay_rates.get(content_type, self.recency_decay_rate)
            min_score = self.recency_min_score
            
            # æŒ‡æ•°è¡°å‡
            score = (1.0 - min_score) * math.exp(-decay_rate * days_ago) + min_score
            
            return round(max(min(score, 1.0), min_score), 3)
                
        except Exception:
            return 0.5
    
    def _calculate_relevance(self, item: Dict, content_type: str) -> float:
        """
        è®¡ç®—å†…å®¹ç›¸å…³åº¦ - åˆ†å±‚å…³é”®è¯ç³»ç»Ÿ
        
        ä½¿ç”¨åˆ†å±‚å…³é”®è¯ç³»ç»Ÿ:
        1. çªç ´æ€§/é‡Œç¨‹ç¢‘äº‹ä»¶ (æœ€é«˜æƒé‡)
        2. å‘å¸ƒ/å…¬å‘Šäº‹ä»¶ (é«˜æƒé‡)
        3. æŠ€æœ¯/æ¨¡å‹ç›¸å…³ (ä¸­æƒé‡)
        4. ä¸€èˆ¬æ€§æè¿° (ä½æƒé‡)
        å¹¶è€ƒè™‘è´Ÿé¢å…³é”®è¯é™æƒ
        
        Args:
            item: æ•°æ®é¡¹
            content_type: åˆ†ç±»ç±»å‹
            
        Returns:
            ç›¸å…³åº¦åˆ†æ•° 0-1
        """
        title = item.get('title', '').lower()
        summary = item.get('summary', '').lower()
        text = f"{title} {summary}"
        
        # åŸºç¡€åˆ†
        score = 0.25
        
        # åˆ†å±‚å…³é”®è¯åŒ¹é… - ä½¿ç”¨é›†åˆé¿å…é‡å¤è®¡åˆ†
        matched_keywords = set()
        layer_scores = {'breakthrough': 0, 'release': 0, 'tech': 0, 'general': 0}
        
        # ç¬¬ä¸€å±‚: çªç ´æ€§å…³é”®è¯ (æœ€é«˜ä»·å€¼)
        for keyword, boost in self.breakthrough_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['breakthrough'] += boost
                matched_keywords.add(keyword)
        
        # ç¬¬äºŒå±‚: å‘å¸ƒå…³é”®è¯
        for keyword, boost in self.release_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['release'] += boost
                matched_keywords.add(keyword)
        
        # ç¬¬ä¸‰å±‚: æŠ€æœ¯å…³é”®è¯
        for keyword, boost in self.tech_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['tech'] += boost
                matched_keywords.add(keyword)
        
        # ç¬¬å››å±‚: ä¸€èˆ¬å…³é”®è¯
        for keyword, boost in self.general_keywords.items():
            if keyword in text and keyword not in matched_keywords:
                layer_scores['general'] += boost
                matched_keywords.add(keyword)
        
        # åˆ†å±‚åŠ åˆ†ï¼Œé«˜å±‚æ¬¡å…³é”®è¯æœ‰æ›´å¤§å½±å“
        # çªç ´å±‚å…¨é¢è®¡åˆ†ï¼Œå…¶ä»–å±‚æœ‰è¡°å‡
        score += layer_scores['breakthrough']  # 100% æƒé‡
        score += layer_scores['release'] * 0.9  # 90% æƒé‡
        score += layer_scores['tech'] * 0.8     # 80% æƒé‡
        score += layer_scores['general'] * 0.6  # 60% æƒé‡
        
        # è´Ÿé¢å…³é”®è¯é™æƒ
        for keyword, penalty in self.negative_relevance_keywords.items():
            if keyword in text:
                score += penalty  # penalty æ˜¯è´Ÿæ•°
        
        # æ ‡é¢˜ä¸­çš„å…³é”®è¯é¢å¤–åŠ åˆ† (æ ‡é¢˜é€šå¸¸æ›´é‡è¦)
        title_bonus = 0
        for keyword in matched_keywords:
            if keyword in title:
                title_bonus += 0.02  # æ¯ä¸ªæ ‡é¢˜ä¸­çš„å…³é”®è¯é¢å¤–+0.02
        score += min(title_bonus, 0.10)  # ä¸Šé™ 0.10
        
        # æ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´
        multiplier = self.type_relevance_multipliers.get(content_type, 0.9)
        score *= multiplier
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
        return round(max(min(score, 1.0), 0.1), 3)
    
    def _calculate_engagement(self, item: Dict) -> float:
        """
        è®¡ç®—ç¤¾äº¤çƒ­åº¦ - ç»Ÿä¸€å½’ä¸€åŒ–ç®—æ³•
        
        ä½¿ç”¨ç»Ÿä¸€çš„å¯¹æ•°å½’ä¸€åŒ–å…¬å¼:
        score = log(value + 1) / log(threshold_high + 1) * weight
        
        æ”¯æŒå¤šä¸ªç¤¾äº¤ä¿¡å·çš„åŠ æƒç»„åˆ
        
        Args:
            item: æ•°æ®é¡¹
            
        Returns:
            çƒ­åº¦åˆ†æ•° 0-1
        """
        signals = []
        
        # ç»Ÿä¸€çš„å½’ä¸€åŒ–å‡½æ•°
        def normalize_signal(value: int, config: dict) -> float:
            """ç»Ÿä¸€çš„å¯¹æ•°å½’ä¸€åŒ–"""
            if not value or value <= 0:
                return None
            
            threshold_low = config['threshold_low']
            threshold_high = config['threshold_high']
            weight = config['weight']
            
            # å¯¹æ•°å½’ä¸€åŒ–ï¼Œå¹¶åº”ç”¨æƒé‡
            # ä½äºé˜ˆå€¼çš„ç»™äºˆè¾ƒä½åˆ†
            if value < threshold_low:
                score = math.log(value + 1) / math.log(threshold_low + 1) * 0.4
            else:
                # åœ¨é˜ˆå€¼èŒƒå›´å†…çš„æ­£å¸¸è®¡åˆ†
                score = 0.4 + 0.6 * math.log(value / threshold_low + 1) / math.log(threshold_high / threshold_low + 1)
            
            return min(score * weight, 1.0)
        
        # GitHub stars
        stars = item.get('stars', 0)
        if stars:
            score = normalize_signal(stars, self.engagement_config['github_stars'])
            if score is not None:
                signals.append(('stars', score, self.engagement_config['github_stars']['weight']))
        
        # HuggingFace downloads
        downloads = item.get('downloads', 0)
        if downloads:
            score = normalize_signal(downloads, self.engagement_config['huggingface_downloads'])
            if score is not None:
                signals.append(('downloads', score, self.engagement_config['huggingface_downloads']['weight']))
        
        # Reddit score
        reddit_score = item.get('score', 0)
        if reddit_score and 'reddit' in item.get('source', '').lower():
            score = normalize_signal(reddit_score, self.engagement_config['reddit_score'])
            if score is not None:
                signals.append(('reddit', score, self.engagement_config['reddit_score']['weight']))
        
        # Hacker News points
        hn_points = item.get('points', 0)
        if hn_points:
            score = normalize_signal(hn_points, self.engagement_config['hn_points'])
            if score is not None:
                signals.append(('hn', score, self.engagement_config['hn_points']['weight']))
        
        # é€šç”¨likes
        likes = item.get('likes', item.get('favorites', 0))
        if likes:
            score = normalize_signal(likes, self.engagement_config['likes'])
            if score is not None:
                signals.append(('likes', score, self.engagement_config['likes']['weight']))
        
        # è¯„è®ºæ•°
        comments = item.get('comments', item.get('num_comments', 0))
        if comments:
            score = normalize_signal(comments, self.engagement_config['comments'])
            if score is not None:
                signals.append(('comments', score, self.engagement_config['comments']['weight']))
        
        # æ— ç¤¾äº¤æ•°æ®ï¼Œç»™ä¸­ç­‰åˆ†
        if not signals:
            return 0.5
        
        # åŠ æƒå¹³å‡
        total_weight = sum(s[2] for s in signals)
        weighted_sum = sum(s[1] for s in signals)
        
        # ç»„åˆå¤šä¸ªä¿¡å·æ—¶ç»™äºˆå°å¹…åŠ åˆ† (å¤šç»´åº¦éªŒè¯)
        multi_signal_bonus = min(len(signals) - 1, 3) * 0.03
        
        final_score = weighted_sum / total_weight + multi_signal_bonus
        
        return round(min(max(final_score, 0.0), 1.0), 3)
    
    def get_importance_level(self, score: float) -> Tuple[str, str]:
        """
        è·å–é‡è¦æ€§ç­‰çº§å’Œæ ‡ç­¾
        
        Args:
            score: é‡è¦æ€§åˆ†æ•°
            
        Returns:
            (ç­‰çº§, emojiæ ‡ç­¾)
        """
        if score >= 0.85:
            return 'critical', 'ğŸ”´'
        elif score >= 0.70:
            return 'high', 'ğŸŸ '
        elif score >= 0.55:
            return 'medium', 'ğŸŸ¡'
        elif score >= 0.40:
            return 'low', 'ğŸŸ¢'
        else:
            return 'minimal', 'âšª'
    
    def update_source_performance(self, source: str, final_importance: float):
        """
        æ›´æ–°æ¥æºçš„å†å²è¡¨ç°ï¼ˆç”¨äºåŠ¨æ€å­¦ä¹ ï¼‰
        
        Args:
            source: æ¥æºåç§°
            final_importance: æœ€ç»ˆé‡è¦æ€§è¯„åˆ†
        """
        if not source:
            return
        
        source_key = source.lower()
        perf = self.source_performance[source_key]
        
        # æ»šåŠ¨çª—å£ï¼šåªä¿ç•™æœ€è¿‘50ä¸ªè¯„åˆ†
        perf['scores'].append(final_importance)
        if len(perf['scores']) > 50:
            perf['scores'] = perf['scores'][-50:]
        
        # æ›´æ–°ç»Ÿè®¡
        perf['count'] = len(perf['scores'])
        perf['avg'] = sum(perf['scores']) / perf['count']
        
        # å®šæœŸä¿å­˜ï¼ˆæ¯10æ¬¡æ›´æ–°ä¿å­˜ä¸€æ¬¡ï¼‰
        self.user_feedback_count += 1
        if self.user_feedback_count % 10 == 0:
            self._save_learning_data()
    
    def _load_learning_data(self):
        """
        åŠ è½½å†å²å­¦ä¹ æ•°æ®
        """
        try:
            if os.path.exists(LEARNING_CONFIG_FILE):
                with open(LEARNING_CONFIG_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # æ¢å¤æ¥æºè¡¨ç°æ•°æ®
                if 'source_performance' in data:
                    for source, perf in data['source_performance'].items():
                        self.source_performance[source] = perf
                
                log.info(f"ğŸ“š Loaded learning data: {len(self.source_performance)} sources")
        except Exception as e:
            log.warning(f"Failed to load learning data: {e}")
    
    def _save_learning_data(self):
        """
        ä¿å­˜å­¦ä¹ æ•°æ®åˆ°æ–‡ä»¶
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(LEARNING_CONFIG_FILE), exist_ok=True)
            
            data = {
                'source_performance': dict(self.source_performance),
                'last_updated': datetime.now().isoformat()
            }
            
            with open(LEARNING_CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            log.info(f"ğŸ’¾ Saved learning data: {len(self.source_performance)} sources")
        except Exception as e:
            log.warning(f"Failed to save learning data: {e}")
    
    def get_learning_stats(self) -> Dict:
        """
        è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        learned_sources = sum(1 for perf in self.source_performance.values() if perf['count'] >= 5)
        
        return {
            'total_sources_tracked': len(self.source_performance),
            'learned_sources': learned_sources,
            'total_samples': sum(perf['count'] for perf in self.source_performance.values()),
            'learning_enabled': learned_sources > 0
        }
